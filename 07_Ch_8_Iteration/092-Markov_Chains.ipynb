{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 8.2 Markov Processes\n",
    "subject:  Iteration\n",
    "subtitle: transitioning with short-term memory\n",
    "short_title: 8.2 Markov Processes\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: \n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/07_Ch_8_Iteration/092-Markov_Chains.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 15 - Linear Iterative Systems, Matrix Powers, Markov Chains, and Googleâ€™s PageRank.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be found in Section 4.9 and Chapter 10 of LAA $5^{th}$ edition, and ALA 9.3.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- what is a Markov chain model using a weather prediction example\n",
    "- what is a probability vector and a transition matrix (regular)\n",
    "- when does a Markov chain converge to a unique probability vector\n",
    "- how the eigenvalue and eigenvector of the transition matrix relates to the convergence of the Markov chain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Prediction: Introduction\n",
    "\n",
    "We will spend this section on _Markov Chains_, which are a widely used linear iterative model to describe a wide variety of situations in biology, business, chemistry, engineering, physics, and elsewhere.\n",
    "\n",
    "In each case, the model is used to describe an experiment or measurement that is performed many times in the same way. The outcome of an experiment can be one of several known possible outcomes, and importantly, the outcome of one experiment depends only on the experiment conducted immediately before it. Before introducing a formal model for Markov chains, let's look at an example.\n",
    "\n",
    ":::{prf:example} Weather Prediction\n",
    ":label: weather_eg\n",
    "Suppose you would like to predict the weather in your city. Looking at local weather records over the past 10 years, you notice that:\n",
    "1. If today is sunny, tomorrow is sunny 70% of the time and cloudy 30% of the time.\n",
    "2. If today is cloudy, tomorrow is cloudy 80% of the time and sunny 20% of the time.\n",
    "\n",
    "Now, suppose today is sunny. What is the _probability_$^{1}$ that the weather 8 days from now will also be sunny?\n",
    "\n",
    "$^{1}$ You will learn how to properly define probabilities in ESE 3010. For our purposes, you can think of it as confidences or likelihood. So saying that 8 days from now will be sunny with probability $60\\%$ is the same as saying that the weather 10 days from now is determined by flipping a biased coin that comes up \"sunny\" $60\\%$ of the time and \"cloudy\" 40% of the time.\n",
    "\n",
    "To formulate this problem mathematically, let's use $S(k)$ to denote the probability that day $k$ is sunny and $C(k)$ the probability that it is cloudy. If these are the only two possibilities, then the individual probabilities must sum to 1 (1 represents 100% likely, .5 50% likely, etc.): $S(k) + C(k) = 1$.\n",
    "\n",
    "According to our historical data, the probability that day $k+1$ is sunny or cloudy can be expressed as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{sun_cloud}\n",
    "S(k+1) = .7 S(k) + .2 C(k),    \\quad C(k+1) = .3 S(k) + .8 C(k)\n",
    "\\end{equation}\n",
    "\n",
    "For example, the [equation](#sun_cloud) says that if day $k$ was sunny, i.e., $S(k)=1$ and $C(k)=0$ there is a 70% chance day $k+1$ is too; similarly, if day $k$ was cloudy, i.e., $S(k)=0$ and $C(k)=1$, there is a 40% chance day $k+1$ is sunny.\n",
    "\n",
    "We rewrite [](#sun_cloud) as the linear iterative system $\\vv x(k+1) = P \\vv x(k)$, where\n",
    "\n",
    "$$\n",
    "P = \\bm .7 & .2 \\\\ .3  & .8 \\em  \\ \\text{and}\\ \\ \\vv x(k) = \\bm S(k) \\\\ C(k) \\em            \n",
    "$$\n",
    "\n",
    "We use $P$ instead of $T$ here as this is a typical convention for describing the _transition matrix_ of a Markov chain. The vector $\\vv x(k)$ is called the $k^{th}$ _state vector_.\n",
    "\n",
    "Now, given that today is sunny, i.e., that $S(0) = 1$ and $C(0) = 0$, what is the probability that 8 days from now is sunny? We can answer this easily by iterating the system $\\vv x(k+1) = P \\vv x(k)$ to compute $\\vv x(8)$!\n",
    "\n",
    "\\begin{align*}\n",
    "\\vv x(0) &= \\bm 1 \\\\ 0 \\em \\vv x(1) = P \\vv x(0) = \\bm .7 \\\\ .3 \\em, \\ \\vv x(2) = P \\vv x(1) = P^2 \\vv x(0) \\approx \\bm .55 \\\\ .45 \\em \\\\\n",
    "\n",
    "\\vv x(3) &= P^3 \\vv x(0) \\approx  \\bm .475 \\\\ .525 \\em, \\ \\vv x(4) \\approx \\bm .438 \\\\ .562 \\em, \\ \\vv x(5) \\approx \\bm .419 \\\\ .581 \\em, \\ \\vv x(6) \\approx \\bm .410 \\\\ .590 \\em \\\\\n",
    "\n",
    "\\vv x(7) &\\approx  \\bm .405 \\\\ .595 \\em, \\ \\vv x(8) \\approx \\bm .402 \\\\ .598 \\em\n",
    "\\end{align*}\n",
    "\n",
    "So we conclude that 40.2% of the time, if today is sunny, then 8 days from now is also sunny.\n",
    ":::\n",
    "\n",
    ":::{note} Observations from [](#weather_eg)\n",
    "We make a few observations about the state vectors $\\vv x(k)$ to motivate some of the new tools we'll introduce:\n",
    "\n",
    "1) Every state vector $\\vv x(k)$ is a _probability vector_, i.e., $ x_1(k)$ and $x_2(k) \\geq 0$ and $x_1(k) + x_2(k) = 1$\n",
    "2) The process converges fairly quickly to $\\vv x^* = \\bm .4 \\\\ .6 \\em $, which is a _fixed point_ of $\\vv x(k+1) = P\\vv x(k)$, i.e., $\\vv x^* = P\\vv x^*$\n",
    "3) The convergence to $\\vv x^*$ actually happens for any initial probability vector $\\vv x(0)$. This means that in the long run, 40% of days are sunny and 60% are rainy.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence in Markov Chains\n",
    "\n",
    "Let's try to understand why the convergence in [](#weather_eg) happens, and then we'll look at some interesting applications of Markov chains.\n",
    "\n",
    "Our starting point is a general definition of a _probability vector_.\n",
    ":::{prf:definition} Probability Vector\n",
    ":label: prob_vec_defn\n",
    "A vector $\\vv x \\in \\mathbb{R}^n$ is called a _probability vector_ if $x_i \\geq 0$ for $i=1,\\ldots,n$ and $x_1 + \\ldots + x_n = 1$. We interpret $x_i$ as the probability (or likelihood) that the system is in state $i$.\n",
    ":::\n",
    "\n",
    "In general, a _Markov chain_ is given by the first order linear iterative system\n",
    "\\begin{equation}\n",
    "\\label{MC_eqn}\n",
    "\\vv x(k+1) = P \\vv x(k)  \\quad (\\text{MC})\n",
    "\\end{equation}\n",
    "\n",
    "whose initial state $\\vv x(0)$ is a probability vector. The entries of the _transition matrix_ $P$ must satisfy\n",
    "\\begin{equation}\n",
    "\\label{TM_eqn}\n",
    "0 \\leq p_{ij} \\leq 1 \\  \\text{and} \\  p_{1j} + \\cdots + p_{nj} = 1.  \\quad (\\text{TM})\n",
    "\\end{equation}\n",
    "\n",
    "for all $i,j=1,\\ldots,n$. The entry $p_{ij}$ is the _transition probability_ that the system will switch from state $j$ to state $i$. Because this covers all possible transitions, this means each column sums to 1. Under these conditions, we can guarantee that if $\\vv x(k)$ is a probability vector, so is $\\vv x(k+1) = P\\vv x(k)$. To see this, note that $\\vv 1^{\\top} P = \\bm \\vv 1^{\\top} \\vv  p_1 & \\cdots & 1^{\\top} \\vv p_n\\em = \\bm 1 & \\cdots & 1 \\em = \\vv 1^{\\top}$ so that $\\vv 1^{\\top} \\vv x(k+1) = \\vv 1^{\\top} P\\vv x(k) = \\vv 1^{\\top} \\vv x(k) = 1$. That $\\vv x(k+1)$ is entrywise non-negative follows from $P$ and $\\vv x(k)$ being entry-wise non-negative.\n",
    "\n",
    "Next, let's investigate convergence properties. We first need to impose a very mild technical condition on the transition matrix $P$, namely we assume that it is _regular_.\n",
    "\n",
    ":::{prf:definition} Regular Transition Matrix\n",
    ":label:regular_defn\n",
    "A transition matrix $P$ ([TM](#TM_eqn)) is _regular_ if for some power $k$, $P^k$ contains no zero entries. This means that it is possible to get from one state to any other state in $k$ steps.\n",
    ":::\n",
    "\n",
    "The long-term behavior of a Markov chain with regular transition matrix $P$ is governed by the _Perron-Frobenius_ theorem, which we state next. The proof is quite involved, so we won't cover it, but if you're curious, check out the end of ALA 9.3.\n",
    "\n",
    ":::{prf:theorem}\n",
    ":label: regular_thm\n",
    "If $P$ is a [regular transition matrix](#regular_defn), then it admits a unique _probability eigenvector_ $\\vv x^*$ with eigenvalue $\\lambda_1=1$. Moreover, a Markov chain with coefficient matrix $P$ will converge to $\\vv x^*: \\vv x(k) \\to \\vv x^*$ as $k \\to \\infty$.\n",
    ":::\n",
    "\n",
    "This is a very exciting development! It tells us that we can understand the long-term behavior of a regular Markov chain by solving for the eigenvector $\\vv x^*$ associated with the eigenvalue $\\lambda_1=1$ of $P$.\n",
    "\n",
    "Returning to our weather prediction example, we compute the steady state probability vector $\\vv x^*$ by just solving $(P-I)\\vv v=\\vv 0$:\n",
    "$$\n",
    "(P-I)\\vv v = \\bm -.3 & .2 \\\\ .3 & -.2 \\em \\bm v_1 & v_2 \\em = \\vv 0 => v_1 = \\frac{2}{3} v_2 \\Rightarrow \\vv v = \\bm \\frac{2}{3} \\\\ 1 \\em\n",
    "$$\n",
    "\n",
    "and then normalizing $\\vv v$ so that its entries add up to 1:\n",
    "$$\n",
    "\\vv x^* = \\frac{1}{1+\\frac{2}{3}} \\bm \\frac{2}{3} \\\\ 1 \\em = \\bm \\frac{2}{5} \\\\ \\frac{3}{5} \\em = \\bm 0.4 \\\\ 0.6 \\em\n",
    "$$\n",
    "\n",
    "This special eigenvector $\\vv x^*$ tells us that _no matter the initial state_ $\\vv x(0)$, the long term behavior is that we are in State 1 (sunny) 40% of days and State 2 (cloudy) 60% of days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{prf:example} Get out the vote!\n",
    ":label: vote_eg\n",
    "Suppose the voting results of a congressional election at a certain voting precinct are represented by a vector $\\vv x \\in \\mathbb{R}^3$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{voting_vec_type}\n",
    "\\vv x = \\bm \\% \\ \\text{voting Democratic (D)} \\\\\n",
    "    \\% \\ \\text{voting Reupublican (R)} \\\\\n",
    "    \\% \\ \\text{voting Libertarian (LD)} \\em\n",
    "\\end{equation}\n",
    "\n",
    "We record the outcome of this election every two years by a vector of [this](#voting_vec_type) type, and let's assume that the outcome of one election depends only on results of the previous one. Then the sequence $\\vv x(k)$ of vectors that describe the votes in each election form a Markov chain. Suppose, using historical data, we estimate the following transition matrix P:\n",
    "$$\n",
    "& \\quad \\quad \\text{From:} \\\\\n",
    "& \\quad \\begin{matrix} \\text{D}  & \\text{R} &  \\text{L} & \\text{To:} \\end{matrix} \\\\                 \n",
    "P =& \\bm .7 & .1 & .3 \\\\ \n",
    "            .2 & .8 & .3 \\\\ \n",
    "    .1 & .1 & .4 \\em \\quad \\begin{matrix}  \\text{D} \\\\ \\text{R} \\\\ \\text{L} \\end{matrix}\n",
    "$$\n",
    "The entries in the first column, labeled D, describe what % of persons who voted D in the last election will vote D, R, and L in this one: in this example, 70% of prior D voters will vote D again, 20% will vote R, and 10% will vote L.\n",
    "\n",
    "If we assume that $P$ remains fixed across many elections, we can predict not only the next election's results, but long-term election results as well. For example, if last election had results:\n",
    "$$\n",
    "\\vv x(0) = \\bm .55 \\\\ .40 \\\\ .05 \\em\n",
    "$$\n",
    "\n",
    "then the next election will have a likely outcome of\n",
    "\n",
    "$$\n",
    "\\vv x(1) = P \\vv x(0) = \\bm .44 \\\\ .445 \\\\ .115 \\em\n",
    "$$\n",
    "\n",
    "and the following election will have likely outcome\n",
    "$$\n",
    "\\vv x(2) = P \\vv x(1) = \\bm .387 \\\\ .4785  \\\\ .1345 \\em\n",
    "$$\n",
    "\n",
    "In the long run, we expect vectors converge to the steady state distribution $\\vv x^*$ satisfying $\\vv x^* = P\\vv x^*$, which we obtain by solving:\n",
    "$$\n",
    "(P - I)\\vv = \\vv 0\n",
    "$$\n",
    "\n",
    "and setting $\\vv x^* = \\frac{1}{\\v 1^{\\top} \\vv v} \\vv v$. In this case, this works out to:\n",
    "\n",
    "$$\n",
    "\\vv x^* \\approx \\begin{bmatrix} 0.321 \\\\ 0.536 \\\\ 0.143 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "which informs that assuming voter patterns do not change, 32.1\\% of voters will go to D, 53.6\\% to R, and 14.5\\% to I in this precinct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/07_Ch_8_Iteration/092-Markov_Chains.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
