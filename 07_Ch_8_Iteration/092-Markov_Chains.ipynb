{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 8.2 Markov Chains\n",
    "subject:  Iteration\n",
    "subtitle: \n",
    "short_title: 8.2 Markov Chains\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: \n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/07_Ch_8_Iteration/092-Markov_Chains.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 15 - Linear Iterative Systems, Matrix Powers, Markov Chains, and Google’s PageRank.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be found in \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Chains (LAA 5th Edition, §4.9 and Ch.10, ALA §3)\n",
    "\n",
    "We will spend the rest of this lecture on Markov Chains, which are a widely used linear iterative model used to describe a wide variety of situations in biology, business, chemistry, engineering, physics, and elsewhere.\n",
    "\n",
    "In each case, the model is used to describe an experiment or measurement that is performed many times in the same way. The outcome of an experiment can be one of several known possible outcomes, and importantly, the outcome of one experiment depends only on the experiment conducted immediately before it. Before introducing a formal model for Markov chains, let's look at an example.\n",
    "\n",
    "Example: Weather Prediction\n",
    "\n",
    "Suppose you would like to predict the weather in your city. Looking at local weather records over the past 10 years, you notice that:\n",
    "(i) If today is sunny, tomorrow is sunny 70% of the time and cloudy 30% of the time.\n",
    "(ii) If today is cloudy, tomorrow is cloudy 60% of the time and sunny 40% of the time.\n",
    "\n",
    "Now, suppose today is sunny. What is the probability¹ that the weather 8 days from now will also be sunny?\n",
    "\n",
    "¹ You will learn how to properly define probabilities in ESE 3010. For our purposes, you can think of it as confidence or likelihood. So saying that 8 days from now will be sunny with probability 60% is the same as saying that the weather 10 days from now is determined by flipping a biased coin that comes up \"sunny\" 60% of the time and \"cloudy\" 40% of the time.\n",
    "\n",
    "To formulate this problem mathematically, let's use S(k) to denote the probability that day k is sunny and C(k) the probability that it is cloudy. If these are the only two possibilities, then the individual probabilities must sum to 1 (i.e., represents 100% likely, as 50% likely, etc.): S(k) + C(k) = 1.\n",
    "\n",
    "According to our historical data, the probability that day k+1 is sunny or cloudy can be expressed as:\n",
    "\n",
    "S(k+1) = .7 S(k) + .4 C(k),    C(k+1) = .3 S(k) + .6 C(k)    (*)\n",
    "\n",
    "For example, the equation says that if day k was sunny, i.e., S(k)=1 and C(k)=0 there is a 70% chance day k+1 is too; similarly, if day k was cloudy, i.e., S(k)=0 and C(k)=1, there is a 40% chance day k+1 is sunny."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rewrite (*) as the linear iterative system x(k+1) = P x(k), where\n",
    "\n",
    "P = [.7  .4]    and x(k) = [S(k)]\n",
    "    [.3  .6]              [C(k)]\n",
    "\n",
    "We use P instead of T here as this is a typical convention for describing the transition matrix of a Markov chain. The vector x(k) is called the kth state vector.\n",
    "\n",
    "Now, given that today is sunny, i.e., that S(0) = 1 and C(0) = 1, what is the probability that 8 days from now is sunny? We can answer this easily by iterating the system x(k+1) = P x(k) to compute x(8)!\n",
    "\n",
    "x(0) = [1], x(1) = Px(0) ≈ [.7], x(2) = Px(1) ≈ P²x(0) ≈ [.55]\n",
    "       [0]                [.3]                         [.45]\n",
    "\n",
    "x(3) = P³x(0) ≈ [.475], x(4) ≈ [.438], x(5) ≈ [.419], x(6) ≈ [.410]\n",
    "                [.525]         [.562]         [.581]         [.590]\n",
    "\n",
    "x(7) ≈ [.405], x(8) ≈ [.402]\n",
    "       [.595]         [.598]\n",
    "\n",
    "So we conclude that 40.2% of the time, if today is sunny, then 8 days from now is also sunny.\n",
    "\n",
    "We make a few observations about the state vectors x(k) to motivate some of the new tools we'll introduce:\n",
    "\n",
    "1) Every state vector x(k) is a probability vector, i.e., x₁(k) and x₂(k) ≥ 0 and x₁(k) + x₂(k) = 1\n",
    "2) The process converges fairly quickly to x* = [.4], which is a fixed\n",
    "                                                [.6]\n",
    "   point of x(k+1) = Px(k), i.e., x* = Px*\n",
    "3) This convergence to x* actually happens for any initial probability vector x(0). This means that in the long run, 40% of days are sunny and 60% are rainy.\n",
    "\n",
    "Let's try to understand why this happens, and then we'll look at some interesting applications of Markov chains.\n",
    "\n",
    "Our starting point is a general definition of a probability vector: a vector x ∈ ℝⁿ is called a probability vector if xᵢ ≥ 0 for i=1,...,n and x₁ + ... + xₙ = 1. We interpret xᵢ as the probability (or likelihood) that the system is in state i."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a Markov chain is given by the first order linear iterative system\n",
    "\n",
    "x(k+1) = P x(k)  (MC)\n",
    "\n",
    "whose initial state x(0) is a probability vector. The entries of the transition matrix P must satisfy\n",
    "\n",
    "0 ≤ pᵢⱼ ≤ 1  and  pᵢ₁ + ... + pᵢₙ = 1.  (TM)\n",
    "\n",
    "for all i,j=1,...,n. The entry pᵢⱼ is the transition probability that the system will switch from state j to state i. Because this covers all possible transitions, this means each column sums to 1. Under these conditions, we can guarantee that if x(k) is a probability vector, so is x(k+1) = Px(k). To see this, note that 1ᵀP = 1ᵀ[p₁₁ ... 1p₁ₙ] = [1 ... 1] = 1ᵀ so that 1ᵀx(k+1) = 1ᵀPx(k) = 1ᵀx(k) = 1. That x(k+1) is entrywise non-negative follows from P and x(k) being entry-wise non-negative.\n",
    "\n",
    "Next, let's investigate convergence properties. We first need to impose a very mild technical condition on the transition matrix P, namely we assume that it is regular: A transition matrix P (TM) is regular if for some power k, Pᵏ contains no zero entries. This means that it is possible to get from one state to any other state in k steps.\n",
    "\n",
    "The long-term behavior of a Markov chain with regular transition matrix P is governed by the Perron-Frobenius theorem, which we state next. The proof is quite involved, so we omit it, but if you're curious, check out the end of ALA §3.\n",
    "\n",
    "Theorem: If P is a regular transition matrix, then it admits a unique probability vector x* with eigenvalue λ₁=1. Moreover, a Markov chain with regular matrix P will converge to x*: x(k) → x* as k→∞\n",
    "\n",
    "This is a very exciting development! It tells us that we can understand the long-term behavior of a regular Markov chain by solving for the eigenvector x* associated with the eigenvalue λ₁=1 of P.\n",
    "\n",
    "Returning to our weather prediction example, we compute the steady state probability vector x* by just solving (P-I)v=0:\n",
    "\n",
    "(P-I)v = [-.3 .4][v₁] = 0 => v₁ = 2 v₂ => v = [⁴⁄₃]\n",
    "         [.3 -.4][v₂]                           [²⁄₃]\n",
    "\n",
    "and then normalizing v so that its entries add up to 1:\n",
    "\n",
    "x* = 1/(1+²⁄₃) [⁴⁄₃] = [⁴⁄₅] = [0.4]\n",
    "               [²⁄₃]   [²⁄₅]   [0.6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This special eigenvector x* tells us that no matter the initial state x(0), the long term behavior is that we are in State 1 (sunny) 40% of days and State 2 (cloudy) 60% of days.\n",
    "\n",
    "Example: Get out the vote!\n",
    "\n",
    "Suppose the voting results of a congressional election at a certain voting precinct are represented by a vector x ∈ ℝ³:\n",
    "\n",
    "x = [% voting Democratic (D)]\n",
    "    [% voting Republican (R)]\n",
    "    [% voting Libertarian (L)]\n",
    "\n",
    "We record the outcome of this election every two years by a vector of this type, and let's assume that the outcome of one election depends only on results of the previous one. Then the sequence x(k) of vectors that describe the votes in each election form a Markov chain. Suppose, using historical data, we estimate the following transition matrix P:\n",
    "\n",
    "        From:\n",
    "        D   R   L   To:\n",
    "    [.7  .1  .3] D\n",
    "P ≈ [.2  .8  .3] R\n",
    "    [.1  .1  .4] L\n",
    "\n",
    "The entries in the first column, labeled D, describe what % of persons who voted D in the last election will vote D, R, and L in this one: in this example, 70% of prior D voters will vote D again, 20% will vote R, and 10% will vote L.\n",
    "\n",
    "If we assume that P remains fixed across many elections, we can predict not only the next election's results, but long-term election results as well. For example, if last election had results:\n",
    "\n",
    "x(0) = [.55]\n",
    "       [.40]\n",
    "       [.05]\n",
    "\n",
    "then the next election will have a likely outcome of\n",
    "\n",
    "x(1) = P x(0) ≈ [.44]\n",
    "               [.445]\n",
    "               [.115]\n",
    "\n",
    "and the following election will have likely outcome\n",
    "\n",
    "x(2) ≈ Px(1) ≈ [.387]\n",
    "              [.475]\n",
    "              [.1345]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the long run, we expect when converge to the steady state distribution $\\pi^*$ satisfying $\\pi^* = P\\pi^*$, which we obtain by solving:\n",
    "\n",
    "\\[(P - I)\\pi = 0\\]\n",
    "\n",
    "and setting $\\pi^* = \\frac{\\pi}{\\sum \\pi}$. In this case, this works out to:\n",
    "\n",
    "\\[\\pi^* \\approx \\begin{bmatrix} 0.521 \\\\ 0.336 \\\\ 0.143 \\end{bmatrix}\\]\n",
    "\n",
    "which informs that assuming voter patterns do not change, 52.1\\% of voters will go to D, 33.6\\% to R, and 14.3\\% to I in this precinct.\n",
    "\n",
    "\\textbf{Random Walks, Page Rank, and the Google Matrix}\n",
    "\n",
    "An internet user's behavior while surfing the web can be modeled as a Markov chain that captures a random walk on a graph. We start with a simple example of this, and then explain how it can be used to design a search engine.\n",
    "\n",
    "Consider the following graph:\n",
    "\n",
    "\\begin{center}\n",
    "\\includegraphics[scale=0.5]{figure4}\n",
    "\\end{center}\n",
    "\n",
    "Which has seven vertices interconnected by edges. Let's pretend this graph models a very simple internet: each vertex (or node) is a webpage and each edge is a hyperlink connecting page to each other. (For now we assume that if page i links to page j, then page j also links to page i, but this isn't necessary.) We assume that if a user is on page i, they will click on one of the hyperlinks uniformly at random. For example, if a user is on page 5, they will visit page 3 next 50\\% of the time and page 6 next 50\\% of the time. Similarly, if a user is on page 3, they will visit page 1, 2, or 4 next 33\\% of the time each.\n",
    "\n",
    "We can model this user behavior using a Markov chain, which is called a simple random walk on a graph. The transition matrix for this graph is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[\n",
    "P = \\begin{bmatrix}\n",
    "0 & 1/3 & 1/4 & 0 & 0 & 0 & 0 \\\\\n",
    "1/2 & 0 & 1/4 & 1 & 1/2 & 0 & 0 \\\\\n",
    "1/2 & 1/3 & 0 & 0 & 0 & 1/3 & 0 \\\\\n",
    "0 & 0 & 1/4 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 1/3 & 0 & 0 & 0 & 1/3 & 0 \\\\\n",
    "0 & 0 & 1/4 & 0 & 1/2 & 0 & 1 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 1/3 & 0\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "This allows us to answer questions such as the following: suppose 100 users start on page 6. After each user clicks on three hyperlinks, what \\% of users do we expect to find on each new page. The solution is given by setting our initial user distribution to:\n",
    "\n",
    "\\[\n",
    "x(0) = \\begin{bmatrix}\n",
    "0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "and computing $x(3) = P^3x(0) = \\begin{bmatrix}\n",
    ".0833 \\\\ .0417 \\\\ .4028 \\\\ 0 \\\\ .2778 \\\\ 0 \\\\ .1944\n",
    "\\end{bmatrix}$\n",
    "\n",
    "This tells, for example, that 40.28\\% of users starting on page 6 end up on page 3 after 3 hyperlink clicks.\n",
    "\n",
    "\\textbf{PageRank}\n",
    "\n",
    "The founders of Google, Sergey Brin and Lawrence Page, reasoned that important pages had links coming from other \"important\" pages, and thus, a typical internet user would spend more time on more important pages, and less time on less important pages. This can be captured by the steady state distribution $\\pi^*$ of the Markov chain we described above. Using this model, we can say that a typical user will spend $\\pi^*_i$ \\% of their time on page $i$. This is precisely the observation used to define the PageRank algorithm, which Google uses to rank the importance of the webpages it catalogs.\n",
    "\n",
    "\\textbf{Key Idea:} The importance of a webpage $i$ can be measured by the corresponding entry $\\pi^*_i$ of the steady state vector $\\pi^*$ of the Markov chain describing the behavior of a typical internet user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if a typical transition matrix $P$ describing the internet were regular, we would be done - we simply compute $\\pi^* = P\\pi^*$ and use $\\pi^*$ to rank webpages. Unfortunately, typical models of the web are directed graphs, which lead to non-regular transition matrices $P$. To address this, Google makes two adjustments, which we illustrate with the following slight modification of our previous example:\n",
    "\n",
    "\\begin{figure}[h]\n",
    "\\centering\n",
    "\\includegraphics[width=0.3\\textwidth]{figure1}\n",
    "\\caption{A seven-page Web}\n",
    "\\end{figure}\n",
    "\n",
    "\\[\n",
    "P = \\begin{bmatrix}\n",
    "0 & 1/2 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 1/3 & 0 & 1/2 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 & 0 & 1/3 & 0 \\\\\n",
    "0 & 0 & 1/3 & 1 & 0 & 0 & 0 \\\\\n",
    "0 & 1/2 & 0 & 0 & 0 & 1/3 & 0 \\\\\n",
    "0 & 0 & 1/3 & 0 & 1/2 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 1/3 & 1\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "The first issue that arises here is that pages 4 and 7 are dangling nodes: if a user ever ends up on page 4 or 7, they never leave as there are no outgoing links. This means that columns 4 and 7 never change as we compute $P^k$, and hence $P$ cannot be regular. To handle dangling nodes, the following adjustment is made:\n",
    "\n",
    "\\textbf{Adjustment 1:} If an internet user reaches a dangling node, they will pick any page on the web with equal probability and move to that page.\n",
    "\n",
    "This means that if state $j$ is an absorbing state, we replace column $j$ of $P$ with the vector $(1/n, ..., 1/n)$. For example, our modified transition matrix is now:\n",
    "\n",
    "\\[\n",
    "P_* = \\begin{bmatrix}\n",
    "0 & 1/2 & 0 & 1/7 & 0 & 0 & 1/7 \\\\\n",
    "0 & 0 & 1/3 & 1/7 & 1/2 & 0 & 1/7 \\\\\n",
    "1 & 0 & 0 & 1/7 & 0 & 1/3 & 1/7 \\\\\n",
    "0 & 0 & 1/3 & 1/7 & 0 & 0 & 1/7 \\\\\n",
    "0 & 1/2 & 0 & 1/7 & 0 & 1/3 & 1/7 \\\\\n",
    "0 & 0 & 1/3 & 1/7 & 1/2 & 0 & 1/7 \\\\\n",
    "0 & 0 & 0 & 1/7 & 0 & 1/3 & 1/7\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "While this helps eliminate dangling nodes, we may still not have a regular transition matrix, as there might still be cycles of pages. For example, if page $i$ only links to page $j$, and page $j$ only links to page $i$, a user entering either page is condemned to moving back and forth between the two pages. This means the corresponding columns of $P_*^k$ will always have zeros in them, and hence $P_*$ would not be regular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\textbf{Adjustment 2:} Pick a number $p$ between 0 and 1. If a user is on page $i$, then $p$ proportion of the time they will pick from all possible hyperlinks on that page with equal probability and move to that page. The other $1-p$ fraction of the time, they will pick any page on the web with equal probability and move to that page.\n",
    "\n",
    "In terms of the modified transition matrix $P_*$, the new transition matrix will be\n",
    "\n",
    "\\[G = pP_* + (1-p)E,\\]\n",
    "\n",
    "where $E \\in \\mathbb{R}^{n \\times n}$ and $E_{ij} = 1/n$ for $i,j=1,...,n$. The matrix $G$ is called the Google matrix. $G$ is easily seen to be regular, as all entries of $G$ are positive.\n",
    "\n",
    "Although any value of $p$ is valid, Google is thought to use $p \\approx 0.85$. For our example, the Google matrix is\n",
    "\n",
    "\\[\n",
    "G = .85\\begin{bmatrix}\n",
    "0 & 1/2 & 0 & 1/7 & 0 & 0 & 1/7 \\\\\n",
    "0 & 0 & 1/3 & 1/7 & 1/2 & 0 & 1/7 \\\\\n",
    "1 & 0 & 0 & 1/7 & 0 & 1/3 & 1/7 \\\\\n",
    "0 & 0 & 1/3 & 1/7 & 0 & 0 & 1/7 \\\\\n",
    "0 & 1/2 & 0 & 1/7 & 0 & 1/3 & 1/7 \\\\\n",
    "0 & 0 & 1/3 & 1/7 & 1/2 & 0 & 1/7 \\\\\n",
    "0 & 0 & 0 & 1/7 & 0 & 1/3 & 1/7\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "+ .15\\begin{bmatrix}\n",
    "1/7 & 1/7 & 1/7 & 1/7 & 1/7 & 1/7 & 1/7 \\\\\n",
    "1/7 & 1/7 & 1/7 & 1/7 & 1/7 & 1/7 & 1/7 \\\\\n",
    "1/7 & 1/7 & 1/7 & 1/7 & 1/7 & 1/7 & 1/7 \\\\\n",
    "1/7 & 1/7 & 1/7 & 1/7 & 1/7 & 1/7 & 1/7 \\\\\n",
    "1/7 & 1/7 & 1/7 & 1/7 & 1/7 & 1/7 & 1/7 \\\\\n",
    "1/7 & 1/7 & 1/7 & 1/7 & 1/7 & 1/7 & 1/7 \\\\\n",
    "1/7 & 1/7 & 1/7 & 1/7 & 1/7 & 1/7 & 1/7\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= \\begin{bmatrix}\n",
    ".021429 & .446429 & .021429 & .142857 & .021429 & .021429 & .142857 \\\\\n",
    ".021429 & .021429 & .304762 & .142857 & .446429 & .021429 & .142857 \\\\\n",
    ".871429 & .021429 & .021429 & .142857 & .021429 & .304762 & .142857 \\\\\n",
    ".021429 & .021429 & .304762 & .142857 & .021429 & .021429 & .142857 \\\\\n",
    ".021429 & .446429 & .021429 & .142857 & .021429 & .304762 & .142857 \\\\\n",
    ".021429 & .021429 & .304762 & .142857 & .446429 & .021429 & .142857 \\\\\n",
    ".021429 & .021429 & .021429 & .142857 & .021429 & .304762 & .142857\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "We can now compute the steady state vector $\\pi^* = G\\pi^*$, which is found to be:\n",
    "\n",
    "\\[\n",
    "\\pi^* = \\begin{bmatrix}\n",
    ".116293 \\\\\n",
    ".168567 \\\\\n",
    ".191263 \\\\\n",
    ".098844 \\\\\n",
    ".164054 \\\\\n",
    ".168567 \\\\\n",
    ".092413\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "Thus, we can rank the pages in terms of descending importance as: 3, 2, 6, 5, 1, 4, 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "The Google matrix for the world wide web has over 8 billion rows and columns, and computing $\\vv x^* = G \\vv x^*$ is a very non-trivial task. An iterative approach known as the power method is used in practice, and typically takes several days for Google to compute a new $\\vv x^*$, which it does every month. \n",
    "\n",
    "**TO DO**: accessible links to power method description\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/07_Ch_8_Iteration/092-Markov_Chains.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
