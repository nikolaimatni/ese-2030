{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 8.2 Markov Chains\n",
    "subject:  Iteration\n",
    "subtitle: \n",
    "short_title: 8.2 Markov Chains\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: \n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/07_Ch_8_Iteration/092-Markov_Chains.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 15 - Linear Iterative Systems, Matrix Powers, Markov Chains, and Google’s PageRank.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be found in \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Chains (LAA 5th Edition, §4.9 and Ch.10, ALA §3)\n",
    "\n",
    "We will spend the rest of this lecture on Markov Chains, which are a widely used linear iterative model used to describe a wide variety of situations in biology, business, chemistry, engineering, physics, and elsewhere.\n",
    "\n",
    "In each case, the model is used to describe an experiment or measurement that is performed many times in the same way. The outcome of an experiment can be one of several known possible outcomes, and importantly, the outcome of one experiment depends only on the experiment conducted immediately before it. Before introducing a formal model for Markov chains, let's look at an example.\n",
    "\n",
    "Example: Weather Prediction\n",
    "\n",
    "Suppose you would like to predict the weather in your city. Looking at local weather records over the past 10 years, you notice that:\n",
    "(i) If today is sunny, tomorrow is sunny 70% of the time and cloudy 30% of the time.\n",
    "(ii) If today is cloudy, tomorrow is cloudy 60% of the time and sunny 40% of the time.\n",
    "\n",
    "Now, suppose today is sunny. What is the probability¹ that the weather 8 days from now will also be sunny?\n",
    "\n",
    "¹ You will learn how to properly define probabilities in ESE 3010. For our purposes, you can think of it as confidence or likelihood. So saying that 8 days from now will be sunny with probability 60% is the same as saying that the weather 10 days from now is determined by flipping a biased coin that comes up \"sunny\" 60% of the time and \"cloudy\" 40% of the time.\n",
    "\n",
    "To formulate this problem mathematically, let's use S(k) to denote the probability that day k is sunny and C(k) the probability that it is cloudy. If these are the only two possibilities, then the individual probabilities must sum to 1 (i.e., represents 100% likely, as 50% likely, etc.): S(k) + C(k) = 1.\n",
    "\n",
    "According to our historical data, the probability that day k+1 is sunny or cloudy can be expressed as:\n",
    "\n",
    "S(k+1) = .7 S(k) + .4 C(k),    C(k+1) = .3 S(k) + .6 C(k)    (*)\n",
    "\n",
    "For example, the equation says that if day k was sunny, i.e., S(k)=1 and C(k)=0 there is a 70% chance day k+1 is too; similarly, if day k was cloudy, i.e., S(k)=0 and C(k)=1, there is a 40% chance day k+1 is sunny."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rewrite (*) as the linear iterative system x(k+1) = P x(k), where\n",
    "\n",
    "P = [.7  .4]    and x(k) = [S(k)]\n",
    "    [.3  .6]              [C(k)]\n",
    "\n",
    "We use P instead of T here as this is a typical convention for describing the transition matrix of a Markov chain. The vector x(k) is called the kth state vector.\n",
    "\n",
    "Now, given that today is sunny, i.e., that S(0) = 1 and C(0) = 1, what is the probability that 8 days from now is sunny? We can answer this easily by iterating the system x(k+1) = P x(k) to compute x(8)!\n",
    "\n",
    "x(0) = [1], x(1) = Px(0) ≈ [.7], x(2) = Px(1) ≈ P²x(0) ≈ [.55]\n",
    "       [0]                [.3]                         [.45]\n",
    "\n",
    "x(3) = P³x(0) ≈ [.475], x(4) ≈ [.438], x(5) ≈ [.419], x(6) ≈ [.410]\n",
    "                [.525]         [.562]         [.581]         [.590]\n",
    "\n",
    "x(7) ≈ [.405], x(8) ≈ [.402]\n",
    "       [.595]         [.598]\n",
    "\n",
    "So we conclude that 40.2% of the time, if today is sunny, then 8 days from now is also sunny.\n",
    "\n",
    "We make a few observations about the state vectors x(k) to motivate some of the new tools we'll introduce:\n",
    "\n",
    "1) Every state vector x(k) is a probability vector, i.e., x₁(k) and x₂(k) ≥ 0 and x₁(k) + x₂(k) = 1\n",
    "2) The process converges fairly quickly to x* = [.4], which is a fixed\n",
    "                                                [.6]\n",
    "   point of x(k+1) = Px(k), i.e., x* = Px*\n",
    "3) This convergence to x* actually happens for any initial probability vector x(0). This means that in the long run, 40% of days are sunny and 60% are rainy.\n",
    "\n",
    "Let's try to understand why this happens, and then we'll look at some interesting applications of Markov chains.\n",
    "\n",
    "Our starting point is a general definition of a probability vector: a vector x ∈ ℝⁿ is called a probability vector if xᵢ ≥ 0 for i=1,...,n and x₁ + ... + xₙ = 1. We interpret xᵢ as the probability (or likelihood) that the system is in state i."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a Markov chain is given by the first order linear iterative system\n",
    "\n",
    "x(k+1) = P x(k)  (MC)\n",
    "\n",
    "whose initial state x(0) is a probability vector. The entries of the transition matrix P must satisfy\n",
    "\n",
    "0 ≤ pᵢⱼ ≤ 1  and  pᵢ₁ + ... + pᵢₙ = 1.  (TM)\n",
    "\n",
    "for all i,j=1,...,n. The entry pᵢⱼ is the transition probability that the system will switch from state j to state i. Because this covers all possible transitions, this means each column sums to 1. Under these conditions, we can guarantee that if x(k) is a probability vector, so is x(k+1) = Px(k). To see this, note that 1ᵀP = 1ᵀ[p₁₁ ... 1p₁ₙ] = [1 ... 1] = 1ᵀ so that 1ᵀx(k+1) = 1ᵀPx(k) = 1ᵀx(k) = 1. That x(k+1) is entrywise non-negative follows from P and x(k) being entry-wise non-negative.\n",
    "\n",
    "Next, let's investigate convergence properties. We first need to impose a very mild technical condition on the transition matrix P, namely we assume that it is regular: A transition matrix P (TM) is regular if for some power k, Pᵏ contains no zero entries. This means that it is possible to get from one state to any other state in k steps.\n",
    "\n",
    "The long-term behavior of a Markov chain with regular transition matrix P is governed by the Perron-Frobenius theorem, which we state next. The proof is quite involved, so we omit it, but if you're curious, check out the end of ALA §3.\n",
    "\n",
    "Theorem: If P is a regular transition matrix, then it admits a unique probability vector x* with eigenvalue λ₁=1. Moreover, a Markov chain with regular matrix P will converge to x*: x(k) → x* as k→∞\n",
    "\n",
    "This is a very exciting development! It tells us that we can understand the long-term behavior of a regular Markov chain by solving for the eigenvector x* associated with the eigenvalue λ₁=1 of P.\n",
    "\n",
    "Returning to our weather prediction example, we compute the steady state probability vector x* by just solving (P-I)v=0:\n",
    "\n",
    "(P-I)v = [-.3 .4][v₁] = 0 => v₁ = 2 v₂ => v = [⁴⁄₃]\n",
    "         [.3 -.4][v₂]                           [²⁄₃]\n",
    "\n",
    "and then normalizing v so that its entries add up to 1:\n",
    "\n",
    "x* = 1/(1+²⁄₃) [⁴⁄₃] = [⁴⁄₅] = [0.4]\n",
    "               [²⁄₃]   [²⁄₅]   [0.6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This special eigenvector x* tells us that no matter the initial state x(0), the long term behavior is that we are in State 1 (sunny) 40% of days and State 2 (cloudy) 60% of days.\n",
    "\n",
    "Example: Get out the vote!\n",
    "\n",
    "Suppose the voting results of a congressional election at a certain voting precinct are represented by a vector x ∈ ℝ³:\n",
    "\n",
    "x = [% voting Democratic (D)]\n",
    "    [% voting Republican (R)]\n",
    "    [% voting Libertarian (L)]\n",
    "\n",
    "We record the outcome of this election every two years by a vector of this type, and let's assume that the outcome of one election depends only on results of the previous one. Then the sequence x(k) of vectors that describe the votes in each election form a Markov chain. Suppose, using historical data, we estimate the following transition matrix P:\n",
    "\n",
    "        From:\n",
    "        D   R   L   To:\n",
    "    [.7  .1  .3] D\n",
    "P ≈ [.2  .8  .3] R\n",
    "    [.1  .1  .4] L\n",
    "\n",
    "The entries in the first column, labeled D, describe what % of persons who voted D in the last election will vote D, R, and L in this one: in this example, 70% of prior D voters will vote D again, 20% will vote R, and 10% will vote L.\n",
    "\n",
    "If we assume that P remains fixed across many elections, we can predict not only the next election's results, but long-term election results as well. For example, if last election had results:\n",
    "\n",
    "x(0) = [.55]\n",
    "       [.40]\n",
    "       [.05]\n",
    "\n",
    "then the next election will have a likely outcome of\n",
    "\n",
    "x(1) = P x(0) ≈ [.44]\n",
    "               [.445]\n",
    "               [.115]\n",
    "\n",
    "and the following election will have likely outcome\n",
    "\n",
    "x(2) ≈ Px(1) ≈ [.387]\n",
    "              [.475]\n",
    "              [.1345]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/07_Ch_8_Iteration/092-Markov_Chains.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
