{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "---\n",
    "title: \"8.4 Case Study: Implementing a Mini-Page-Rank\"\n",
    "subject:  Iteration\n",
    "subtitle: ranking websites\n",
    "short_title: \"8.4 Case Study: Implementing a Mini-Page-Rank\"\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: \n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/07_Ch_8_Iteration/093a-more-page-rank.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NxDAmJuPQXEG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Learning objectives\n",
    "\n",
    "In this case study, we will:\n",
    "* Walk through implementing the Page Rank algorithm on a toy internet\n",
    "* Understand through example its key mathematical design choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Surfing a Toy Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "Let's first quickly review the motivation behind Page Rank. Page Rank, as developed by Google cofounders Sergey Brin and Larry Page, is an algorithm to produce an, at least in its original form, *global* relevance ranking of websites. As we've seen in lecture, the key idea is that by continuously clicking links and going to new webpages, one can model a user's probability of landing at any given website as a random walk.\n",
    "\n",
    "Mathematically, sufficiently \"well-behaved\" random walks, more specifically [*regular* Markov Processes](./092-Markov_Chains.ipynb#regular_defn), are nice in that as one keeps iterating the transition matrix, the state vector converges to a fixed \"steady-state vector\", implying that it doesn't matter what webpage a user starts on initially, the long-run probability of visiting each website is the same. Of course, there are many pathologies that make these well-behaved Markov Processes a poor model of web usage, for example \"dangling nodes\", where a link leads to a webpage that has not outgoing links. Avoiding these pathologies is what makes Page Rank more than just computing the top eigenvector of a transition matrix.\n",
    "\n",
    "Let's start by downloading a toy internet with a total of six indexed webpages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Fe_Arts6QYST",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A helper function to download a file from a website\n",
    "try:\n",
    "    from urllib2 import urlopen\n",
    "except ImportError:\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "def progress_bar_downloader(url, fname, progress_update_every=5):\n",
    "    #from http://stackoverflow.com/questions/22676/how-do-i-download-a-file-over-http-using-python/22776#22776\n",
    "    u = urlopen(url)\n",
    "    f = open(fname, 'wb')\n",
    "    meta = u.info()\n",
    "    file_size = int(meta.get(\"Content-Length\"))\n",
    "    print(\"Downloading: %s Bytes: %s\" % (fname, file_size))\n",
    "    file_size_dl = 0\n",
    "    block_sz = 8192\n",
    "    p = 0\n",
    "    while True:\n",
    "        buffer = u.read(block_sz)\n",
    "        if not buffer:\n",
    "            break\n",
    "        file_size_dl += len(buffer)\n",
    "        f.write(buffer)\n",
    "        if (file_size_dl * 100. / file_size) > p:\n",
    "            status = r\"%10d  [%3.2f%%]\" % (file_size_dl, file_size_dl * 100. / file_size)\n",
    "            print(status)\n",
    "            p += progress_update_every\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ePS0ir_yQXEH",
    "outputId": "70727ebf-ffc1-4dba-d36e-95732a3bae2d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pages.zip already downloaded!\n"
     ]
    }
   ],
   "source": [
    "# Download a 6-page miniweb from a ubc course\n",
    "import os\n",
    "\n",
    "pages_link = 'http://www.cs.ubc.ca/~nando/340-2009/lectures/pages.zip'\n",
    "dlname = 'pages.zip'\n",
    "#This will unzip into a directory called pages\n",
    "if not os.path.exists('./%s' % dlname):\n",
    "    progress_bar_downloader(pages_link, dlname)\n",
    "    os.system('unzip %s' % dlname)\n",
    "else:\n",
    "    print('%s already downloaded!' % dlname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "Our website is now comprised of the following pages: \"angelinajolie.html\", \"bradpitt.html\", \"jenniferaniston.html\", \"jonvoight.html\", \"martinscorcese.html\", and \"robertdeniro.html\".\n",
    "\n",
    "Some sample webpages look like:\n",
    "```{figure} 093a-page-rank/angelina_jolie.png\n",
    ":width: 70%\n",
    ":alt: A sample webpage 1\n",
    ":align: \"center\"\n",
    "```\n",
    "\n",
    "```{figure} 093a-page-rank/jennifer_aniston.png\n",
    ":width: 70%\n",
    ":alt: A sample webpage 2\n",
    ":align: \"center\"\n",
    "```\n",
    "\n",
    "As we can see, these webpages contain various number of links to the other webpages in our small internet. Now we can trawl through and collect the links leaving each webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOH7TcwMQXEI"
   },
   "source": [
    "Building the Matrix\n",
    "-------------------\n",
    "\n",
    "To build the link matrix, we parse websites for outgoing links. For every page referenced by a page, we will add a 1 to the associated row. Adding a small term `eps` to all entries, in order guarantee the matrix is fully connected and renormalizing will ensure a valid link matrix with a unique solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "m5oCIDlxQXEI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links in robertdeniro.html: ['martinscorcese.html']\n",
      "Links in jenniferaniston.html: []\n",
      "Links in martinscorcese.html: []\n",
      "Links in jonvoight.html: ['angelinajolie.html', 'angelinajolie.html', 'bradpitt.html']\n",
      "Links in angelinajolie.html: ['jonvoight.html', 'bradpitt.html']\n",
      "Links in bradpitt.html: ['jenniferaniston.html', 'angelinajolie.html', 'martinscorcese.html', 'angelinajolie.html']\n"
     ]
    }
   ],
   "source": [
    "#Quick and dirty link parsing as per http://www.cs.ubc.ca/~nando/540b-2011/lectures/book540.pdf\n",
    "links = {}\n",
    "for fname in os.listdir(dlname[:-4]):\n",
    "    links[fname] = []\n",
    "    f = open(dlname[:-4] + '/' + fname)\n",
    "    for line in f.readlines():\n",
    "        while True:\n",
    "            p = line.partition('<a href=\"http://')[2]\n",
    "            if p == '':\n",
    "                break\n",
    "            url, _, line = p.partition('\\\">')\n",
    "            links[fname].append(url)\n",
    "    f.close()\n",
    "\n",
    "for key in links.keys():\n",
    "    print(f\"Links in {key}: {links[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "Graphically, our little internet looks like:\n",
    "```{figure} 093a-page-rank/celebrity_net.png\n",
    ":width: 50%\n",
    ":alt: internet graph\n",
    ":align: \"center\"\n",
    "A visual of the toy internet. Black bars indicate a link going *in* to a webpage.\n",
    "```\n",
    "\n",
    "To form a transition matrix associated with this (directed) graph, recall that the basic form of Page Rank assumes that when a web surfer is on a given page, they will click on any link with equal probability. With this assumption in mind, we can go through the link dictionary we formed just now and create the transition matrix in `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOh9bfVYQXEJ",
    "outputId": "6ffa6518-ea23-442f-9682-e3c01628cb57",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw transition matrix P\n",
      " [[0.         0.5        0.         0.66666667 0.         0.        ]\n",
      " [0.5        0.         0.         0.33333333 0.         0.        ]\n",
      " [0.         0.25       1.         0.         0.         0.        ]\n",
      " [0.5        0.         0.         0.         0.         0.        ]\n",
      " [0.         0.25       0.         0.         1.         1.        ]\n",
      " [0.         0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "num_pages = len(links.keys())\n",
    "P = np.zeros((num_pages, num_pages))\n",
    "\n",
    "#Assign identity numbers to each page, along with a reverse lookup\n",
    "idx = {}\n",
    "lookup = {}\n",
    "for n,k in enumerate(sorted(links.keys())):\n",
    "    idx[k] = n\n",
    "    lookup[n] = k\n",
    "\n",
    "#Go through all keys, and add a 1 for each link to another page\n",
    "for k in links.keys():\n",
    "    v = links[k]\n",
    "    for e in v:\n",
    "        P[idx[e],idx[k]] += 1\n",
    "    if len(v) > 0:\n",
    "        P[:, idx[k]] = P[:, idx[k]]/np.sum(P[:, idx[k]])\n",
    "    else:\n",
    "        # no outgoing links: user stays at current state\n",
    "        P[idx[k], idx[k]] = 1\n",
    "\n",
    "print('Raw transition matrix P\\n', P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "Unfortunately, there are some columns in the transition that are all $0$ apart from the diagonal entry, corresponding to webpages that have no outgoing links. This implies that the transition matrix in its current form is not a [regular transition matrix](./092-Markov_Chains.ipynb#regular_defn). To see why this is problematic, consider a web-user starting on page $3$, i.e. $\\bold x(1) = \\bm 0 & 0 & 1 & 0 & 0 & 0 \\em^\\top$. Since there are no outgoing links, the next state vector will be $\\bold x(2) = \\bold x(1)$, ensuring that it remains at $\\bold x(1)$ for all time. Similarly, since there are incoming links to these dead-end webpages, iterating the transition matrix will shift more and more weight from other webpages that have outgoing links to the dead-end links. \n",
    "\n",
    "Not only is this behavior undesirable--we don't want to put undue weight on websites just because they have no outgoing links--this also implies that the long-run $\\bold x(k)$ becomes dependent on the initial distribution. For example, starting at page 3 will have $\\bold x(k) = \\bm 0 & 0 & 1 & 0 & 0 & 0 \\em^\\top$ for all $k$, whereas starting at page 5 will have $\\bold x(k) = \\bm 0 & 0 & 0 & 0 & 1 & 0 \\em^\\top$ for all $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x(1) = Px = [0. 0. 1. 0. 0. 0.]\n",
      "x(2) = P^2 x = [0. 0. 1. 0. 0. 0.]\n",
      "\n",
      "Init x: [0.167 0.167 0.167 0.167 0.167 0.167]\n",
      "x(50) = [0.    0.    0.417 0.    0.583 0.   ]\n",
      "\n",
      "Init x: [0.1 0.1 0.5 0.  0.2 0.1]\n",
      "x(50) = [0.  0.  0.6 0.  0.4 0. ]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0,0,1,0,0,0])\n",
    "print(\"x(1) = Px =\", P@x)\n",
    "print(\"x(2) = P^2 x =\", P@P@x)\n",
    "\n",
    "x = np.ones(6)/6\n",
    "print(\"\\nInit x:\", np.round(x,3))\n",
    "for i in range(50):\n",
    "    # print(\"next x:\", G@x)\n",
    "    x = P@x\n",
    "    # print(\"Sum of entries:\",np.sum(x))\n",
    "print(\"x(50) =\", np.round(x,3))\n",
    "\n",
    "x = np.array([0.1, 0.1, 0.5, 0., 0.2, 0.1])\n",
    "print(\"\\nInit x:\", np.round(x,3) )\n",
    "for i in range(50):\n",
    "    # print(\"next x:\", G@x)\n",
    "    x = P@x\n",
    "    # print(\"Sum of entries:\",np.sum(x))\n",
    "print(\"x(50) =\", np.round(x,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "To summarize, naively computing the transition matrix is not sufficient for Page-Rank because:\n",
    "* Dead-end webpages (\"dangling nodes\") get unnaturally weighted due to not having outgoing links.\n",
    "* The ranking, i.e. sorted entries of the converged state vector, is therefore **not unique**.\n",
    "\n",
    "This motivates the first adjustment Page rank proposes:\n",
    ":::{note}Adjustment 1 \n",
    "If an internet user reaches a dangling node, they will pick any page on the web with equal probability and move to that page.\n",
    ":::\n",
    "Therefore, once a user reaches a dead-end, instead of staying there with 100% probability (which is an extremely poor model of reality anyways), Page Rank instead assumes the user resets and starts over at a completely random website. Of course, this random website is in practice not chosen uniformly at random across *every single website* out there, and the Page Rank Google used in production certainly did many proprietary optimizations upon this, but this adjustment suffices for our basic model.\n",
    "\n",
    "Is this adjustment enough? What happens when there are cycles? For example, let us consider a $4 \\times 4$ transition matrix (with no dangling nodes!):\n",
    "\\begin{align*}\n",
    "P = \\bm 0.5 & 0.5 & 0 & 0 \\\\ 0.5 & 0.5 & 0 & 0 \\\\ 0 & 0 & 0.5 & 0.5 \\\\ 0 & 0 & 0.5 & 0.5 \\em.\n",
    "\\end{align*}\n",
    "Notice there are no entries that cross between $1,2$ and $3,4$. Therefore, if the user starts on page $1$ or $2$, they will stay there perpetually, and similarly if they start on page $3$ or $4$. This also implies there is no unique long-run state probability vector, since $\\bold x(k)$ is once again dependent on the initial $\\bold x(1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P =  [[0.5 0.5 0.  0. ]\n",
      " [0.5 0.5 0.  0. ]\n",
      " [0.  0.  0.5 0.5]\n",
      " [0.  0.  0.5 0.5]]\n",
      "\n",
      "Init x: [1 0 0 0]\n",
      "x(50) = [0.5 0.5 0.  0. ]\n",
      "\n",
      "Init x: [0 0 1 0]\n",
      "x(50) = [0.  0.  0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "P0 = np.array([[0.5, 0.5, 0, 0],[0.5, 0.5, 0, 0],[0, 0, 0.5, 0.5],[0, 0, 0.5, 0.5]])\n",
    "print(\"P = \", P0)\n",
    "x = np.array([1, 0, 0, 0])\n",
    "print(\"\\nInit x:\", np.round(x,3) )\n",
    "for i in range(50):\n",
    "    # print(\"next x:\", G@x)\n",
    "    x = P0@x\n",
    "    # print(\"Sum of entries:\",np.sum(x))\n",
    "print(\"x(50) =\", np.round(x,3))\n",
    "\n",
    "x = np.array([0, 0, 1, 0])\n",
    "print(\"\\nInit x:\", np.round(x,3) )\n",
    "for i in range(50):\n",
    "    # print(\"next x:\", G@x)\n",
    "    x = P0@x\n",
    "    # print(\"Sum of entries:\",np.sum(x))\n",
    "print(\"x(50) =\", np.round(x,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "This pathology is in spirit the same as dangling nodes. Instead of dangling nodes, there may now be dangling *sets of nodes* that only link within each other. Therefore, an additional adjustment needs to be made to ensure these dangling communities do not occur, which brings us to the second adjustment.\n",
    "\n",
    ":::{note}Adjustment 2\n",
    "Pick a number $p$ between 0 and 1. If a user is on page $i$, then $p$ proportion of the time, they will pick from all possible hyperlinks on that page with equal probability and move to that page. The other $1-p$ fraction of the time, they will pick any page on the web with equal probability and move to that page.\n",
    ":::\n",
    "\n",
    "As discussed in lecture, this amounts to taking a weighted sum between the transition matrix and a matrix $K = [k_{ij}] \\in \\R^{n \\times n}$ where $k_{ij} = 1/n$ for all $i,j$.\n",
    "$$\n",
    "G = pP + (1-p)K,\n",
    "$$\n",
    "This all depends on the weighting parameter $p \\in (0,1)$, which discussed in class was approximately $0.85$ in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed link matrix G guaranteeing unique solution\n",
      " [[0.025      0.45       0.025      0.59166667 0.025      0.025     ]\n",
      " [0.45       0.025      0.025      0.30833333 0.025      0.025     ]\n",
      " [0.025      0.2375     0.875      0.025      0.025      0.025     ]\n",
      " [0.45       0.025      0.025      0.025      0.025      0.025     ]\n",
      " [0.025      0.2375     0.025      0.025      0.875      0.875     ]\n",
      " [0.025      0.025      0.025      0.025      0.025      0.025     ]]\n"
     ]
    }
   ],
   "source": [
    "G = np.zeros((num_pages, num_pages))\n",
    "for k in links.keys():\n",
    "    v = links[k]\n",
    "    for e in v:\n",
    "        G[idx[e],idx[k]] += 1\n",
    "    if len(v) > 0:\n",
    "        G[:, idx[k]] = G[:, idx[k]]/np.sum(G[:, idx[k]])\n",
    "    else:\n",
    "        # no outgoing links: user stays at current state\n",
    "        G[idx[k], idx[k]] = 1\n",
    "        \n",
    "eps = 1. / num_pages\n",
    "p = 0.85\n",
    "G = p * G + (1-p)*eps * np.ones((num_pages, num_pages))\n",
    "print('Processed link matrix G guaranteeing unique solution\\n', G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "This matrix $G$ should now permit a unique $x^\\star$ such that $x^\\star = G x^\\star$. To sanity check this, we can list the eigenvalues of $G$. If $x^\\star$ is unique, then the *multiplicity* of the eigenvalue $\\lambda = 1$ should be $1$ (why?). Let's check this using `np.linalg.eigvals`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigs of G: [ 1.     0.85   0.703 -0.    -0.126 -0.577]\n",
      "Eigs of original P: [ 1.     1.     0.827  0.    -0.148 -0.679]\n"
     ]
    }
   ],
   "source": [
    "G_eigs = np.linalg.eigvals(G)\n",
    "P_eigs = np.linalg.eigvals(P)\n",
    "\n",
    "print(\"Eigs of G:\", np.round(np.sort(G_eigs)[::-1],3))\n",
    "print(\"Eigs of original P:\", np.round(np.sort(P_eigs)[::-1],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "Therefore, we see that $G$ does indeed ensure a unique long-run $x^\\star$, whereas the original transition matrix does not. To solve for the corresponding $x^\\star$ that satisfies $x^\\star = G x^\\star$, we could just use `np.linalg.eig` to directly compute it. However, we note that this can be posed as a regression problem.\n",
    "\\begin{align*}\n",
    "    &x^\\star = G x^\\star \\\\\n",
    "    \\implies\\;& (I-G)x^\\star = \\boldsymbol 0.\n",
    "\\end{align*}\n",
    "The unique *non-zero* solution to the above problem is precisely $x^\\star$. However, we have to ensure the least-squares solver doesn't just default to the trivial solution $(I-G)\\boldsymbol 0 = \\boldsymbol 0$. A trick to avoid this is to *augment* the left and right hand sides of the equation. Define $A := I-G$. Then consider the augmented system formed by adding a row of $1$s to $A$ and a last entry of $1$ to the right-hand side.\n",
    "\\begin{align*}\n",
    "    \\bm &A & \\\\ 1 & \\cdots & 1  \\em \\bold x^\\star = \\bm \\boldsymbol 0 \\\\ 1 \\em.\n",
    "\\end{align*}\n",
    "One can verify that $\\bold x^\\star$ solving the original system also solves this system. Additionally, since this system is now \"tall\" a.k.a. overdetermined, the solution is guaranteed to be unique!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vSSUD0XGRly6",
    "outputId": "61a33598-0cf5-4755-88e5-6e43cf881dc9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented matrix with sum constraint\n",
      " [[ 0.975      -0.45       -0.025      -0.59166667 -0.025      -0.025     ]\n",
      " [-0.45        0.975      -0.025      -0.30833333 -0.025      -0.025     ]\n",
      " [-0.025      -0.2375      0.125      -0.025      -0.025      -0.025     ]\n",
      " [-0.45       -0.025      -0.025       0.975      -0.025      -0.025     ]\n",
      " [-0.025      -0.2375     -0.025      -0.025       0.125      -0.875     ]\n",
      " [-0.025      -0.025      -0.025      -0.025      -0.025       0.975     ]\n",
      " [ 1.          1.          1.          1.          1.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# In order to eliminate the trivial solution, we append the constraint\n",
    "# that sum(x) == 1\n",
    "ones = np.ones(shape=(1,num_pages))\n",
    "e_7 = np.zeros(shape=(num_pages+1,))\n",
    "e_7[-1] = 1\n",
    "Gg = np.vstack((np.eye(num_pages)-G,ones))\n",
    "print('Augmented matrix with sum constraint\\n', Gg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fL5wc2spQ9cc",
    "outputId": "ed1888e9-9e03-483e-c9ba-4d0f183a0bb4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_solve, residuals, _, _ = np.linalg.lstsq(Gg, e_7, rcond=None)\n",
    "\n",
    "# We verify that the residual is zero\n",
    "assert np.abs(residuals)<10e-12, f'Something went wrong, we have nonzero residual: {residual}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "We have therefore passed all the checks to output a *unique* page-rank vector. Let's check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHsSL83CROSg",
    "outputId": "c1f49dc9-2dc9-4336-e561-e346ca7921bf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our importance rankings are  [0.10012444 0.08669287 0.28948157 0.06755289 0.43114823 0.025     ]\n",
      "Our page rankings, from least to most important, are therefore  [5 3 1 0 2 4]\n",
      "This is the mapping between the indices and pages: \n",
      " {'angelinajolie.html': 0, 'bradpitt.html': 1, 'jenniferaniston.html': 2, 'jonvoight.html': 3, 'martinscorcese.html': 4, 'robertdeniro.html': 5}\n"
     ]
    }
   ],
   "source": [
    "print('Our importance rankings are ', p_solve)\n",
    "print('Our page rankings, from least to most important, are therefore ', np.argsort(p_solve))\n",
    "print('This is the mapping between the indices and pages: \\n', idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Wait, where does Search come into this?\n",
    "\n",
    "Having put quite some attention on Page Rank, one might be wondering: we've output a vector that measures the *global* relevance of websites on the internet. That's great, but where does Google's claim-to-fame Search come in? A global ranking doesn't seem to have any effect on returning personalized rankings relevant to a search query.\n",
    "\n",
    "This is where years of hyperoptimization and billions of dollars on Google's part comes in, but we can in fact build a primitive search engine without too much effort. A naive idea is the following:\n",
    "* Break down a search query to its component words\n",
    "* Restrict attention only webpages containing the component words\n",
    "* Return webpage therein with highest Page Rank score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrdUYJqbQXEK"
   },
   "source": [
    "Now let's search\n",
    "--------------------\n",
    "\n",
    "Now that the PageRank for each page is calculated, how can we actually perform a search? Note that this is pure programming/parsing, and not related to class content, but may still be of interest to you.\n",
    "\n",
    "We create an index of every word in a page. When we search for words, we will then sort the output by the PageRank of those pages, thus ordering the links by the *importance* we associated with that page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "WSo7wV3nQXEL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "search = {}\n",
    "for fname in os.listdir(dlname[:-4]):\n",
    "    f = open(dlname[:-4] + '/' + fname)\n",
    "    for line in f.readlines():\n",
    "        #Ignore header lines\n",
    "        if '<' in line or '>' in line:\n",
    "            continue\n",
    "        words = line.strip().split(' ')\n",
    "        words = filter(lambda x: x != '', words)\n",
    "        #Remove references like [1], [2]\n",
    "        words = filter(lambda x: not ('[' in x or ']' in x), words)\n",
    "\n",
    "    for word in words:\n",
    "        if word in search:\n",
    "            if fname in search[word]:\n",
    "                search[word][fname] += 1\n",
    "            else:\n",
    "                search[word][fname] = 1\n",
    "        else:\n",
    "            search[word] = {fname: 1}\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "Our search engine is not very sophisticated, taking only individual words, and we can only search for queries found within search.keys()--case must also match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kcqgge9f38mP",
    "outputId": "26d524d1-4d58-4ab8-f2a9-4a62ee7a82bc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Robert', 'Mario', 'De', 'Niro,', 'Jr.', '(born', 'August', '17,', '1943)', 'is', 'an', 'American', 'actor,', 'director,', 'and', 'producer.', 'She', 'has', 'starred', 'in', 'several', 'Hollywood', 'films.', 'While', 'most', 'of', 'her', 'film', 'roles', 'have', 'been', 'comedies', 'such', 'as', 'Bruce', 'Almighty,', 'Office', 'Space,', 'Rumor', 'Has', 'It,', 'the', 'romantic', 'Along', 'Came', 'Polly', 'The', 'Break-Up,', 'she', 'also', 'appeared', 'films', 'from', 'other', 'genres,', 'comedy-horror', 'Leprechaun', 'crime', 'thriller', 'Derailed.', \"Scorsese's\", 'body', 'work', 'addresses', 'themes', 'Italian', 'identity,', 'Roman', 'Catholic', 'concepts', 'guilt', 'machismo,', 'violence.', 'Scorsese', 'widely', 'considered', 'to', 'be', 'one', 'significant', 'influential', 'filmmakers', 'his', 'era,', 'directing', 'landmark', 'Taxi', 'Driver,', 'Raging', 'Bull', 'Goodfellas;', 'all', 'which', 'he', 'collaborated', 'on', 'with', 'actor', 'He', 'earned', 'Academy', 'Award', 'for', 'Best', 'Director', 'Departed', 'MFA', 'New', 'York', 'University', 'Tisch', 'School', 'Arts.', 'Jonathan', 'Vincent', '\"Jon\"', 'Voight', 'December', '29,', '1938)', 'television', 'actor.', 'had', 'a', 'long', 'distinguished', 'career', 'both', 'leading', 'man', 'and,', 'recent', 'years,', 'character', 'extensive', 'compelling', 'range.', 'came', 'prominence', 'at', 'end', '1960s,', 'performance', 'would-be', 'hustler', \"1969's\", 'Picture', 'winner,', 'Midnight', 'Cowboy,', 'first', 'nomination.', 'Throughout', 'following', 'decades,', 'built', 'reputation', 'array', 'challenging', 'Deliverance', '(1972),', 'Coming', 'Home', '(1978),', 'received', 'Actor.', \"Voight's\", 'impersonation', 'sportscaster/journalist', 'Howard', 'Cosell,', '2001', 'biopic', 'Ali,', 'critical', 'raves', 'fourth', 'Oscar', 'seventh', 'season', '24', 'villain', 'Jonas', 'Hodges.', 'Angelina', 'Jolie', 'Voight;', 'June', '4,', '1975)', 'actress', 'Goodwill', 'Ambassador', 'UN', 'Refugee', 'Agency.', 'three', 'Golden', 'Globe', 'Awards,', 'two', 'Screen', 'Actors', 'Guild', 'Award.', 'promoted', 'humanitarian', 'causes', 'throughout', 'world,', 'noted', 'refugees', 'through', 'UNHCR.', 'cited', \"world's\", 'beautiful', 'women', 'off-screen', 'life', 'Pitt', 'began', 'acting', 'guest', 'appearances,', 'included', 'role', 'CBS', 'soap', 'opera', 'Dallas', '1987.', 'gained', 'recognition', 'cowboy', 'hitchhiker', 'who', 'seduces', 'Geena', \"Davis's\", '1991', 'road', 'movie', 'Thelma', '&', 'Louise.', \"Pitt's\", 'big-budget', 'productions', 'A', 'River', 'Runs', 'Through', 'It', '(1992)', 'Interview', 'Vampire', '(1994).', 'was', 'cast', 'opposite', 'Anthony', 'Hopkins', '1994', 'drama', 'Legends', 'Fall,', 'him', 'In', '1995,', 'gave', 'critically', 'acclaimed', 'performances', 'Seven', 'science', 'fiction', 'Twelve', 'Monkeys,', 'latter', 'earning', 'Supporting', 'Actor', 'Four', 'years', 'later', '1999,', 'cult', 'hit', 'Fight', 'Club.', 'Subsequently', '2001,', 'major', 'international', \"Ocean's\", 'Eleven', 'its', 'sequels', '(2004)', 'Thirteen', '(2007).', 'biggest', 'commercial', 'successes', 'Troy', 'Mr.', 'Mrs.', 'Smith', '(2005).', 'second', 'nomination', 'title', '2008', 'Curious', 'Case', 'Benjamin', 'Button.'])\n",
      "\n",
      "\n",
      "Some values from search dict:\n",
      " [{'robertdeniro.html': 1, 'martinscorcese.html': 1}, {'robertdeniro.html': 1}, {'robertdeniro.html': 1, 'martinscorcese.html': 1}, {'robertdeniro.html': 1}, {'robertdeniro.html': 1}, {'robertdeniro.html': 1, 'jonvoight.html': 1, 'angelinajolie.html': 1}, {'robertdeniro.html': 1}, {'robertdeniro.html': 1}, {'robertdeniro.html': 1}, {'robertdeniro.html': 1, 'martinscorcese.html': 1, 'jonvoight.html': 1, 'angelinajolie.html': 3}, {'robertdeniro.html': 1, 'martinscorcese.html': 1, 'jonvoight.html': 4, 'angelinajolie.html': 2, 'bradpitt.html': 1}, {'robertdeniro.html': 1, 'martinscorcese.html': 2, 'jonvoight.html': 1, 'angelinajolie.html': 1}, {'robertdeniro.html': 1, 'jonvoight.html': 1}, {'robertdeniro.html': 1}, {'robertdeniro.html': 1, 'jenniferaniston.html': 3, 'martinscorcese.html': 5, 'jonvoight.html': 6, 'angelinajolie.html': 4, 'bradpitt.html': 6}, {'robertdeniro.html': 1}, {'jenniferaniston.html': 1, 'angelinajolie.html': 2}, {'jenniferaniston.html': 2, 'jonvoight.html': 3, 'angelinajolie.html': 3, 'bradpitt.html': 1}, {'jenniferaniston.html': 1, 'jonvoight.html': 1, 'bradpitt.html': 2}, {'jenniferaniston.html': 3, 'martinscorcese.html': 1, 'jonvoight.html': 5, 'bradpitt.html': 11}, {'jenniferaniston.html': 1}, {'jenniferaniston.html': 1}, {'jenniferaniston.html': 1}, {'jenniferaniston.html': 1}, {'jenniferaniston.html': 1, 'martinscorcese.html': 1, 'angelinajolie.html': 1}, {'jenniferaniston.html': 1, 'martinscorcese.html': 6, 'jonvoight.html': 4, 'angelinajolie.html': 1, 'bradpitt.html': 2}, {'jenniferaniston.html': 1, 'angelinajolie.html': 2}, {'jenniferaniston.html': 1, 'martinscorcese.html': 1, 'jonvoight.html': 1, 'bradpitt.html': 2}, {'jenniferaniston.html': 1, 'jonvoight.html': 1, 'bradpitt.html': 1}, {'jenniferaniston.html': 1}, {'jenniferaniston.html': 1, 'angelinajolie.html': 1}, {'jenniferaniston.html': 2}, {'jenniferaniston.html': 2, 'martinscorcese.html': 2, 'jonvoight.html': 1}, {'jenniferaniston.html': 2, 'martinscorcese.html': 2, 'jonvoight.html': 4, 'angelinajolie.html': 1, 'bradpitt.html': 1}, {'jenniferaniston.html': 1}, {'jenniferaniston.html': 1}, {'jenniferaniston.html': 1}, {'jenniferaniston.html': 1}, {'jenniferaniston.html': 1}, {'jenniferaniston.html': 1}, {'jenniferaniston.html': 1}, {'jenniferaniston.html': 3, 'martinscorcese.html': 4, 'jonvoight.html': 6, 'angelinajolie.html': 3, 'bradpitt.html': 13}, {'jenniferaniston.html': 1}, {'jenniferaniston.html': 1}, {'jenniferaniston.html': 1}, {'jenniferaniston.html': 1}, {'jenniferaniston.html': 1, 'martinscorcese.html': 1, 'bradpitt.html': 1}, {'jenniferaniston.html': 1}, {'jenniferaniston.html': 1}, {'jenniferaniston.html': 1}]\n"
     ]
    }
   ],
   "source": [
    "print(search.keys())\n",
    "print(\"\\n\")\n",
    "print(\"Some values from search dict:\\n\", list(search.values())[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "Therefore, inputting a search term into `search[]` will return the websites containing the term, from which we can the return the page with the highest Page Rank score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnRD9oATQXEM"
   },
   "source": [
    "Ranking The Results\n",
    "-------------------\n",
    "\n",
    "With words indexed, we can now complete the task. Searching for a particular word (in this case, 'film'), we get back all the pages with references and counts. Sorting these so that the highest pagerank comes first, we see the ***Googley***(TM) result for our tiny web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pT5LWZ09QXEM",
    "outputId": "b731fb3a-fff1-4b2e-d5ab-cf9dee4a57e5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages containing 'films': {'jenniferaniston.html': 1, 'martinscorcese.html': 1, 'jonvoight.html': 1}\n",
      "\n",
      "Associated Page Rank scores\n",
      "page: jenniferaniston.html, pagerank: 0.28948156780637474\n",
      "page: martinscorcese.html, pagerank: 0.4311482344730417\n",
      "page: jonvoight.html, pagerank: 0.06755288679953375\n",
      "Sorted: ['martinscorcese.html', 'jenniferaniston.html', 'jonvoight.html']\n"
     ]
    }
   ],
   "source": [
    "def get_pr(fname):\n",
    "    print(f'page: {fname}, pagerank: {p_solve[idx[fname]]}')\n",
    "    return p_solve[idx[fname]]\n",
    "\n",
    "r = search['films']\n",
    "print(f\"Pages containing 'films': {r}\\n\")\n",
    "\n",
    "print(\"Associated Page Rank scores\")\n",
    "\n",
    "print(\"Sorted:\", sorted(r, reverse=True, key=get_pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Concluding remarks\n",
    "\n",
    "We've now seen a prototype to Google's Page Rank algorithm, as well as the various pathologies that motivated its various components. However, many parts of the algorithm we described leave many facets underexplored. For example, people certainly don't reset to a completely random website, and will usually default to well-known domains like Wikipedia or Reddit. Also, how does one incorporate semantic meaning when considering search queries ([Case Study 3.3 perhaps](../02_Ch_3_Inner_Products_and_Norms/042a-NLP-case-study.ipynb))? As food for thought, now knowing the idea behind Page Rank, come up with a few ideas of how Page Rank can be be exploited. This is a prelude to the decades-long Search Engine Optimization (SEO) saga."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
