{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 8.3 Page Rank and the Google Matrix\n",
    "subject:  Iteration\n",
    "subtitle: ranking websites\n",
    "short_title: 8.3 Page Rank and the Google Matrix\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: \n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/07_Ch_8_Iteration/093-Random_Walks.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 15 - Linear Iterative Systems, Matrix Powers, Markov Chains, and Googleâ€™s PageRank.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be found in LAA $5^{th}$ edition 10.2.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- what is a random walk on a graph\n",
    "- how a simple internet is modeled as a random walk on a graph\n",
    "- how ranking webpages helps internet users\n",
    "- the primary modifications made by Google to address the non-regularity of transition matrices that model links between webpages\n",
    "- the magnanimity of Google's transition matrix and how they handle them"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Walk on the Internet\n",
    "\n",
    "An internet user's behavior while surfing the web can be modeled as a Markov chain that captures a _random walk on a graph_. We start with a simple example of this, and then explain how it can be used to design a search engine.\n",
    "\n",
    "Consider the following graph:\n",
    "\n",
    ":::{figure}../figures/08-graph_google.jpg\n",
    ":label:graph_google\n",
    ":alt:Graph\n",
    ":width: 250px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    "which has seven vertices interconnected by edges. Let's pretend this graph models a very simple internet! each vertex, or node, is a webpage, and each edge is a hyperlink connecting pages to each other. (For now we assume that if page $i$ links to page $j$, then page $j$ also links to page $i$, but this isn't necessary.) We assume that if a user is on page $i$, they will click on one of the hyperlinks with equal probability. For example, in our simple internet, if a user is on page 5, they will visit page 2 next 50\\% of the time and page 6 next 50\\% of the time. Similarly, if a user is on page 3, they will visit page 1, 2, or 4 next 33\\% of the time each.\n",
    "\n",
    "We can model this user behavior using a Markov chain, which is called a _simple random walk on a graph_. The transition matrix for this graph is given by:\n",
    "\n",
    "$$\n",
    "& \\quad \\quad  \\begin{matrix}\n",
    "1 & 2 & \\ 3 & 4 & 5 & 6 & 7 \\end{matrix} \\\\\n",
    "P &= \\begin{bmatrix}\n",
    "0 & \\frac{1}{3} & \\frac{1}{4} & 0 & 0 & 0 & 0 \\\\\n",
    "\\frac{1}{2} & 0 & \\frac{1}{4} & 0 & \\frac{1}{2} & 0 & 0 \\\\\n",
    "\\frac{1}{2} &  \\frac{1}{3} & 0 & 1 & 0 &  \\frac{1}{3} & 0 \\\\\n",
    "0 & 0 & \\frac{1}{4} & 0 & 0 & 0 & 0 \\\\\n",
    "0 &  \\frac{1}{3} & 0 & 0 & 0 &  \\frac{1}{3} & 0 \\\\\n",
    "0 & 0 & \\frac{1}{4} & 0 & \\frac{1}{2} & 0 & 1 \\\\\n",
    "0 & 0 & 0 & 0 & 0 &  \\frac{1}{3} & 0\n",
    "\\end{bmatrix} \n",
    "\\begin{matrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7\\end{matrix} \n",
    "$$\n",
    "\n",
    "This allows us to answer questions such as the following: suppose 100 users start on page 6. After each user clicks on three hyperlinks, what \\% of users do we expect to find on each web page. The solution is given by setting our initial user distribution to:\n",
    "\n",
    "$$\n",
    "\\vv x(0) = \\begin{bmatrix}\n",
    "0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and computing $\\vv x(3) = P^3\\vv x(0) = \\begin{bmatrix}\n",
    ".0833 \\\\ .0417 \\\\ .4028 \\\\ 0 \\\\ .2778 \\\\ 0 \\\\ .1944\n",
    "\\end{bmatrix}$\n",
    "\n",
    "This tells, for example, that 40.28\\% of users starting on page 6 end up on page 3 after 3 hyperlink clicks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Rank\n",
    "\n",
    "The founders of Google, Sergey Brin and Lawrence Page, reasoned that important pages had links coming from other \"important\" pages, and thus, a typical internet user would spend more time on more important pages, and less time on less important pages. This can be captured by the steady state distribution $\\vv x^*$ of the Markov chain we are using to model the internet: in the long run, a typical user will spend $\\vv x^*_i$ \\% of their time on page $i$. This is precisely the observation used to define the Page Rank algorithm, which Google uses to rank the importance of the webpages it catalogs.\n",
    "\n",
    ":::{important} Key Idea\n",
    "The importance of a webpage $i$ can be measured by the corresponding entry $\\vv x^*_i$ of the steady state vector $\\vv x^*$ of the Markov chain describing the behavior of a typical internet user.\n",
    ":::\n",
    "\n",
    "Now, if a typical transition matrix $P$ describing the internet were [regular](./092-Markov_Chains.ipynb#regular_defn), we would be done - we simply compute $\\vv x^* = P\\vv x^*$ and use $\\vv x^*$ to rank websites. Unfortunately, typical models of the web are [directed graphs](https://en.wikipedia.org/wiki/Directed_graph), which lead to non-regular transition matrices $P$. To address this, Google makes two adjustments, which we illustrate with the following slight modification of our previous example:\n",
    "\n",
    ":::{figure}../figures/08-directed_graph.jpg\n",
    ":label:directed_graph\n",
    ":alt:Directed Graph\n",
    ":width: 250px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    "$$\n",
    "& \\quad \\quad  \\begin{matrix}\n",
    "1 & 2 & \\ 3 & 4 & 5 & 6 & 7 \\end{matrix} \\\\\n",
    "P &= \\begin{bmatrix}\n",
    "0 & \\frac{1}{2} & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & \\frac{1}{3} & 0 & \\frac{1}{2} & 0 & 0 \\\\\n",
    "1 &  0 & 0 & 0 & 0 &  \\frac{1}{3} & 0 \\\\\n",
    "0 & 0 & \\frac{1}{3} & 1 & 0 & 0 & 0 \\\\\n",
    "0 &  \\frac{1}{2} & 0 & 0 & 0 &  \\frac{1}{3} & 0 \\\\\n",
    "0 & 0 & \\frac{1}{3} & 0 & \\frac{1}{2} & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 &  \\frac{1}{3} & 1\n",
    "\\end{bmatrix} \n",
    "\\begin{matrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7\\end{matrix} \n",
    "$$\n",
    "\n",
    "The first issue that arises here is that pages 4 and 7 are dangling nodes: if a user ever ends up on page 4 or 7, they never leave as there are no outgoing links. This means that columns 4 and 7 never change as we compute $P^k$, and hence $P$ cannot be regular. To handle dangling nodes, the following adjustment is made:\n",
    "\n",
    ":::{note}Adjustment 1 \n",
    "If an internet user reaches a dangling node, they will pick any page on the web with equal probability and move to that page.\n",
    ":::\n",
    "\n",
    "The above means that if state $j$ is an absorbing state, we replace column $j$ of $P$ with the vector $\\bm \\frac{1}{n} & \\ldots & \\frac{1}{n} \\em$. For example, our modified transition matrix is now:\n",
    "\n",
    "$$\n",
    "& \\quad \\quad  \\begin{matrix}1 & 2 & \\ 3 & 4 & 5 & 6 & 7 \\end{matrix} \\\\\n",
    "P_{*} &= \\begin{bmatrix}\n",
    "0 & \\frac{1}{2} & 0 & \\frac{1}{7} & 0 & 0 & \\frac{1}{7} \\\\\n",
    "0 & 0 & \\frac{1}{3} & \\frac{1}{7} & \\frac{1}{2} & 0 & \\frac{1}{7} \\\\\n",
    "1 & 0 & 0 & \\frac{1}{7} & 0 & \\frac{1}{3} & \\frac{1}{7} \\\\\n",
    "0 & 0 & \\frac{1}{3} &\\frac{1}{7} & 0 & 0 & \\frac{1}{7} \\\\\n",
    "0 & \\frac{1}{2} & 0 &\\frac{1}{7} & 0 & \\frac{1}{3} & \\frac{1}{7} \\\\\n",
    "0 & 0 & \\frac{1}{3} & \\frac{1}{7} & \\frac{1}{2} & 0 & \\frac{1}{7} \\\\\n",
    "0 & 0 & 0 & \\frac{1}{7} & 0 & \\frac{1}{3} & \\frac{1}{7}\n",
    "\\end{bmatrix}\n",
    "\\begin{matrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7\\end{matrix} \n",
    "$$\n",
    "\n",
    "While this helps eliminate dangling nodes, we may still not have a regular transition matrix, as there might still be **cycles** of pages. For example, if page $i$ only links to page $j$, and page $j$ only links to page $i$, a user entering either page is condemned to moving back and forth between the two pages. This means the corresponding columns of $P_*^k$ will always have zeros in them, and hence $P_*$ would not be regular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ":::{note}Adjustment 2\n",
    "Pick a number $p$ between 0 and 1. If a user is on page $i$, then $p$ proportion of the time, they will pick from all possible hyperlinks on that page with equal probability and move to that page. The other $1-p$ fraction of the time, they will pick any page on the web with equal probability and move to that page.\n",
    ":::\n",
    "\n",
    "In terms of the modified transition matrix $P_*$, the new transition matrix will be\n",
    "\n",
    "$$\n",
    "G = pP_* + (1-p)K,\n",
    "$$\n",
    "\n",
    "where $K \\in \\mathbb{R}^{n \\times n}$ and $K_{ij} = \\frac{1}{n}$ for $i,j=1,\\ldots,n$. The matrix $G$ is called the _Google matrix_. $G$ is easily seen to be regular, as all entries of $G$ are positive.\n",
    "\n",
    "Although any value of $p$ is valid, Google is thought to use $p = 0.85$. For our example, the Google matrix is\n",
    "\n",
    "$$\n",
    "G &= .85\\begin{bmatrix}\n",
    "0 & \\frac{1}{2} & 0 & \\frac{1}{7} & 0 & 0 & \\frac{1}{7} \\\\\n",
    "0 & 0 & \\frac{1}{3} & \\frac{1}{7} & \\frac{1}{2} & 0 & \\frac{1}{7} \\\\\n",
    "1 & 0 & 0 & \\frac{1}{7} & 0 & \\frac{1}{3} & \\frac{1}{7} \\\\\n",
    "0 & 0 & \\frac{1}{3} &\\frac{1}{7} & 0 & 0 & \\frac{1}{7} \\\\\n",
    "0 & \\frac{1}{2} & 0 &\\frac{1}{7} & 0 & \\frac{1}{3} & \\frac{1}{7} \\\\\n",
    "0 & 0 & \\frac{1}{3} & \\frac{1}{7} & \\frac{1}{2} & 0 & \\frac{1}{7} \\\\\n",
    "0 & 0 & 0 & \\frac{1}{7} & 0 & \\frac{1}{3} & \\frac{1}{7}\n",
    "\\end{bmatrix} \n",
    "+ .15\\begin{bmatrix}\n",
    "\\frac{1}{7}  & \\frac{1}{7} & \\frac{1}{7}  & \\frac{1}{7}  & \\frac{1}{7}  & \\frac{1}{7}  & \\frac{1}{7}  \\\\\n",
    "\\frac{1}{7}  & \\frac{1}{7} & \\frac{1}{7}  & \\frac{1}{7}  & \\frac{1}{7}  & \\frac{1}{7}  & \\frac{1}{7} \\\\\n",
    "\\frac{1}{7}  & \\frac{1}{7} & \\frac{1}{7}  & \\frac{1}{7}  & \\frac{1}{7}  & \\frac{1}{7}  & \\frac{1}{7} \\\\\n",
    "\\frac{1}{7}  & \\frac{1}{7} & \\frac{1}{7}  & \\frac{1}{7}  & \\frac{1}{7}  & \\frac{1}{7}  & \\frac{1}{7}  \\\\\n",
    "\\frac{1}{7}  & \\frac{1}{7} & \\frac{1}{7}  & \\frac{1}{7}  & \\frac{1}{7}  & \\frac{1}{7}  & \\frac{1}{7}\n",
    "\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}\n",
    ".021429 & .446429 & .021429 & .142857 & .021429 & .021429 & .142857 \\\\\n",
    ".021429 & .021429 & .304762 & .142857 & .446429 & .021429 & .142857 \\\\\n",
    ".871429 & .021429 & .021429 & .142857 & .021429 & .304762 & .142857 \\\\\n",
    ".021429 & .021429 & .304762 & .142857 & .021429 & .021429 & .142857 \\\\\n",
    ".021429 & .446429 & .021429 & .142857 & .021429 & .304762 & .142857 \\\\\n",
    ".021429 & .021429 & .304762 & .142857 & .446429 & .021429 & .142857 \\\\\n",
    ".021429 & .021429 & .021429 & .142857 & .021429 & .304762 & .142857\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can now compute the steady state vector $\\vv x^* = G\\vv x^*$, which is found to be:\n",
    "\n",
    "$$\n",
    "\\vv x^* = \\begin{bmatrix}\n",
    ".116293 \\\\\n",
    ".168567 \\\\\n",
    ".191263 \\\\\n",
    ".098844 \\\\\n",
    ".164054 \\\\\n",
    ".168567 \\\\\n",
    ".092413\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Thus, we can rank the pages in terms of descending importance as: 3, 2, 6, 5, 1, 4, 7.\n",
    "\n",
    ":::{note}\n",
    "The Google matrix for the world wide web has over 8 billion rows and columns, and computing $\\vv x^* = G \\vv x^*$ is a very non-trivial task. An iterative approach known as the _power method_ is used in practice, and typically takes several days for Google to compute a new $\\vv x^*$, which it does every month. \n",
    "\n",
    "Some resources for the power method:\n",
    "\n",
    "1. https://www.mpp.mpg.de/~jingliu/ECPI/PowerMethodProof.pdf\n",
    "2. https://en.wikipedia.org/wiki/Power_iteration\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/07_Ch_8_Iteration/093-Random_Walks.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
