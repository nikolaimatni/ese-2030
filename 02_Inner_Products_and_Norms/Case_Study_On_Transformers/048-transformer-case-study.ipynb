{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1940a536-1a33-471c-b6c8-b4bbb78f114c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "title: Understanding Transformers\n",
    "subject: Applied Linear Algebra\n",
    "subtitle: Building intuition for Attention Mechanisms\n",
    "short_title: Understanding Transformers\n",
    "authors:\n",
    "  - name: Renukanandan Tumu\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nandant@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: sample notes, ese 2030, linear algebra\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a417db5-0891-4e82-9c09-89ca997c687c",
   "metadata": {},
   "source": [
    "## What are transformers?\n",
    "\n",
    "Transformers are a popular architecture for machine-learning models today, powering most notably, [Large Language Models (LLMs)](https://openai.com/index/better-language-models/). We will examine how the inner product makes up its core."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cc9bd4-0ed7-4a40-8770-3d6f5603b763",
   "metadata": {},
   "source": [
    "First, let's look at the archictecture itself. The Transformer was first proposed by Vaswani et. al. in [*Attention is all you need*](https://arxiv.org/abs/1706.03762v7). At the core of the transformer is the Attention Mechanism. \n",
    "\n",
    "The Transformer was originally proposed to help with machine translation, to be able to translate large documents like books from one language to another. This is one of the core topics of study of Natural Language Processing or NLP. At the time the Transformer was developed, the state of the art approach was the [LSTM](https://dl.acm.org/doi/10.1162/neco.1997.9.8.1735). Unfortunately when translating an entire book, these approaches were unable to keep context from earlier in the book, or even in a sentence. This presented challenges when translating from languages that flip the order of the verb and the subject.\n",
    "\n",
    "## Machine Translation Basics\n",
    "\n",
    "In order for us to try operating on words, we need to find mathematical representations for them. The way that words are represented in Machine Translation is through vectors. An example word 'with' is below:\n",
    "\n",
    "```{math}\n",
    "\\text{with} \\mapsto\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "```\n",
    "We can build dictionaries that consist of these vectors, and create a library of these keys $K$. We can also have a set of vectors that represent our target language, in a set of values $V$. For our example, $K$ can have a set of vectors that represent the meaning of some words, like 'with', 'above', and 'below'. If we are translating to french, we would use the corresponding words in French 'avec', 'au-dessus', 'ci-dessous'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f54a41e-fbd7-4d9c-9bf1-76e0c68ce849",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What does Attention do?\n",
    "\n",
    "Attention helps us relate words from our query $Q$, to a set of keys $K$, into a domain of values $V$. In the case we just described, if we query for the word 'with', this would match with the key for 'with' in the matrix $K$. Finally, this would match with the value $V$ for 'avec', which is the french translation of 'with'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c89948-1dff-4736-ac42-c7d6fcf86c73",
   "metadata": {},
   "source": [
    "Let's look at the formula for Attention.\n",
    "\n",
    ":::{prf:definition} Attention Definition\n",
    "$$ \\text{Attention}(Q, K, V) =  \\text{softmax}\\left(\\frac{QK^T}{d_k}\\right)V$$\n",
    "$Q$ is the query, $K$ are the keys, and $V$ are values, and $d_k$ is the dimension of the keys\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e37df7f-0eb4-4bf1-a5be-edf172267fb2",
   "metadata": {},
   "source": [
    "## Worked Example\n",
    "### Definitions\n",
    "```{math}\n",
    "\\begin{gather}\n",
    "Q = \\text{with} = \\begin{bmatrix}\n",
    "0 &\n",
    "1 &\n",
    "0\n",
    "\\end{bmatrix}\\\\\n",
    "\n",
    "K = \\begin{bmatrix}\n",
    "\\text{above} &\n",
    "\\text{below} &\n",
    "\\text{with}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\n",
    "V = \\begin{bmatrix}\n",
    "\\text{au-dessus} &\n",
    "\\text{ci-dessous} &\n",
    "\\text{avec}\n",
    "\\end{bmatrix}\n",
    "\\end{gather}\n",
    "```\n",
    "\n",
    "\n",
    "### Numerator\n",
    "Let's examine this definition in parts. First let's consider the numerator $QK^T$. This is an inner product, which we can interpret as the similarity between query and vectors in the keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fce4479e-ba36-4a2e-9ec6-f2404006332c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "Q = np.array([0,1,0])\n",
    "K = np.array([\n",
    "    [1,0,0],\n",
    "    [0,0,1],\n",
    "    [0,1,0]\n",
    "])\n",
    "Q@K.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71414b22-3ff7-4271-9774-edac77eff327",
   "metadata": {},
   "source": [
    "We can see that the selected key corresponds to the most similar word in the dictionary.\n",
    "\n",
    "### Normalization and Softmax\n",
    "\n",
    "The next part of the formula is: $$ \\text{softmax}\\left(\\frac{QK^T}{d_k}\\right) $$ has two parts. First, we can look at the normalization constant, which is the dimension of the keys. $d_t=3$. The authors of the paper found that this constant aided in training. The next part of this is the softmax. In a vector with $K$ dimensions, the function is defined as:\n",
    "\n",
    "$$\n",
    "\\text{softmax}\\left(z\\right)_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\n",
    "$$\n",
    "\n",
    "This generates a vector which sums to one, with elements between zero and one. One way to think about a softmax is that it is an approximation of the maximum function that is differentiable.\n",
    "\n",
    "The value we get at the end is:\n",
    "\n",
    "$$\\begin{bmatrix}0,0,1\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "### Multiplying by Value\n",
    "\n",
    "Finally, we multiply the value from above with $V$, and get the translated word $\\text{avec}$!\n",
    "\n",
    "## Looking at real words\n",
    "To do this, we will use a library `gensim` which provides a pretrained set of word embeddings. We will download word embeddings from twitter for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d0a6d1e-43c7-4301-a0a0-87f2ea533128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89469254-ab20-4190-a01c-f16e754dd21e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98231aa-d8ca-4163-895b-920847efa05f",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can look up words, like screen, television, and remote using the function calls below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47d64856-c65e-463d-a3c9-81578e06634e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Screen:  [ 0.31098   -0.53336    0.9123    -0.15256    0.89117   -0.10692\n",
      "  0.78658   -0.19013    0.93285    0.52754    0.31475    0.63839\n",
      " -3.4433    -0.65918    0.055317   1.3083     0.4009     0.0071616\n",
      " -0.27728    0.017544  -0.86985   -0.60072    1.3789     0.25096\n",
      " -1.4383   ]\n",
      "rock:  [ 0.021641 -0.54484   0.78102  -0.10052   0.10482   0.53319   1.0586\n",
      "  0.90491   0.2594   -0.73547  -0.12972  -0.59679  -3.4979   -0.61679\n",
      " -0.15259   0.072474 -0.45453  -0.14681   0.09392   0.32735  -0.68834\n",
      " -0.020972  0.21344  -0.63178   1.3292  ]\n",
      "Remote:  [ 0.1125    0.38049   0.40254   0.089511  0.58018   0.067418  0.41842\n",
      " -0.46138   1.8756    0.99621   0.19743   0.27248  -2.7099   -0.65497\n",
      "  0.54752   1.0199    0.58964   0.1559    0.93753  -0.28045  -0.8659\n",
      " -0.88299   0.85855  -0.39055  -1.0604  ]\n"
     ]
    }
   ],
   "source": [
    "screen = glove_vectors['screen']\n",
    "rock = glove_vectors['rock']\n",
    "remote = glove_vectors['remote']\n",
    "print('Screen: ',screen)\n",
    "print('rock: ',rock)\n",
    "print('Remote: ',remote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb723747-191d-4a79-b0da-c69b51703e19",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we can calculate the cosine similarity based on the formula:\n",
    "$$\n",
    "w\\cdot v = \\|w\\| \\|v\\| \\cos(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ab17641-8c27-4ebe-811f-3c6fdd66826c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5682209"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screen@rock/(np.linalg.norm(screen)*np.linalg.norm(rock))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47428b03-a649-4f7b-814d-8b44b729547d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4561119"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote@rock/(np.linalg.norm(remote)*np.linalg.norm(rock))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4dd8837-5a02-4203-9a08-4de2a2e52a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86155677"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screen@remote/(np.linalg.norm(screen)*np.linalg.norm(remote))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea3afe0-79e2-4ff4-80b9-e723b60dbc2d",
   "metadata": {},
   "source": [
    "Based on this dataset, we can see that screen and remote are the most similar, remote and rock are the least similar, and screen and rock are not very similar either. The library also provides the ability to search the database for the most similar words to a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28702a77-df2e-47de-b436-f5597a0236d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('keyboard', 0.9022769331932068),\n",
       " ('monitor', 0.9000157713890076),\n",
       " ('plug', 0.8990083932876587),\n",
       " ('engine', 0.8985315561294556),\n",
       " ('switch', 0.896084725856781),\n",
       " ('device', 0.888617753982544),\n",
       " ('charger', 0.8855589032173157),\n",
       " ('counter', 0.8843674063682556),\n",
       " ('automatic', 0.878197431564331),\n",
       " ('sync', 0.877406656742096)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors.most_similar('remote')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42e962c-2028-4f4f-9bc7-30c50d3d4283",
   "metadata": {},
   "source": [
    "The most similar words to 'remote' in this dataset are shown above. Dot products and cosine similarity are key components of technologies which underpin the latest innovations in Large Language Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c152e-e653-4e40-a559-23bf18c1b81d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
