{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 12.1 Applications\n",
    "subject:  Optimization\n",
    "subtitle: \n",
    "short_title: 12.1 Applications\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: \n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/10_Ch_11_PCA_Apps/121-Apps.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 21 - An introduction to unconstrained optimization, gradient descent, and Newtonâ€™s method.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\textbf{A Brief Introduction to Optimization}\n",
    "\n",
    "There were a few times during the semester where we tried to find the\n",
    "``best'' vector (or matrix) among a collection of many such vectors or matrices.\n",
    "For example, in least squares, we looked to find the vector $x$ that minimized\n",
    "the expression $\\|Ax - b\\|^2$. In low-rank approximation, we looked to find the\n",
    "matrix $\\hat{M}$ that has rank k (or less) that minimized the expression $\\|\\hat{M} - M\\|_F^2$.\n",
    "\n",
    "These were both specific instances of what is called a \\textit{mathematical optimization\n",
    "problem}. We will focus on \\textit{unconstrained problems}. You will learn a lot more\n",
    "about optimization problems in ESE 3210, and today is only meant to give you\n",
    "a small taste, and to show how essential linear algebra is in finding solutions.\n",
    "\n",
    "Optimization is the process of finding one (or more) vectors $x \\in \\mathbb{R}^n$ that minimize\n",
    "a function $f: \\mathbb{R}^n \\to \\mathbb{R}$. This is written as the optimization problem\n",
    "\n",
    "\\begin{center}\n",
    "minimize $f(x)$ \\hspace{1cm} (P)\n",
    "\\end{center}\n",
    "\n",
    "Here, $x \\in \\mathbb{R}^n$. In (P), the variable $x \\in \\mathbb{R}^n$ is called the \\textit{decision variable},\n",
    "and the function $f: \\mathbb{R}^n \\to \\mathbb{R}$ is called the \\textit{cost function} or \\textit{objective function}.\n",
    "Optimization problem (P) is called \\textit{unconstrained} because we are free to pick any\n",
    "$x \\in \\mathbb{R}^n$ we like to minimize $f(x)$. A constrained optimization problem has the additional\n",
    "requirement that $x$ must satisfy some added conditions, e.g., lie in the solution set\n",
    "of $Ax = b$. We will not consider such problems today, but you'll see many in\n",
    "ESE 3210.\n",
    "\n",
    "The goal of optimization is to find a special decision variable $x^*$ for which\n",
    "the cost function $f$ is as small as possible, i.e., such that\n",
    "\n",
    "\\begin{equation}\n",
    "f(x^*) \\leq f(x) \\quad \\text{for all } x \\in \\mathbb{R}^n \\quad (*)\n",
    "\\end{equation}\n",
    "\n",
    "Such an $x^*$ is called an \\textit{optimal solution} to problem (P), and is defined as\n",
    "the arg min of $f$:\n",
    "\n",
    "\\begin{equation}\n",
    "x^* = \\arg\\min_{x \\in \\mathbb{R}^n} f(x). \\quad (AM)\n",
    "\\end{equation}\n",
    "\n",
    "Equation (AM) simply says in math that $x^*$ satisfies the definition (*) of\n",
    "an optimal point. [Note that if there are multiple optimal points, we instead write\n",
    "\n",
    "\\begin{equation}\n",
    "x^* \\in \\arg\\min_{x \\in \\mathbb{R}^n} f(x)\n",
    "\\end{equation}\n",
    "\n",
    "to indicate $x^*$ belongs to the set of optimal points. Pedantic.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\textbf{Example: The least-squares problem}\n",
    "\n",
    "\\begin{center}\n",
    "minimize $\\|Ax - b\\|^2$\n",
    "\\end{center}\n",
    "\n",
    "over $x \\in \\mathbb{R}^n$ is an unconstrained optimization problem. The objective function is\n",
    "$f(x) = \\|Ax - b\\|^2$, and the optimal solution is\n",
    "\n",
    "\\begin{equation}\n",
    "x^* = (A^TA)^{-1}A^Tb = A^{\\dagger}b\n",
    "\\end{equation}\n",
    "\n",
    "when $(A^TA)^{-1}$ exists. Otherwise, $x^* \\in \\arg\\min \\|Ax - b\\|^2 \\Leftrightarrow A^TAx^* = A^Tb$, i.e.,\n",
    "if and only if $x^*$ satisfies the normal equations $A^TAx = A^Tb$.\n",
    "\n",
    "Despite how simple and innocuous problem (P) looks, it can be used to encode\n",
    "very rich and very challenging problems. Even for $x \\in \\mathbb{R}$, we can get ourselves\n",
    "into trouble. Consider the following two functions that we wish to minimize:\n",
    "\n",
    "[Two graphs are drawn here, labeled $f_1(x)$ and $f_2(x)$]\n",
    "\n",
    "Which do you think is easier to optimize? The left figure, with function $f_1(x)$, is\n",
    "\"bowl-shaped\" and is smallest at $x^* = 2$. What's nice about $f_1$ is that there's also an obvious\n",
    "algorithm for finding $x^* = 2$: If you imagine yourself as an ant standing on the function\n",
    "$f_1(x)$, all you need to do is \"walk downhill\" until you eventually find the bottom of the bowl.\n",
    "\n",
    "In contrast, the right figure with function $f_2(x)$, there are many hills and valleys. The\n",
    "optimal value $x^* = 3$ is the one for which $f_2(x)$ is smallest. But now if we again imagine\n",
    "ourselves as an ant standing on the function $f_2(x)$, our strategy of walking downhill will not\n",
    "always work! For example, if we were to start at $x = 1.5$, then walking downhill would bring\n",
    "us to the bottom of the first valley at $x = 1$. Now $x = 1$ is not an optimal point,\n",
    "since $f_2(3) < f_2(1)$, but from our ant's perspective, $f_2(1)$ appears to be the\n",
    "smallest. We call such a point $\\tilde{x}$ that satisfies $f(\\tilde{x}) \\leq f(x)$ for all $x$ near to\n",
    "$\\tilde{x}$, say for $|x - \\tilde{x}| \\leq \\varepsilon$ for some $\\varepsilon > 0$, a \\textit{local minimum}, and when we wish to\n",
    "emphasize that a point $x^*$ satisfying (*) is indeed the best possible value, we call it\n",
    "a \\textit{global minimum}.\n",
    "\n",
    "As you may have guessed, we really like \"bowl-shaped\" functions for which our\n",
    "walking downhill strategy finds a global minimum. Such functions satisfy a geometric\n",
    "property called \\textit{convexity}. A convex function $f: \\mathbb{R}^n \\to \\mathbb{R}$ is one which satisfies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following property:\n",
    "\\begin{equation}\n",
    "f(\\theta x + (1-\\theta)y) \\leq \\theta f(x) + (1-\\theta)f(y) \\text{ for all } \\theta \\in [0,1] \\quad (CVX)\n",
    "\\end{equation}\n",
    "and all $x,y \\in \\mathbb{R}^n$\n",
    "\n",
    "To understand what (CVX) is saying, it is best to draw what it means for a scalar\n",
    "function $f: \\mathbb{R} \\to \\mathbb{R}$.\n",
    "\n",
    "[A graph is drawn here illustrating the convexity property]\n",
    "\n",
    "(CVX) says that if I pick any two points $f(x)$ and $f(y)$ on the graph, and\n",
    "draw a line segment between these two points, then this line lies above the graph. It\n",
    "turns out that this is exactly the right way to mathematically characterize \"bowl-shaped\"\n",
    "functions, even when $x \\in \\mathbb{R}^n$. The important feature of convex functions is that \"walking\n",
    "downhill\" will always bring us to a global minimum. We will see much more about\n",
    "convex functions, but you'll see them again in ESE 3210, and there is a graduate\n",
    "level course, ESE 6050, which focuses entirely on convex optimization problems.\n",
    "\n",
    "Example: The affine function $f(x) = Ax - b$ is convex. To see this, we check that\n",
    "\\begin{equation*}\n",
    "f(\\theta x + (1-\\theta)y) \\leq \\theta f(x) + (1-\\theta)f(y) \\text{ for all } x,y \\in \\mathbb{R}^n \\text{ and } \\theta \\in [0,1]\n",
    "\\end{equation*}\n",
    "But $f(\\theta x + (1-\\theta)y) = A(\\theta x + (1-\\theta)y) - b = \\theta (Ax - b) + (1-\\theta)(Ay - b) = \\theta f(x) + (1-\\theta)f(y)$\n",
    "Affine functions are on the \"boundary\" of being convex.\n",
    "\n",
    "Example: The least squares objective $f(x) = \\|Ax - b\\|^2$ is convex. One can check\n",
    "this from the definition (CVX), but this is very tedious.\n",
    "\n",
    "To gain some intuition, let's consider the scalar setting $f(x) = \\|ax - b\\|^2$. Expanding\n",
    "out $f(x)$ we see\n",
    "\\begin{equation*}\n",
    "f(x) = x^2 \\|a\\|^2 - 2a^Tbx + b^2,\n",
    "\\end{equation*}\n",
    "\n",
    "which is an upward pointing quadratic since $\\|a\\|^2 > 0$ for any $a \\neq 0$. This same\n",
    "intuitive extends to $x \\in \\mathbb{R}^n$ setting. Expanding out $f(x) = \\|Ax - b\\|^2$, we get\n",
    "\n",
    "\\begin{equation*}\n",
    "f(x) = x^T A^TAx - 2x^TA^Tb + \\|b\\|^2.\n",
    "\\end{equation*}\n",
    "\n",
    "This is a quadratic function, with quadratic term given by the quadratic form\n",
    "$x^T A^TAx$, defined by the positive semidefinite matrix $A^TA$. This means that $f(x)$ is an\n",
    "upward pointing bowl with ellipsoidal level sets (recall from Lecture 16), and hence is convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/10_Ch_11_PCA_Apps/121-Apps.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
