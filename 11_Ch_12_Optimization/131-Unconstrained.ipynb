{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 12.1 Applications\n",
    "subject:  Optimization\n",
    "subtitle: \n",
    "short_title: 12.1 Applications\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: \n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/10_Ch_11_PCA_Apps/121-Apps.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 21 - An introduction to unconstrained optimization, gradient descent, and Newtonâ€™s method.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be found in\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\textbf{A Brief Introduction to Optimization}\n",
    "\n",
    "There were a few times during the semester where we tried to find the\n",
    "``best'' vector (or matrix) among a collection of many such vectors or matrices.\n",
    "For example, in least squares, we looked to find the vector $x$ that minimized\n",
    "the expression $\\|Ax - b\\|^2$. In low-rank approximation, we looked to find the\n",
    "matrix $\\hat{M}$ that has rank k (or less) that minimized the expression $\\|\\hat{M} - M\\|_F^2$.\n",
    "\n",
    "These were both specific instances of what is called a \\textit{mathematical optimization\n",
    "problem}. We will focus on \\textit{unconstrained problems}. You will learn a lot more\n",
    "about optimization problems in ESE 3210, and today is only meant to give you\n",
    "a small taste, and to show how essential linear algebra is in finding solutions.\n",
    "\n",
    "Optimization is the process of finding one (or more) vectors $x \\in \\mathbb{R}^n$ that minimize\n",
    "a function $f: \\mathbb{R}^n \\to \\mathbb{R}$. This is written as the optimization problem\n",
    "\n",
    "\\begin{center}\n",
    "minimize $f(x)$ \\hspace{1cm} (P)\n",
    "\\end{center}\n",
    "\n",
    "Here, $x \\in \\mathbb{R}^n$. In (P), the variable $x \\in \\mathbb{R}^n$ is called the \\textit{decision variable},\n",
    "and the function $f: \\mathbb{R}^n \\to \\mathbb{R}$ is called the \\textit{cost function} or \\textit{objective function}.\n",
    "Optimization problem (P) is called \\textit{unconstrained} because we are free to pick any\n",
    "$x \\in \\mathbb{R}^n$ we like to minimize $f(x)$. A constrained optimization problem has the additional\n",
    "requirement that $x$ must satisfy some added conditions, e.g., lie in the solution set\n",
    "of $Ax = b$. We will not consider such problems today, but you'll see many in\n",
    "ESE 3210.\n",
    "\n",
    "The goal of optimization is to find a special decision variable $x^*$ for which\n",
    "the cost function $f$ is as small as possible, i.e., such that\n",
    "\n",
    "\\begin{equation}\n",
    "f(x^*) \\leq f(x) \\quad \\text{for all } x \\in \\mathbb{R}^n \\quad (*)\n",
    "\\end{equation}\n",
    "\n",
    "Such an $x^*$ is called an \\textit{optimal solution} to problem (P), and is defined as\n",
    "the arg min of $f$:\n",
    "\n",
    "\\begin{equation}\n",
    "x^* = \\arg\\min_{x \\in \\mathbb{R}^n} f(x). \\quad (AM)\n",
    "\\end{equation}\n",
    "\n",
    "Equation (AM) simply says in math that $x^*$ satisfies the definition (*) of\n",
    "an optimal point. [Note that if there are multiple optimal points, we instead write\n",
    "\n",
    "\\begin{equation}\n",
    "x^* \\in \\arg\\min_{x \\in \\mathbb{R}^n} f(x)\n",
    "\\end{equation}\n",
    "\n",
    "to indicate $x^*$ belongs to the set of optimal points. Pedantic.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\textbf{Example: The least-squares problem}\n",
    "\n",
    "\\begin{center}\n",
    "minimize $\\|Ax - b\\|^2$\n",
    "\\end{center}\n",
    "\n",
    "over $x \\in \\mathbb{R}^n$ is an unconstrained optimization problem. The objective function is\n",
    "$f(x) = \\|Ax - b\\|^2$, and the optimal solution is\n",
    "\n",
    "\\begin{equation}\n",
    "x^* = (A^TA)^{-1}A^Tb = A^{\\dagger}b\n",
    "\\end{equation}\n",
    "\n",
    "when $(A^TA)^{-1}$ exists. Otherwise, $x^* \\in \\arg\\min \\|Ax - b\\|^2 \\Leftrightarrow A^TAx^* = A^Tb$, i.e.,\n",
    "if and only if $x^*$ satisfies the normal equations $A^TAx = A^Tb$.\n",
    "\n",
    "Despite how simple and innocuous problem (P) looks, it can be used to encode\n",
    "very rich and very challenging problems. Even for $x \\in \\mathbb{R}$, we can get ourselves\n",
    "into trouble. Consider the following two functions that we wish to minimize:\n",
    "\n",
    "[Two graphs are drawn here, labeled $f_1(x)$ and $f_2(x)$]\n",
    "\n",
    "Which do you think is easier to optimize? The left figure, with function $f_1(x)$, is\n",
    "\"bowl-shaped\" and is smallest at $x^* = 2$. What's nice about $f_1$ is that there's also an obvious\n",
    "algorithm for finding $x^* = 2$: If you imagine yourself as an ant standing on the function\n",
    "$f_1(x)$, all you need to do is \"walk downhill\" until you eventually find the bottom of the bowl.\n",
    "\n",
    "In contrast, the right figure with function $f_2(x)$, there are many hills and valleys. The\n",
    "optimal value $x^* = 3$ is the one for which $f_2(x)$ is smallest. But now if we again imagine\n",
    "ourselves as an ant standing on the function $f_2(x)$, our strategy of walking downhill will not\n",
    "always work! For example, if we were to start at $x = 1.5$, then walking downhill would bring\n",
    "us to the bottom of the first valley at $x = 1$. Now $x = 1$ is not an optimal point,\n",
    "since $f_2(3) < f_2(1)$, but from our ant's perspective, $f_2(1)$ appears to be the\n",
    "smallest. We call such a point $\\tilde{x}$ that satisfies $f(\\tilde{x}) \\leq f(x)$ for all $x$ near to\n",
    "$\\tilde{x}$, say for $|x - \\tilde{x}| \\leq \\varepsilon$ for some $\\varepsilon > 0$, a \\textit{local minimum}, and when we wish to\n",
    "emphasize that a point $x^*$ satisfying (*) is indeed the best possible value, we call it\n",
    "a \\textit{global minimum}.\n",
    "\n",
    "As you may have guessed, we really like \"bowl-shaped\" functions for which our\n",
    "walking downhill strategy finds a global minimum. Such functions satisfy a geometric\n",
    "property called \\textit{convexity}. A convex function $f: \\mathbb{R}^n \\to \\mathbb{R}$ is one which satisfies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following property:\n",
    "\\begin{equation}\n",
    "f(\\theta x + (1-\\theta)y) \\leq \\theta f(x) + (1-\\theta)f(y) \\text{ for all } \\theta \\in [0,1] \\quad (CVX)\n",
    "\\end{equation}\n",
    "and all $x,y \\in \\mathbb{R}^n$\n",
    "\n",
    "To understand what (CVX) is saying, it is best to draw what it means for a scalar\n",
    "function $f: \\mathbb{R} \\to \\mathbb{R}$.\n",
    "\n",
    "[A graph is drawn here illustrating the convexity property]\n",
    "\n",
    "(CVX) says that if I pick any two points $f(x)$ and $f(y)$ on the graph, and\n",
    "draw a line segment between these two points, then this line lies above the graph. It\n",
    "turns out that this is exactly the right way to mathematically characterize \"bowl-shaped\"\n",
    "functions, even when $x \\in \\mathbb{R}^n$. The important feature of convex functions is that \"walking\n",
    "downhill\" will always bring us to a global minimum. We will see much more about\n",
    "convex functions, but you'll see them again in ESE 3210, and there is a graduate\n",
    "level course, ESE 6050, which focuses entirely on convex optimization problems.\n",
    "\n",
    "Example: The affine function $f(x) = Ax - b$ is convex. To see this, we check that\n",
    "\\begin{equation*}\n",
    "f(\\theta x + (1-\\theta)y) \\leq \\theta f(x) + (1-\\theta)f(y) \\text{ for all } x,y \\in \\mathbb{R}^n \\text{ and } \\theta \\in [0,1]\n",
    "\\end{equation*}\n",
    "But $f(\\theta x + (1-\\theta)y) = A(\\theta x + (1-\\theta)y) - b = \\theta (Ax - b) + (1-\\theta)(Ay - b) = \\theta f(x) + (1-\\theta)f(y)$\n",
    "Affine functions are on the \"boundary\" of being convex.\n",
    "\n",
    "Example: The least squares objective $f(x) = \\|Ax - b\\|^2$ is convex. One can check\n",
    "this from the definition (CVX), but this is very tedious.\n",
    "\n",
    "To gain some intuition, let's consider the scalar setting $f(x) = \\|ax - b\\|^2$. Expanding\n",
    "out $f(x)$ we see\n",
    "\\begin{equation*}\n",
    "f(x) = x^2 \\|a\\|^2 - 2a^Tbx + b^2,\n",
    "\\end{equation*}\n",
    "\n",
    "which is an upward pointing quadratic since $\\|a\\|^2 > 0$ for any $a \\neq 0$. This same\n",
    "intuitive extends to $x \\in \\mathbb{R}^n$ setting. Expanding out $f(x) = \\|Ax - b\\|^2$, we get\n",
    "\n",
    "\\begin{equation*}\n",
    "f(x) = x^T A^TAx - 2x^TA^Tb + \\|b\\|^2.\n",
    "\\end{equation*}\n",
    "\n",
    "This is a quadratic function, with quadratic term given by the quadratic form\n",
    "$x^T A^TAx$, defined by the positive semidefinite matrix $A^TA$. This means that $f(x)$ is an\n",
    "upward pointing bowl with ellipsoidal level sets (recall from Lecture 16), and hence is convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A curious remark is in order: This argument is so general that the Hessian $\\nabla^2f(x)$ of a twice differentiable function $f:\\mathbb{R}^n\\to\\mathbb{R}$ which is twice positive semi-definite. This is the matrix/vector equivalent of saying that $f(x)=c^T x + b$ with $c$ is an upward pointing but if and only if $c=0$.\n",
    "\n",
    "If you don't remember what the Hessian $\\nabla^2f(x)$ of a function is, or how we computed $\\nabla^2f(x) = \\frac{\\partial^2f}{\\partial x_i\\partial x_j}$ in the above, don't worry, we'll review this in = 5.3.\n",
    "\n",
    "Which way is down?\n",
    "\n",
    "Let's assume that we are either in a \"nice\" setting where our objective function is convex (bowl shaped), or that we're happy to settle for a local minimum. How can we figure out which way is down so that we can tell our little ant friend which way they should walk to get somewhere lower? Let's start with functions with $x\\in\\mathbb{R}^2$ and look at some cost function contour plots.\n",
    "\n",
    "Let's start with two familiar examples:\n",
    "\n",
    "\\begin{itemize}\n",
    "\\item $f(x) = ||x||^2 = x_1^2 + x_2^2$, which is nothing but the Euclidean norm squared of x, and\n",
    "\\item $g(x) = ||x-b||^2 = (x_1-b_1)^2+(x_2-b_2)^2$, which is a very simple least squares objective with $A=I$.\n",
    "\\end{itemize}\n",
    "\n",
    "We have plotted both of these functions, and their contour plots, below:\n",
    "\n",
    "[Image of 4 plots: 3D surface plots and contour plots for $f(x)$ and $g(x)$]\n",
    "\n",
    "The contour plots show the level sets of $f(x)$, which we've seen before. These are the sets $C_a = \\{x\\in\\mathbb{R}^2 | f(x)=a\\}$ for some constant a. Here, these end up as circles, where for example $C_1$ is the set of $x_1,x_2$ such that $x_1^2 + x_2^2 = 1$ or $(x_1-b_1)^2 + (x_2-b_2)^2 = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use these contour plots to identify which way is down if our ant is currently sitting at point $x\\in\\mathbb{R}^2$?\n",
    "\n",
    "To answer this question, we'll need the gradient $\\nabla f(x)$ of our function. Recall from Math 1410 that for a function $f:\\mathbb{R}^n\\to\\mathbb{R}$, its gradient $\\nabla f(x)$ is an n-vector of the partial derivatives of f:\n",
    "\n",
    "\\[\\nabla f(x) = \\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1}(x) \\\\[6pt]\n",
    "\\vdots \\\\[6pt]\n",
    "\\frac{\\partial f}{\\partial x_n}(x)\n",
    "\\end{bmatrix} \\in \\mathbb{R}^n\\]\n",
    "\n",
    "For our functions on $\\mathbb{R}^2$, this reduces to $\\nabla f(x) = (\\frac{\\partial f}{\\partial x_1}(x), \\frac{\\partial f}{\\partial x_2}(x)) \\in \\mathbb{R}^2$.\n",
    "\n",
    "In the figure below, we show contour plots for $f(x)=||x-b||^2=(x_1-3)^2+(x_2-3)^2$ and $f_2(x)=||Ax-b||^2$, along with the gradients $\\nabla f(x)$ (green arrows) at various points.\n",
    "\n",
    "[Image placeholder for contour plots with gradients]\n",
    "\n",
    "In both cases, the gradients are pointing in the direction of maximal increase. If we go in the opposite $-\\nabla f(x)$ direction, we will move in the direction of maximal decrease!\n",
    "\n",
    "Let's flex our calculus muscles a bit and compute the gradients of $f(x)$ and $f_2(x)$:\n",
    "\n",
    "\\[\\nabla f_1(x) = \\begin{bmatrix}\n",
    "\\frac{\\partial}{\\partial x_1}((x_1-3)^2+(x_2-3)^2) \\\\[6pt]\n",
    "\\frac{\\partial}{\\partial x_2}((x_1-3)^2+(x_2-3)^2)\n",
    "\\end{bmatrix} = 2\\begin{bmatrix}\n",
    "x_1-3 \\\\[6pt]\n",
    "x_2-3\n",
    "\\end{bmatrix}\\]\n",
    "\n",
    "[If you don't know how to compute this gradient, you may wish to review Math110 notes, but don't worry, we won't ask you to calculate gradients on the homework & exams]\n",
    "\n",
    "\\[\\nabla f_2(x) = \\nabla (Ax-b)^T(Ax-b) = 2A^T(Ax-b)\\]\n",
    "\n",
    "One thing you might notice in the plots above is that the red dots, which we placed on the function minimum, have no green arrows. This is because the gradient at these points $x^*$ is zero. In fact, this is true in general: a point $x^*$ is a local minimum only if $\\nabla f(x^*)=0$.\n",
    "\n",
    "We won't prove this, but intuitively, this makes sense: if $\\nabla f(x)\\neq 0$, then this means there is a direction $-\\nabla f(x)$ in which I could walk a little bit downhill to decrease $f(x)$, so $\\nabla f(x)$ must be zero if we're at a local minima. Let's check that this holds for $\\nabla f_1(x)$ and $\\nabla f_2(x)$ above:\n",
    "\n",
    "For $f_1(x)$, $x^* = (3,3)$, and $\\nabla f_1(x^*) = 2\\begin{bmatrix} x_1^*-3 \\\\ x_2^*-3 \\end{bmatrix} = \\mathbf{0}$, and so that checks out. And for $f_2(x)$, $x^* = (A^TA)^{-1}A^Tb$, and\n",
    "\n",
    "$\\nabla f_2(x^*) = 2(A^TAx^* - A^Tb) = 2(A^TA((A^TA)^{-1}A^Tb) - A^Tb) = 2(A^Tb - A^Tb) = \\mathbf{0}$!\n",
    "\n",
    "In general, while $\\nabla f(x^*) = \\mathbf{0}$ is necessary for $x^*$ to be a local minima, it is not sufficient, i.e., there may be certain points with $\\nabla f(x^*)=\\mathbf{0}$ that are not local minima. For example, all red dots in the plot below have vanishing gradients, but only two are minima:\n",
    "\n",
    "[Image of a function with multiple stationary points]\n",
    "\n",
    "However, if our function $f$ is convex (bowl shaped), we have the following theorem which tells us that walking downhill will always find a global minimum:\n",
    "\n",
    "Theorem: Let $f:\\mathbb{R}^n \\to \\mathbb{R}$ be convex. Then $x^*$ is globally optimal, i.e.,\n",
    "\n",
    "$f(x^*) \\leq f(x) \\quad \\forall x\\in\\mathbb{R}^n$\n",
    "\n",
    "if and only if $\\nabla f(x^*) = \\mathbf{0}$.\n",
    "\n",
    "Next, we'll use our new found insights to define gradient descent, a widely used iterative algorithm for finding local minima of optimization problems (P)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/10_Ch_11_PCA_Apps/121-Apps.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
