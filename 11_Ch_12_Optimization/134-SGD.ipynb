{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 12.4 SGD\n",
    "subject:  Optimization\n",
    "subtitle: \n",
    "short_title: 12.4 SGD\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: \n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/10_Ch_11_PCA_Apps/121-Apps.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 22 - An Introduction to Backpropagation.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section*{Stochastic Gradient Descent}\n",
    "\n",
    "Last class, motivated by machine learning applications, we discussed how to apply gradient descent to solve the following optimization problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{minimize } \\text{loss}((z_i,y_i),x) = \\text{minimize } \\sum_{i=1}^N l(m(z_i;x)-y_i) \\quad (L)\n",
    "\\end{equation}\n",
    "\n",
    "over the parameters $x$ of a model $m$ so that $m(z_i;x) \\approx y_i$ for all training data input/output pairs $(z_i,y_i)$, $i=1,\\ldots,N$.\n",
    "\n",
    "The bulk of our effort was spent on understanding how to compute the gradient of $l(m(z_i;x)-y_i)$ with respect to the model parameters $x$, with a particular focus on models $m$ that can be written as the following composition of models:\n",
    "\n",
    "\\begin{align*}\n",
    "O_0 &= z_i \\\\\n",
    "O_1 &= m_1(O_0;x_1), \\quad O_1 \\in \\mathbb{R}^{p_1}, O_0 \\in \\mathbb{R}^{p_0} \\\\\n",
    "O_2 &= m_2(O_1;x_2), \\quad O_2 \\in \\mathbb{R}^{p_2}, O_1 \\in \\mathbb{R}^{p_1} \\quad (DNN) \\\\\n",
    "&\\vdots \\\\\n",
    "O_L &= m_L(O_{L-1};x_L), \\quad O_L \\in \\mathbb{R}^{p_L}, O_{L-1} \\in \\mathbb{R}^{p_{L-1}}\n",
    "\\end{align*}\n",
    "\n",
    "which is the structure of contemporary models used in machine learning called deep neural networks (we'll talk about these much more later). We made two key observations about the model structure that allowed us to effectively apply the matrix chain rule to compute gradients $\\frac{\\partial l}{\\partial x_1}, \\ldots, \\frac{\\partial l}{\\partial x_L}$:\n",
    "\n",
    "1) $\\frac{\\partial l}{\\partial x_j}$ only needs to compute partial derivatives of $l$ and layers $j, j+1, \\ldots, L$;\n",
    "\n",
    "2) If we start from layer $L$ ($\\frac{\\partial l}{\\partial x_L}$) and work our way backwards we can\n",
    "   (i) reuse previously computed partial derivatives, and\n",
    "   (ii) save space on memory by exploiting that $\\frac{\\partial l}{\\partial x_L}$ is a row-vector.\n",
    "\n",
    "The resulting algorithm is called backpropagation, and is a key enabling technology in modern machine learning. You will learn more about this in ESE 5460.\n",
    "\n",
    "Now, despite all of this cleverness, when the model parameter vectors $x_1, \\ldots, x_L$ and layer outputs $O_1, \\ldots, O_L$ are very high dimensional (it is not uncommon for each $x_i$ to have 100s of thousands or even millions of components) computing the gradient $\\nabla_x l(m(z_i;x)-y_i)$ of a single term in the sum (L) can be quite costly. Add to that the fact that the number of data points $N$ is often very large (order of millions in many settings), and we quickly run into some serious computational bottlenecks. And remember, this all just so we can run a single iteration of gradient descent. This may seem hopeless, but luckily, there is a very simple trick that lets us work around this problem: stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent (SGD) is the work horse algorithm of modern machine learning and has been rediscovered by various communities over the past 70 years, although it is usually credited to Robbins and Monro for a paper they wrote in 1951.\n",
    "\n",
    "\\textbf{Key Idea:} Since our loss function can be written as a sum over examples, i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "(LL) \\quad \\text{loss}((z_i,y_i),x) = \\frac{1}{N} \\sum_{i=1}^N l(m(z_i;x)-y_i) \\quad (\\text{loss}(x) = \\mathbb{E} l_i(x))\n",
    "\\end{equation}\n",
    "\n",
    "then the gradient is also a sum: $\\nabla_x \\text{loss} = \\frac{1}{N} \\sum \\nabla_x l_i$. Therefore we expect each individual gradient $\\nabla_x l_i$ to have some useful information in it. SGD minimizes (LL) by following the gradient of a \\textbf{single randomly selected example} (or a small batch of $B$ randomly selected samples).\n",
    "\n",
    "The SGD algorithm can be summarized as follows: Start with an initial guess $x^{(0)}$, and at each iteration $k=0,1,2,\\ldots$, do:\n",
    "\n",
    "(i) Select an index $i \\in \\{1,\\ldots,N\\}$ at random\n",
    "(ii) Update\n",
    "\\begin{equation}\n",
    "x^{(k+1)} = x^{(k)} - s^{(k)} \\nabla_x l_i(x^{(k)}) \\quad (SGD)\n",
    "\\end{equation}\n",
    "\n",
    "Using the gradient of only the $i$th loss term $l_i(x) = l(m(z_i;x)-y_i)$.\n",
    "\n",
    "As before, $s^{(k)} > 0$ is a step-size that can change as a function of the current iterate.\n",
    "\n",
    "This method works shockingly well in practice and is computationally tractable as at each iteration, the gradient of only the $i$th loss term needs to be computed. Modern versions of this algorithm replace step (i) with a mini-batch, i.e., by selecting $B$ indices at random, and step (ii) replaces $\\nabla_x l_i(x^{(k)})$ with the average gradient:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{B} \\sum_{b=1}^B \\nabla_x l_b(x) \\quad (\\hat{G})\n",
    "\\end{equation}\n",
    "\n",
    "The overall idea behind why SGD works (take ESE 605U if you want to see a rigorous proof) is that while each individual update (SGD) may not be an accurate gradient for the overall loss function loss$(x)$, we are still following $\\nabla_x \\text{loss}(x)$ \"on average\". This also explains why you may want to use a mini-batch $B$ to compute a better gradient estimate $(\\hat{G})$, as having more loss terms leads to a better approximation of the true gradient. The tradeoff here is that as $B$ becomes larger, computing $(\\hat{G})$ is more computationally demanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section*{Linear Classification and the Perceptron}\n",
    "\n",
    "An important problem in machine learning is that of binary classification. In one of the online case studies, you saw how to use least squares to solve this problem. Here, we offer an alternate perspective that will lead us to one important historical reason for the emergence of deep neural networks.\n",
    "\n",
    "The problem set up for linear binary classification is as follows. We are given a set of $N$ vectors $z_1,\\ldots,z_N \\in \\mathbb{R}^n$ with associated binary labels $y_1,\\ldots,y_N \\in \\{-1,+1\\}$. The objective in linear classification is to find an affine function $x^Tz+v$, defined by unknown parameters $x \\in \\mathbb{R}^n$ and $v \\in \\mathbb{R}$, that strictly separates the two classes. We can pose this as finding a feasible solution to the following linear inequalities:\n",
    "\n",
    "\\begin{equation}\n",
    "(LC) \\quad\n",
    "\\begin{cases}\n",
    "x^Tz_i + v > 0 & \\text{if } y_i = +1 \\\\\n",
    "x^Tz_i + v < 0 & \\text{if } y_i = -1\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "The geometry of this problem is illustrated on the right. There are three key components:\n",
    "\n",
    "1) The separating hyperplane $H = \\{z \\in \\mathbb{R}^n : x^Tz+v = 0\\}$. This is the set of vectors $z \\in \\mathbb{R}^n$ that lie on the subspace $H$, which is the solution set to the linear equation\n",
    "\n",
    "\\[x^Tz = -v.\\]\n",
    "\n",
    "The coefficient matrix here is $x^T \\in \\mathbb{R}^{1\\times n}$, and so rank$(x^T) = 1$. This tells us that $\\dim \\text{Null}(x^T) = \\dim H = n-1$. In $\\mathbb{R}^2$, this is the equation of a line:\n",
    "\n",
    "\\[\\begin{bmatrix} x_1 & x_2 \\end{bmatrix} \\begin{bmatrix} z_1 \\\\ z_2 \\end{bmatrix} + v = x_1z_1 + x_2z_2 + v = 0 \\implies z_2 = -\\frac{x_1}{x_2}z_1 - \\frac{v}{x_2}\\]\n",
    "\n",
    "In $\\mathbb{R}^3$, this is the equation of a plane with normal vector $x$ going through point $v$. And in $\\mathbb{R}^n$ is called a hyperplane. A key feature of a hyperplane is that it splits $\\mathbb{R}^n$ into two half-spaces, i.e., the subsets of $\\mathbb{R}^n$ on either side.\n",
    "\n",
    "2) The half-space $H^+ = \\{z \\in \\mathbb{R}^n : x^Tz+v > 0\\}$, which is the \"half\" of $\\mathbb{R}^n$ for which $x^Tz+v > 0$. We want all of our positive (+) examples to live here.\n",
    "\n",
    "3) The half-space $H^- = \\{z \\in \\mathbb{R}^n : x^Tz+v < 0\\}$, which is the \"half\" of $\\mathbb{R}^n$ for which $x^Tz+v < 0$. We want all (-) examples to live here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The problem of finding the parameters $(x,v)$ defining the classifier in (LC) can be solved using linear programming, a family of optimization algorithms that you'll learn about in BE 3040 and ESE 6050. It can also be solved using SGD as applied to a special loss function called the hinge loss:\n",
    "\n",
    "\\[\n",
    "\\text{loss}((z_i,y_i);(x,v)) = \\frac{1}{N} \\sum_{i=1}^N \\max\\{1-y_i(x^Tz_i+v), 0\\},\n",
    "\\]\n",
    "\n",
    "which is a commonly used loss function for classification (you'll learn why in your ML classes).\n",
    "\n",
    "The reason we are taking this little digression is that applying SGD to the hinge loss gives us The Perceptron Algorithm:\n",
    "\n",
    "Initialize initial guess $(x^{(0)},v^{(0)})$\n",
    "For each iteration $k=0,1,2,\\ldots$, do:\n",
    "\\begin{enumerate}\n",
    "    \\item Draw a random index $i \\in \\{1,\\ldots,N\\}$\n",
    "    \\item If $y_i(x^{(k)T}z_i+v^{(k)}) < 1$: Update $(x^{(k+1)},v^{(k+1)}) = (x^{(k)},v^{(k)}) + y_i\\begin{bmatrix} z_i \\\\ 1 \\end{bmatrix}$ \\hspace{2em} (U)\n",
    "    \\\\ Else, if $y_i(x^{(k)T}z_i+v^{(k)}) \\geq 1$, do not update $(x^{(k+1)},v^{(k+1)}) = (x^{(k)},v^{(k)})$.\n",
    "\\end{enumerate}\n",
    "\n",
    "This algorithm goes through the examples $(z_i,y_i)$ one at a time, and updates the classifier only when it makes a mistake (U). The intuition is it \"nudges\" the classifier to be \"less\" wrong by $\\|z_i\\|^2+1$ on any example $(z_i,y_i)$ it currently misclassifies.\n",
    "\n",
    "This incremental update works, and you can show that if there exists a solution to (LC), the perceptron algorithm will find it. People got REALLY EXCITED ABOUT THIS. See next page for a NYT article about the perceptron algorithm, which in hindsight seems a little silly given that we now know it's just SGD applied to a particular loss function. But then again, so is most of today's AI!\n",
    "\n",
    "\\section*{Single and Multi Layer Perceptrons}\n",
    "\n",
    "Given the excitement about the perceptron, why do we not use them anymore? It turns out, it is very easy to stump! Consider the following set of positive and negative examples:\n",
    "\n",
    "[XOR function diagram]\n",
    "\n",
    "No linear classifier can separate the + from the -\n",
    "Is AI doomed?\n",
    "\n",
    "These define an XOR function: the positive examples are in quadrants where $\\text{sign}(z_1) \\neq \\text{sign}(z_2)$ and the negative ones are in quadrants for which $\\text{sign}(z_1) = \\text{sign}(z_2)$. These can't be separated by a linear classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO**: Electronic Brain teaches itself pic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "But suppose we were allowed to have two classifiers, and then combine them using a nonlinearity?\n",
    "\n",
    "\\begin{tikzpicture}\n",
    "\\draw[->] (-2,0) -- (2,0) node[right] {$z_1$};\n",
    "\\draw[->] (0,-2) -- (0,2) node[above] {$z_2$};\n",
    "\\draw[blue, thick] (-2,1) -- (2,1) node[right] {$2$};\n",
    "\\draw[purple, thick] (1,-2) -- (1,2);\n",
    "\\draw[magenta, thick] (-2,-2) -- (2,2);\n",
    "\\node[magenta] at (1.5,1.5) {$H_1^+ = \\{z_2 - z_1 > 0\\}$};\n",
    "\\node[purple] at (-1.5,1.5) {$H_2^+ = \\{z_1 - z_2 > 0\\}$};\n",
    "\\end{tikzpicture}\n",
    "\n",
    "In the image above, we define a pink classifier that returns $f_1(z) > z_1$\n",
    "a purple classifier that returns $f_2(z) = z_2$. If we define our output to be\n",
    "\n",
    "\\[f(z) = f_1(z) f_2(z) = -z_1 z_2,\\]\n",
    "\n",
    "then we see that this 'works':\n",
    "\n",
    "\\begin{tabular}{c|c|c|c|c}\n",
    " & 1 & 2 & 3 & 4 \\\\\n",
    "\\hline\n",
    "sign $f_1(z)$ & +1 & -1 & -1 & +1 \\\\\n",
    "sign $f_2(z)$ & -1 & -1 & +1 & +1 \\\\\n",
    "sign $f(z)$ & -1 & +1 & -1 & +1\n",
    "\\end{tabular}\n",
    "\n",
    "This 'worked'! The two key ingredients here are:\n",
    "\\begin{enumerate}\n",
    "\\item Having intermediate computation, called hidden layers\n",
    "\\item Allowing for some nonlinearity.\n",
    "\\end{enumerate}\n",
    "\n",
    "These two ingredients are combined to define the Multilayer Perceptron (MLP).\n",
    "\n",
    "A single hidden layer MLP is defined by the equations:\n",
    "\n",
    "\\begin{align*}\n",
    "h &= \\sigma(W_1 z + b_1) \\quad \\text{(MLP1)} \\\\\n",
    "o &= W_2 h + b_2\n",
    "\\end{align*}\n",
    "\n",
    "The key features of (MLP1) are:\n",
    "\\begin{itemize}\n",
    "\\item An element-wise nonlinearity $\\sigma$, called an activation function.\n",
    "\\item The input is $z \\in \\mathbb{R}^n$.\n",
    "\\item The hidden layer is defined by a weight matrix $W_1 \\in \\mathbb{R}^{h \\times n}$ and a bias vector $b_1 \\in \\mathbb{R}^h$\n",
    "\\item The output layer is defined by a weight matrix $W_2 \\in \\mathbb{R}^{p \\times h}$ and bias vector $b_2 \\in \\mathbb{R}^p$\n",
    "\\item The overall map maps input $z \\in \\mathbb{R}^n$ to output $o \\in \\mathbb{R}^p$.\n",
    "\\end{itemize}\n",
    "\n",
    "In practice, (MLP1) is trained to find $W_1, W_2, b_1, b_2$ using SGD and backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section*{Why do we need a nonlinear activation function?}\n",
    "\n",
    "Suppose we didn't include $\\sigma(\\cdot)$, and defined our MLP as $h = W_1 z + b_1$, $o = W_2 h + b_2$.\n",
    "If we eliminate the hidden layer variable $h$, we get\n",
    "\n",
    "\\[o = W_2(W_1 z + b_1) + b_2 = \\underbrace{W_2 W_1}_W z + \\underbrace{W_2 b_1 + b_2}_b = Wz + b.\\]\n",
    "\n",
    "This shows that we do not increase the expressivity of our model; as without the activation\n",
    "function, our model class reduces to affine functions. In some sense, this nonlinearity is\n",
    "the \"secret sauce\" of MLPs.\n",
    "\n",
    "Some common activation functions include:\n",
    "\n",
    "\\subsection*{The Sigmoid function:}\n",
    "\n",
    "\\begin{itemize}\n",
    "\\item Maps input into (0,1):\n",
    "\n",
    "\\[\\text{sigmoid}(x) = \\frac{1}{1+e^{-x}}\\]\n",
    "\n",
    "[Insert sigmoid function graph here]\n",
    "\n",
    "\\item Can view as a \"soft version\" of $\\sigma(x) = 1$ if $x > 0$ and $\\sigma(x) = 0$ if $x \\leq 0$.\n",
    "\\item This is allows for binary classification over classes \\{0,1\\}.\n",
    "\\end{itemize}\n",
    "\n",
    "\\subsection*{The Rectified Linear Unit (ReLU):}\n",
    "\n",
    "\\[ReLU(x) = \\max\\{x, 0\\}.\\]\n",
    "\n",
    "[Insert ReLU function graph here]\n",
    "\n",
    "Which activation function to use is a bit of an art, but these are generally accepted tricks\n",
    "of the trade that you'll learn about in ESE 5460. There are also many more than these two, with\n",
    "new ones being invented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\section*{Deep MLPs}\n",
    "\n",
    "There is nothing preventing us from adding more hidden layers. The $L$-hidden-layer\n",
    "MLP is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "h_1 &= \\sigma(W_1 x + b_1) \\\\\n",
    "h_2 &= \\sigma(W_2 h_1 + b_2) \\\\\n",
    "&\\vdots \\\\\n",
    "h_L &= \\sigma(W_L h_{L-1} + b_L) \\\\\n",
    "o &= W_{L+1} h_L + b_{L+1}\n",
    "\\end{align*}\n",
    "\n",
    "Shown on the right is an example with 3 hidden layers.\n",
    "\n",
    "The important thing to notice is these functions are\n",
    "compatible with our discussion on backpropagation, meaning computing gradients with respect to\n",
    "the parameters $W_1,\\ldots,W_{L+1}, b_1,\\ldots,b_{L+1}$ can be done efficiently!\n",
    "\n",
    "In the online notes, we'll show you how to take advantage of autodifferentiation to\n",
    "efficiently train MLPs in code.\n",
    "\n",
    "\\begin{tikzpicture}[scale=0.7]\n",
    "\\node[circle,draw,fill=blue!20] (x1) at (0,0) {$x_1$};\n",
    "\\node[circle,draw,fill=blue!20] (x2) at (2,0) {$x_2$};\n",
    "\\node[circle,draw,fill=blue!20] (x3) at (4,0) {$x_3$};\n",
    "\\node[circle,draw,fill=blue!20] (x4) at (6,0) {$x_4$};\n",
    "\n",
    "\\node[circle,draw,fill=blue!20] (h11) at (0,2) {$h_1$};\n",
    "\\node[circle,draw,fill=blue!20] (h12) at (1.5,2) {$h_2$};\n",
    "\\node[circle,draw,fill=blue!20] (h13) at (3,2) {$h_3$};\n",
    "\\node[circle,draw,fill=blue!20] (h14) at (4.5,2) {$h_4$};\n",
    "\\node[circle,draw,fill=blue!20] (h15) at (6,2) {$h_5$};\n",
    "\n",
    "\\node[circle,draw,fill=blue!20] (h21) at (1,4) {$h_1$};\n",
    "\\node[circle,draw,fill=blue!20] (h22) at (3,4) {$h_2$};\n",
    "\\node[circle,draw,fill=blue!20] (h23) at (5,4) {$h_3$};\n",
    "\n",
    "\\node[circle,draw,fill=blue!20] (h31) at (2,6) {$h_1$};\n",
    "\\node[circle,draw,fill=blue!20] (h32) at (4,6) {$h_2$};\n",
    "\n",
    "\\node[circle,draw,fill=blue!20] (o1) at (2,8) {$o_1$};\n",
    "\\node[circle,draw,fill=blue!20] (o2) at (4,8) {$o_2$};\n",
    "\n",
    "\\draw (x1) -- (h11) -- (h21) -- (h31) -- (o1);\n",
    "\\draw (x2) -- (h12) -- (h22) -- (h32) -- (o2);\n",
    "\\draw (x3) -- (h13) -- (h23);\n",
    "\\draw (x4) -- (h14);\n",
    "\\draw (h15) -- (h23);\n",
    "\n",
    "\\node[text width=3cm] at (-2,0) {Input layer};\n",
    "\\node[text width=3cm] at (-2,2) {Hidden layer};\n",
    "\\node[text width=3cm] at (-2,4) {Hidden layer};\n",
    "\\node[text width=3cm] at (-2,6) {Hidden layer};\n",
    "\\node[text width=3cm] at (-2,8) {Output layer};\n",
    "\n",
    "\\end{tikzpicture}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/10_Ch_11_PCA_Apps/121-Apps.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
