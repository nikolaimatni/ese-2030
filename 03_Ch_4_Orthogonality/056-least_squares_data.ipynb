{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 4.6 Least Squares and Data Fitting\n",
    "subject:  Orthogonality\n",
    "subtitle: model some observed data\n",
    "short_title: 4.6 Least Squares and Data Fitting\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: Data, Model parameterization, Minimum Mean Square Error, Regression\n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/03_Ch_4_Orthogonality/056-least_squares_data.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 09 - Least Squares Data Fitting.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be found in VMLS 13.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- what a data fitting problem is and how it relates to least squares\n",
    "- model parameterization and examples of different parameterizations\n",
    "- the Minimum Mean Squared Error (MMSE)\n",
    "- the regression model and one of its application (auto-regressive model for time-series modeling)\n",
    "- the standard tricks for model selection: generalization and validation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We will introduce one of the most important applications of least squares methods: fitting a mathematical model to some relation given some observed data.\n",
    "\n",
    "A typical data fitting problem takes the following form: There is some underlying _feature vector or independent variable_ $\\vv x \\in \\mathbb{R}^m$ and a scalar _outcome or response variable_ $y \\in \\mathbb{R}$ that we believe are (approximately) related by some function $f: \\mathbb{R}^m \\to \\mathbb{R}$ such that\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{y_app_f}\n",
    "y \\approx f(x).  \\qquad (M)\n",
    "\\end{equation}\n",
    "\n",
    "### Data\n",
    "\n",
    "Our goal is to fit (or learn) a _model_ $f$ given some _data_:\n",
    "\n",
    "$$\n",
    "(\\vv x^{(1)}, y^{(1)}), (\\vv x^{(2)}, y^{(2)}), \\ldots, (\\vv x^{(N)}, y^{(N)}).\n",
    "$$\n",
    "\n",
    "These _data pairs_ $(\\vv x^{(i)}, y^{(i)})$ are sometimes also called _observations, examples, samples, or measurements_ depending on context.\n",
    "\n",
    ":::{note}\n",
    "The superscript $^{(i)}$ denotes the $i$-th data point. For example, ${\\vv x^{(i)} \\in \\mathbb{R}^m}$ is the $i^{th}$ independent variable, and $x_j^{(i)}$ is the value of $j^{th}$ feature for example $i$.\n",
    ":::\n",
    "\n",
    "### Model Parameterization\n",
    "\n",
    "Our goal is to choose a model $\\hat{f}: \\mathbb{R}^m \\to \\mathbb{R}$ that approximates the [model](#y_app_f) well, that is, $y \\approx \\hat{f}(x)$. The hat notation is traditionally used to highlight that $\\hat{f}$ is an approximation to $f$. Specifically, we will write $\\hat{y} = \\hat{f}(x)$ to highlight that $\\hat{y}$ is an approximate prediction of the outcome $y$.\n",
    "\n",
    "In order to efficiently search over candidate model functions $\\hat{f}$, we need to _parameterize a model class $\\mathcal{F}$_ that is easy to work with. A powerful and commonly used model class is the set of _linear in the parameters_ models of the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{LP_eqn}\n",
    "\\hat{f}(\\vv x) = \\theta_1 f_1(\\vv x) + \\theta_2 f_2(\\vv x) + \\cdots + \\theta_p f_p(\\vv x). \\qquad (LP) \n",
    "\\end{equation}\n",
    "\n",
    "In [(LP)](#LP_eqn), the functions $f_i: \\mathbb{R}^m \\to \\mathbb{R}$ are _basis functions_ or _features_ that we choose before hand. \n",
    "\n",
    ":::{warning}\n",
    "Note that the term basis here is related to, but different from, our [previous use](../01_Ch_2_Vector_Spaces_and_Bases/034-basis_dim.ipynb#basis_defn) of the term. \n",
    ":::\n",
    "\n",
    "When we solve the data fitting problem, we will look for the _parameters_ $\\theta_i$ that, among other things, make the model prediction $\\hat{y}_i = \\hat{f}(\\vv x^{(i)})$ **consistent with the observed data**, i.e., we want $y^{(i)} \\approx y^{(i)}$.\n",
    "\n",
    "### Data fitting:\n",
    "\n",
    "For the $i$-th observation $y^{(i)}$ and the $i^{th}$ prediction $\\hat{y}^{(i)}$, we define the _prediction error_ or _residual_ $r^{(i)} = \\hat{y}^{(i)} - y^{(i)}$.\n",
    "\n",
    "The _least squares data fitting problem_ chooses the model parameters $\\theta_i$ that minimize the (average of the) sum of the squares of the prediction errors on the data set:\n",
    "\n",
    "$$\n",
    "\\frac{(r^{(1)})^2 + \\cdots + (r^{(N)})^2}{N}\n",
    "$$\n",
    "\n",
    "Next we'll show that this problem can be cast as a least squares problem over the model parameters $\\theta_i$. Before doing that though, we want to highlight the conceptual shift we are making.\n",
    "\n",
    ":::{important} DATA-DRIVEN\n",
    "Rather than hand crafting our function $\\hat{f}$ from scratch, we **solve an optimization problem** to identify the parameters $\\theta_i$ that best explain the data, i.e., we _learn_ the model from from the data. Of course, if we know something about the model structure, we should encode this in our choice of feature functions $f_i$. We'll see examples of such _feature engineering_ later.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fitting as Least Squares\n",
    "\n",
    "We start by stacking the outcomes $y^{(i)}$, predictions $\\hat{y}^{(i)}$, and residuals $r^{(i)}$ as vectors in $\\mathbb{R}^N$:\n",
    "\n",
    "$$\n",
    "\\vv y = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(N)} \\end{bmatrix}, \\quad \n",
    "\\hat{\\vv y} = \\begin{bmatrix} \\hat{y}^{(1)} \\\\ \\hat{y}^{(2)} \\\\ \\vdots \\\\ \\hat{y}^{(N)} \\end{bmatrix}, \\quad \n",
    "\\vv r = \\begin{bmatrix} r^{(1)} \\\\ r^{(2)} \\\\ \\vdots \\\\ r^{(N)} \\end{bmatrix} = \\begin{bmatrix} y^{(1)} - \\hat{y}^{(1)} \\\\ y^{(2)} - \\hat{y}^{(2)} \\\\ \\vdots \\\\ y^{(N)} - \\hat{y}^{(N)} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then we can compactly write the _squared prediction error_ as $\\|\\vv r\\|_2^2$. Next, we compile our model parameters into a vector $\\vv \\theta \\in \\mathbb{R}^p$, and build our _feature matrix or measurement matrix_ $A \\in \\mathbb{R}^{N \\times p}$ by setting\n",
    "\n",
    "$$\n",
    "A_{ij} = f_j(\\vv x^{(i)}), \\quad i=1,\\ldots,N, \\quad j=1,\\ldots,p.\n",
    "$$\n",
    "\n",
    "The $j$-th column of the matrix $A$ is composed of the $j$-th basis function evaluated on each of the data points $\\vv x^{(1)},\\ldots,\\vv x^{(N)}$:\n",
    "\n",
    "$$\n",
    "\\vv f_1(\\vv x) = \\begin{bmatrix} f_1(\\vv x^{(1)}) \\\\ f_1(\\vv x^{(2)}) \\\\ \\vdots \\\\ f_1(\\vv x^{(N)}) \\end{bmatrix}, \\cdots,  \\vv f_p(x) = \\begin{bmatrix} f_p(\\vv x^{(1)}) \\\\ f_p(\\vv x^{(2)}) \\\\ \\vdots \\\\ f_p(\\vv x^{(N)}) \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "and $A = [\\vv f_1(\\vv x) \\cdots \\vv f_p(\\vv x)]$. In matrix-vector notation, we then have\n",
    "\n",
    "$$\n",
    "\\hat{\\vv y} = A\\vv \\theta = \\theta_1 \\vv f_1(\\vv x) + \\cdots + \\theta_p \\vv f_p(\\vv x). \n",
    "$$\n",
    "\n",
    "The least squares data fitting problem then becomes to\n",
    "\n",
    "$$\n",
    "\\text{minimize } \\|\\vv r\\|^2 \\Rightarrow \\text{minimize } \\|\\vv y - A\\vv \\theta\\|^2\n",
    "$$\n",
    "\n",
    "over the model parameters $\\vv \\theta$, which we recognize as a [least squares problem](./055-least_squares.ipynb#least-squares-defn)! Assuming we have chosen basis functions $f_i$ such that the columns of $A$ are linearly independent (what would it mean if this wasn't true?), we have that the least squares solution is\n",
    "\n",
    "$$\n",
    "\\hat{\\vv \\theta} = (A^TA)^{-1}A^T\\vv y. \n",
    "$$\n",
    "\n",
    ":::{prf:definition}\n",
    ":label: MMSE_defn\n",
    "The resulting average least squares error \n",
    "$$\\frac{\\|A\\hat{\\vv \\theta} - \\vv y\\|^2}{N}\n",
    "$$\n",
    "is called the _Minimum Mean-Square Error (MMSE)_.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up: Fitting a Constant Model\n",
    "\n",
    "We start with the simplest possible model and set the number of features $p=1$ and $f_1(x) = 1$, so that our (admittedly boring) model becomes $\\hat{f}(\\vv x) = \\theta_1$.\n",
    "\n",
    "First, we construct $A \\in \\mathbb{R}^{N \\times 1}$ by setting $A_{i1} = f_1(\\vv x^{(i)}) = 1$. Therefore $A$ is the $N$-dimensional all ones vector $\\mathbf{1}_N$. We plug this into our formula for $\\hat{\\vv \\theta}$:\n",
    "\n",
    "$$\n",
    "\\hat{\\vv \\theta} = \\hat{\\theta}_1 = (\\vv 1^T\\vv 1)^{-1}\\vv 1^T\\vv y = \\frac{1}{N}\\sum_{i=1}^N y^{(i)} = \\text{average}(\\vv y). \n",
    "$$\n",
    "\n",
    "We have just shown that the _mean_ or _average_ of the outcomes $y^{(1)},\\ldots,y^{(N)}$ is the best least squares fit of a constant model. In this case, the MMSE is\n",
    "\n",
    "$$\n",
    "\\frac{1}{N}\\sum_{i=1}^N (\\text{average}(\\vv y) - y^{(i)})^2,\n",
    "$$\n",
    "\n",
    "which is called the _variance_ of $\\vv y$, and measures how \"wiggly\" $\\vv y$ is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Function: Straight Line Fit\n",
    "\n",
    "We start by considering the univariate function setting where our feature vector $\\vv x = x \\in \\mathbb{R}$ is a scalar, and hence we are looking to approximate a function $f: \\mathbb{R} \\to \\mathbb{R}$. This is a nice way to get intuition because it is easy to plot the data $(x^{(i)}, y^{(i)})$ and the model function $\\hat{y} = \\hat{f}(x)$.\n",
    "\n",
    "We'll start with a _straight line fit_ model: we set $p=2$, with $f_1(x) = 1$ and $f_2(x) = x$. In this case our model function is composed of models of the form\n",
    "\n",
    "$$\n",
    "\\hat{f}(x) = \\theta_1 + \\theta_2 x. \n",
    "$$\n",
    "\n",
    "Here, we can easily interpret $\\theta_1$ as the y-intercept and $\\theta_2$ as the slope of the straight line model we are searching for.\n",
    "\n",
    "In this case, the matrix $A \\in \\mathbb{R}^{N \\times 2}$ and takes the form\n",
    "\n",
    "$$ A = \\begin{bmatrix}\n",
    "1 & x^{(1)} \\\\\n",
    "1 & x^{(2)} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & x^{(N)}\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Although we can work out formulas for $\\hat{\\theta}_1$ and $\\hat{\\theta}_2$, they are not particularly interesting or informative. Instead, we'll focus on some examples of how to use these ideas. A straight-line fit to 50 data points is given [below](#straight_line).\n",
    "\n",
    ":::{figure}../figures/05-straight_line.jpg\n",
    ":label:straight_line\n",
    ":alt:Straight Line fit\n",
    ":width: 400px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    "::::{prf:example}Time Series Trend\n",
    ":label: ex_time_series\n",
    "In this setting, $y^{(i)}$ is the value of a quantity of interest at time $x^{(i)} = i$. The straight line model $\\hat{y}^{(i)} = \\hat{\\theta}_1 + \\hat{\\theta}_2 i$ is called a _trend line_, and $\\vv y - \\hat{\\vv y}$ is called the _de-trended time series_, and $\\hat{\\theta}_2$ is the _trend coefficient_.\n",
    "\n",
    "When the de-trended time series is positive, it means the time series lies above the straight-line fit; when it is negative, it is below the straight-line fit. In the [figures below](#petroleum), we apply this idea to world petroleum consumption. (Can you identify when major geopolitical events occurred based on the de-trended line?)\n",
    "\n",
    ":::{figure}../figures/05-petroleum.jpg\n",
    ":label:petroleum\n",
    ":alt:Petroleum Data fitting\n",
    ":width: 600px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Function: Polynomial Fit\n",
    "\n",
    "\n",
    "A simple extension beyond the straight-line fit is a _polynomial fit_ where we set the $j^{th}$ feature to be\n",
    "\n",
    "$$\n",
    "f_j(x) = x^{j-1}\n",
    "$$\n",
    "\n",
    "for $j = 1,\\ldots,p$. This leads to a model class composed of polynomials of at most degree $p-1$:\n",
    "\n",
    "$$\n",
    "\\hat{f}(x) = \\theta_1 + \\theta_2 x + \\theta_3 x^2 + \\cdots + \\theta_p x^{p-1} \n",
    "$$\n",
    "\n",
    ":::{warning}\n",
    "Here $x^i$ means a scalar raised to the $i^{th}$ power; $x^{(i)}$ means the $i^{th}$ observed scalar data value.\n",
    ":::\n",
    "\n",
    "In this case, our matrix $A \\in \\mathbb{R}^{N \\times p}$ and takes the form\n",
    "\n",
    "$$ A = \\begin{bmatrix}\n",
    "1 & x^{(1)} & \\cdots & (x^{(1)})^{p-1} \\\\\n",
    "1 & x^{(2)} & \\cdots & (x^{(2)})^{p-1} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x^{(N)} & \\cdots & (x^{(N)})^{p-1}\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "which you might recognize as a Vandermonde Matrix, which we encountered earlier in the class when discussing polynomial interpolation. An important property of such matrices is that their columns are linearly independent provided that the numbers $x^{(1)}, \\ldots, x^{(N)}$ include at least $p$ different values. The [figures below](#poly_fit) show examples of least squares fits of polynomials of degree 2, 6, 10, and 15 to a set of 100 data points.\n",
    "\n",
    ":::{figure}../figures/05-poly_fit.jpg\n",
    ":label:poly_fit\n",
    ":alt:Polynomial Model Data fitting\n",
    ":width: 600px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    ":::{important}\n",
    "An important observation is that since any polynomial of degree less than $r$ is also a polynomial of degree less than $s$ if $r \\leq s$, it follows that the MMSE will decrease as we make the polynomial degree larger. This suggests that we should use the largest degree polynomial possible so as to get a model with the smallest MMSE possible. We will see later that this is **NOT TRUE**, and you will explore methods for model selection in recitation and the homework.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Models\n",
    "\n",
    "We now consider the setting of vector-valued independent variables ${\\vv x \\in \\mathbb{R}^n}$. The analog of a straight-line fit here is a _linear regression model_ of the form:\n",
    "$$\n",
    "\\hat{y} = \\hat{f}(\\vv x) = \\boldsymbol \\beta^{\\top} \\vv x + v,\n",
    "$$\n",
    "where $\\boldsymbol\\beta \\in \\mathbb{R}^{n}$ and $v \\in \\mathbb{R}$. If we set $\\theta = \\begin{bmatrix} v \\\\ \\boldsymbol \\beta \\end{bmatrix}$, then the model becomes:\n",
    "\\begin{equation}\n",
    "\\label{regress_eqn}\n",
    "\\hat{y} = \\theta_1 + \\theta_2 x_1 + \\cdots + \\theta_{n+1} x_n.\n",
    "\\end{equation}\n",
    "We can view this as fitting within our general [linear in the parameters model](#LP_eqn) by setting $f_1(\\vv x) = 1$ and $f_i(\\vv x) = x_{i-1}$ for $i = 2, \\ldots, n+1$, so that $p = n+1$.\n",
    "\n",
    "We are of course not obliged to use these features. Instead, suppose that we have $p-1$ features $f_2(\\vv x), \\ldots, f_p(\\vv x)$, and assume we have set $f_1(x) = 1$, as is common done. If we define:\n",
    "$$\n",
    "\\tilde{\\vv x} = \\begin{bmatrix} f_2(x) \\\\ \\vdots \\\\ f_p(x) \\end{bmatrix} \\in \\mathbb{R}^{p-1}\n",
    "$$\n",
    "we can write a linear regression model in the new feature vector $\\tilde{\\vv x}$:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\theta_1 f_1(\\vv x) + \\cdots + \\theta_p f_p(\\vv x) = \\boldsymbol \\beta^{\\top} \\tilde{\\vv x} + v\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "1. $\\tilde{\\vv x} = (f_2(\\vv x), \\ldots, f_p(\\vv x))$ are the _transformed features_\n",
    "2. $v = \\theta_1$ is called the _affine term_\n",
    "3. $\\boldsymbol\\beta = (\\theta_2, \\theta_3, \\ldots, \\theta_p)$ is the _linear term_\n",
    "\n",
    "### Application: Auto-Regressive Time Series Modeling\n",
    "\n",
    "Here is a very widely used application of the above ideas in the context of time-series forecasting. Our goal here is to fit a model that predicts elements of a time series $z_1, z_2, \\ldots,$ where $z_t \\in \\mathbb{R}$ is a scalar quantity of interest.\n",
    "\n",
    "A standard approach is to use an [auto-regressive (AR) prediction model](https://en.wikipedia.org/wiki/Autoregressive_model):\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{AR_eqn}\n",
    "\\hat{z}_{t+1} = \\theta_1 z_t + \\theta_2 z_{t-1} + \\cdots + \\theta_M z_{t-M+1}, \\quad t = M, M+1, \\ldots \\quad (AR)\n",
    "\\end{equation}\n",
    "\n",
    "In [equation (AR)](#AR_eqn), the parameter $M$ is the _memory of the model_, and $\\hat{z}_{t+1}$ is the prediction of the next value based on the previous $M$ observations. We will choose $\\boldsymbol \\theta \\in \\mathbb{R}^M$ to minimize\n",
    "\n",
    "$$\n",
    "(\\hat{z}_{M+1} - z_{M+1})^2 + \\cdots + (\\hat{z}_{T} - z_{T})^2\n",
    "$$\n",
    "We can fit this within our [regression model](#regress_eqn) framework by \n",
    "$$\n",
    "y^{(i)} = z_{M+i}, \\quad \\vv x^{(i)} = \\begin{bmatrix} z_{M+i-1} \\\\ z_{M+i-2} \\\\ \\vdots \\\\ z_{i} \\end{bmatrix} \\in \\mathbb{R}^M, \\quad i = 1, \\ldots, T-M.\n",
    "$$\n",
    "\n",
    "A little bit of bookkeeping allows us to conclude that we have ${N = T-M}$ examples and $p = M$ features.\n",
    "\n",
    "::::{prf:example} LAX Temperature Prediction\n",
    ":label: LAX_tem_eg\n",
    "\n",
    "As an example, consider the time series of hourly temperature at Los Angeles International Airport, May 1-31, 2016, with length $31 \\cdot 24 = 744$. The simple constant prediction $\\hat{z}_{t+1} = 61.76^o \\ \\text{F}$ (the average temperature) has RMS prediction error $3.05^o \\ \\text{F}$ (the standard deviation). The very simple predictor $\\hat{z}_{t+1} = z_t$, i.e., guessing that the temperature next hour is the same as the current temperature, has RMS error $1.16^o \\ \\text{F}$. The predictor $\\hat{z}_{t+1} = z_{t-23}$, i.e., guessing that the temperature next hour is what is was yesterday at the same time, has RMS error $1.73^o \\ \\text{F}$.\n",
    "\n",
    "We fit an AR model with memory $M=8$ using least squares, with $N=31 \\cdot 24 - 8 = 736$ samples. The RMS error of this predictor is $0.98^o \\ \\text{F}$, smaller than the RMS errors for the simple predictors described above. [This figure](#LAX_temp) shows the temperature and the predictions for the first five days.\n",
    "\n",
    ":::{figure}../figures/05-LAX_temp.jpg\n",
    ":label:LAX_temp\n",
    ":alt:LAX temperature prediction\n",
    ":width: 400px\n",
    ":align: center\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection, Generalization, and Validation\n",
    "\n",
    "This section is entirely **practical**: these are standard “tricks of the trade” that you will revisit in more detail in mae advanced classes on statistics and machine learning.\n",
    "\n",
    "Our starting point is a philosophical question: what is the foul of a learned model? Perhaps surprisingly, it is **NOT TO PREDICT OUTCOMES FOR THE GIVEN DATA**; after all, we already have this data! Instead, we want to **predict the outcome on new, unseen data.**\n",
    "\n",
    "If a model makes reasonable predictions on new unseen data, it is said to _generalize_. On the other hand, a model that makes poor predictions on new unseen data, but predicts the given data well, is said to be **over-fit.**\n",
    "\n",
    ":::{note} Validation\n",
    "A simple but effective methal to guess if a model will generalize is called _validation_. The idea is to split your original data into a training set and a test set. Typical splits used in practice are $80 \\%/20\\%$ and $90 \\%/10\\%$.\n",
    ":::\n",
    "\n",
    "Then, we **only** use the _training data_ to fit (or “train”) our model, and then evaluate the model’s performance on the **test set**. If the prediction errors on the training and test sets are similar, then we **guess the model will generalize**. This is rarely guaranteed, but such a comparison is often predictive of a model’s generalization properties.\n",
    "\n",
    "Validation is often used for _model selection_, ie., to choose among different candidate models. For example, by comparing train/test errors, we can select between;\n",
    "\n",
    "1. Polynomial models of different degrees.\n",
    "2. Regression models with different sets of features\n",
    "3. AR models with different memories.\n",
    "\n",
    "::::{prf:example} Errors with Varying Degrees of Polynomial\n",
    "Models are fit using a training set of 100 points, and the plots below show test set of 100 (green) points.\n",
    "\n",
    ":::{figure}../figures/05-data_ls_eg_1.jpg\n",
    ":label:data_ls_eg_1\n",
    ":alt:Polynomial fit 100\n",
    ":width: 400px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    ":::{figure}../figures/05-data_ls_eg_2.jpg\n",
    ":label:data_ls_eg_2\n",
    ":alt:RMS_degree\n",
    ":width: 400px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    "[This plot](#data_ls_eg_2) shows train and test error (RMS standards for root mean square, and is the square root of [MMSE](#MMSE_defn)) vs. the degree of the polynomial being fit. Notice that despite train error decreasing monotonically, test error goes down and then increases as we start to over-fit. This plot suggests that pdynomials of degree 4,5, or 6 will generalize well while achieving a small error.\n",
    "\n",
    "For more about validation and feature engineering, refer to VMLS Ch. 13.2 and 13.3. These are essertivel components of modern data science and machine learning.\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/03_Ch_4_Orthogonality/056-least_squares_data.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
