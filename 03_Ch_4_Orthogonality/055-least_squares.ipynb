{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 4.5 Least Squares\n",
    "subject:  Orthogonality\n",
    "subtitle: approximate linear system\n",
    "short_title: 4.5 Least Squares\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: Orthogonal Projection, Decomposition, Least Squares\n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/03_Ch_4_Orthogonality/055-least_squares.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 08 - Orthogonal Projections and Subspaces, Least Squares Problems and Solutions.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be found in LAA 6.5 and VMLS 12.1.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- the least squares problem\n",
    "- how the least squares problem relates to a solving an approximate linear system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Inconsistent Linear Equations\n",
    "\n",
    "Suppose we are presented with an inconsistent set of linear equations $A \\vv x \\approx \\vv b$. This typically coincides with $A \\in \\mathbb{R}^{m \\times n}$ being a \"tall matrix\", i.e., $m > n$. This corresponds to an overdetermined system of $m$ linear equations in $n$ unknowns. A typical setup assumes this arises is one of data fitting: we are given feature variables $\\vv a_i \\in \\mathbb{R}^n$ and response variables $b_i \\in \\mathbb{R}$, and we believe that $\\vv a_i^{\\top} \\vv x \\approx b_i$ for measurements $i=1,\\ldots, m$ and $\\vv x \\in \\mathbb{R}^n$ are our model parameters. We will revisit this application in detail later.\n",
    "\n",
    "The question then becomes, if no $\\vv x \\in \\mathbb{R}^n$ exists such that $A \\vv x = \\vv b$ exists, what should we do? A natural idea is to select an $\\vv x$ that makes the error or _residual_ $\\vv r = A\\vv x - \\vv b$ as small as possible, i.e., to find the $\\vv x$ that _minimizes_ $\\|\\vv r\\| = \\|A\\vv x - \\vv b\\|$. Now minimizing the residual or its square gives the same answer, so we may as well minimize\n",
    "\\begin{equation}\n",
    "\\label{residual_eqn}\n",
    "\\|A\\vv x - \\vv b\\|^2 = \\|\\vv r\\|^2 = r_1^2 + \\cdots + r_m^2,\n",
    "\\end{equation}\n",
    "\n",
    "the sum of squares of the residuals. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Least Squares Problem\n",
    "\n",
    ":::{prf:definition} Least Squares Problem\n",
    ":label: least-squares-defn\n",
    "The problem of finding $\\hat{\\vv  x} \\in \\mathbb{R}^n$ that minimizes $\\|A \\vv x - \\vv b\\|^2$ over all possible choices of $\\vv x \\in \\mathbb{R}^n$ is called the _least-squares problem_, and is written as:\n",
    "\\begin{equation}\n",
    "\\label{least-squares-eqn}\n",
    "\\textrm{minimize} \\|A \\vv x - \\vv b\\|^2 \\ \\textrm{(LS)}\n",
    "\\end{equation}\n",
    "\n",
    "over the variable $\\vv x$. Any $\\hat{\\vv  x}$ satisfying $\\|A\\hat{\\vv  x} - \\vv b\\|^2 \\leq \\|A\\vv x - \\vv b\\|^2$ for all $\\vv x$ is a solution of the least-squares problem (LS), and is also called a _least-squares approximate solution of $A\\vv x = \\vv b$_.\n",
    ":::\n",
    "\n",
    "### Solving by Orthogonal Projection \n",
    "\n",
    "There are many ways of deriving the solution to [(LS)](#least-squares-defn): you may have seen a vector calculus-based derivation in Math 1410. Here, we will use our new understanding of orthogonal projections to provide an intuitive and elegant _geometric_ derivation.\n",
    "\n",
    "Our starting point is a _column interpretation_ of the least squares objective: let $\\vv a_1, \\ldots, \\vv a_n \\in \\mathbb{R}^m$ be the columns of $A$: then the least squares (LS) problem is the problem of finding a linear combination of the columns that is closest to the vector $\\vv b \\in \\mathbb{R}^m$, with coefficients specified by $\\vv x$:\n",
    "$$\n",
    "\\|A\\vv x - \\vv b\\|^2 = \\|(x_1a_1 + \\cdots + x_na_n) - b\\|^2\n",
    "$$\n",
    "\n",
    ":::{important}\n",
    "Another way of stating the above is we are seeking the vector $A \\hat{\\vv x} \\in$Col$(A)$ in the column space of $A$ that is as close to $\\vv b$ as possible. Perhaps not surprisingly, it turns out this can be computed by taking the _[orthogonal projection](./054-proj_subspace.ipynb#orth-proj) of $\\vv b$ onto Col$(A)$._\n",
    ":::\n",
    "\n",
    ":::{figure}../figures/05-least_squares.jpg\n",
    ":label:least_squares_fig\n",
    ":alt:Least squares\n",
    ":width: 400px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    "To prove the above geometrically intuitive fact (see [](#least_squares_fig)), we need to decompose $\\vv b$ into its orthogonal projection onto Col$(A)$, which we denote by $\\hat{\\vv b}$, and the element in its orthogonal complement Col$(A)$, which we denote by $\\vv e$. Recall $\\vv b, \\hat{\\vv b}, \\vv e\\in \\mathbb{R}^m$ and Col$(A) \\subset \\mathbb{R}^m$. \n",
    "\n",
    "We then have that\n",
    "$$\n",
    "\\vv r = A \\vv x - \\vv b = \\left(A \\vv x - \\hat{\\vv b}\\right) - \\vv e.\n",
    "$$\n",
    "Since $A \\vv x, \\hat{\\vv b} \\in $Col$(A)$, so is $A \\vv x -  \\hat{\\vv b}$ (why?), and thus we have decomposed $\\vv r$ into components lying in Col$(A)$ and Col$(A)^{\\perp}$. Using our generalized Pythagorean theorem, it then follows that \n",
    "$$\n",
    "\\|A \\vv x - \\vv b\\|^2 = \\|\\vv r\\|^2 = \\|A \\vv x - \\hat{\\vv b}\\|^2 + \\|\\vv e\\|^2.\n",
    "$$\n",
    "The above expression can be made as small as possible be choosing $\\hat{\\vv x}$ such that $A\\hat{\\vv x} = \\hat{\\vv b}$, which always has a solution (why?) leaving the residual error\n",
    "$\\|\\vv e\\|^2 = \\|\\vv b - \\hat{\\vv b}\\|^2$, ie, the component of $\\vv b$ that is orthogonal to Col$(A)$.\n",
    "\n",
    "This gives us a nice geometric interpretation of the lest squares solution $\\hat{\\vv x}$, but how should we compute it? We now recall from [here](../03_Ch_4_Orthogonality/054-proj_subspace.ipynb#thm_orth_fund) that Col$(A)^{\\perp} = $Null$(A^{\\top})$. So, we therefore have that $\\vv e \\in $Null$(A^{\\top})$. This means that\n",
    "$$\n",
    "A^{\\top} \\vv e=A^{\\top} \\left(\\vv b - \\hat{\\vv b}\\right) = A^{\\top} \\left(\\vv b - A \\hat{\\vv x}\\right) = 0.\n",
    "$$\n",
    "or, equivalently that\n",
    "\\begin{equation}\n",
    "\\label{norm_eqn}\n",
    "A^{\\top} A \\hat{\\vv x} = A^{\\top} \\vv b. \\ (\\textrm{NE})\n",
    "\\end{equation}\n",
    "The above equations are the _normal equations_ associated with the lest squares problem specified by $A$ and $\\vv b$. We have just informally argued that the set of least squares solutions $\\hat{\\vv x}$ coincide with the set of solutions to the [normal equations (NE)](#norm_eqn): this is in fact true, and can be proven (we wont do that here).\n",
    "\n",
    "Thus, we have reduced solving a least squares problem to our favorite problem, solving a system of linear equations! One question you might have is when do the normal equations (NE) have a _unique solution_? The answer, perhaps unsurprisingly, is when the columns of\n",
    "are linearly independent, and hence form a basis for Col$(A)$. The following theorem is a useful summary of our discussion thus fur:\n",
    "\n",
    ":::{prf:theorem}\n",
    ":label: least_squares_thm\n",
    "Let $A\\in \\mathbb{R}^{m \\times n}$ be an $m \\times n$ matrix. Then the following statements are logically equivalent, i.e., any one being true implies all the other are true):\n",
    "\n",
    "(i) The least squares problem **minimize $\\|A \\vv x - \\vv b\\|^2$** has a unique sdution for any $\\vv b \\in \\mathbb{R}^m$;\n",
    "\n",
    "(ii) The columns of $A$ are linearly independent;\n",
    "\n",
    "(iii) the matrix $A^{\\top}A$ is invertible.\n",
    "\n",
    "When these are true, the unique least squares solution is given by\n",
    "\\begin{equation}\n",
    "\\label{least_squares_thm_eqn}\n",
    " \\hat{\\vv x} = \\left(A^{\\top}A\\right)^{-1}A^{\\top} \\vv b. \\ (\\textrm{XLS})\n",
    "\\end{equation}\n",
    ":::\n",
    "\n",
    ":::{note}\n",
    "The [formula (XLS)](#least_squares_thm_eqn) is useful mainly for theoretical purposes and for hand calculations when $A^{\\top}A$ is a $2 \\times 2$ matrix. Computational approaches are typically based on QR factorizations of $A$ (the QR factorization we saw in class for square matrices can be easily extended to tall matrices with more rows than columns).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/03_Ch_4_Orthogonality/055-least_squares.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
