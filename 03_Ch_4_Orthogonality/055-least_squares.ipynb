{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 4.5 Least Squares\n",
    "subject:  Orthogonality\n",
    "subtitle: approximate linear system\n",
    "short_title: 4.5 Least Squares\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: Orthogonal Projection, Linear Systems, Least Squares\n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/03_Ch_4_Orthogonality/055-least_squares.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 08 - Orthogonal Projections and Subspaces, Least Squares Problems and Solutions.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be found in LAA 6.5 and VMLS 12.1.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- the least squares problem and how to solve it\n",
    "- how the least squares problem relates to a solving an approximate linear system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Inconsistent Linear Equations\n",
    "\n",
    "Suppose we are presented with an inconsistent set of linear equations $A \\vv x \\approx \\vv b$. This typically coincides with $A \\in \\mathbb{R}^{m \\times n}$ being a \"tall matrix\", i.e., $m > n$. This corresponds to an overdetermined system of $m$ linear equations in $n$ unknowns. A typical setup assumes this arises is one of data fitting: we are given feature variables $\\vv a_i \\in \\mathbb{R}^n$ and response variables $b_i \\in \\mathbb{R}$, and we believe that $\\vv a_i^{\\top} \\vv x \\approx b_i$ for measurements $i=1,\\ldots, m$ and $\\vv x \\in \\mathbb{R}^n$ are our model parameters. We will revisit this application in detail later.\n",
    "\n",
    "The question then becomes, if no $\\vv x \\in \\mathbb{R}^n$ exists such that $A \\vv x = \\vv b$ exists, what should we do? A natural idea is to select an $\\vv x$ that makes the error or _residual_ $\\vv r = A\\vv x - \\vv b$ as small as possible, i.e., to find the $\\vv x$ that _minimizes_ $\\|\\vv r\\| = \\|A\\vv x - \\vv b\\|$. Now minimizing the residual or its square gives the same answer, so we may as well minimize\n",
    "\\begin{equation}\n",
    "\\label{residual_eqn}\n",
    "\\|A\\vv x - \\vv b\\|^2 = \\|\\vv r\\|^2 = r_1^2 + \\cdots + r_m^2,\n",
    "\\end{equation}\n",
    "\n",
    "the sum of squares of the residuals. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Least Squares Problem\n",
    "\n",
    ":::{prf:definition} Least Squares Problem\n",
    ":label: least-squares-defn\n",
    "The problem of finding $\\hat{\\vv  x} \\in \\mathbb{R}^n$ that minimizes $\\|A \\vv x - \\vv b\\|^2$ over all possible choices of $\\vv x \\in \\mathbb{R}^n$ is called the _least-squares problem_, and is written as:\n",
    "\\begin{equation}\n",
    "\\label{least-squares-eqn}\n",
    "\\textrm{minimize} \\|A \\vv x - \\vv b\\|^2 \\ \\textrm{(LS)}\n",
    "\\end{equation}\n",
    "\n",
    "over the variable $\\vv x$. Any $\\hat{\\vv  x}$ satisfying $\\|A\\hat{\\vv  x} - \\vv b\\|^2 \\leq \\|A\\vv x - \\vv b\\|^2$ for all $\\vv x$ is a solution of the least-squares problem (LS), and is also called a _least-squares approximate solution of $A\\vv x = \\vv b$_.\n",
    ":::\n",
    "\n",
    "### Solving by Orthogonal Projection \n",
    "\n",
    "There are many ways of deriving the solution to [(LS)](#least-squares-defn): you may have seen a vector calculus-based derivation in Math 1410. Here, we will use our new understanding of orthogonal projections to provide an intuitive and elegant _geometric_ derivation.\n",
    "\n",
    "Our starting point is a _column interpretation_ of the least squares objective: let $\\vv a_1, \\ldots, \\vv a_n \\in \\mathbb{R}^m$ be the columns of $A$: then the least squares (LS) problem is the problem of finding a linear combination of the columns that is closest to the vector $\\vv b \\in \\mathbb{R}^m$, with coefficients specified by $\\vv x$:\n",
    "$$\n",
    "\\|A\\vv x - \\vv b\\|^2 = \\|(x_1a_1 + \\cdots + x_na_n) - b\\|^2\n",
    "$$\n",
    "\n",
    ":::{important}\n",
    "Another way of stating the above is we are seeking the vector $A \\hat{\\vv x} \\in$Col$(A)$ in the column space of $A$ that is as close to $\\vv b$ as possible. Perhaps not surprisingly, it turns out this can be computed by taking the _[orthogonal projection](./054-proj_subspace.ipynb#orth-proj) of $\\vv b$ onto Col$(A)$._\n",
    ":::\n",
    "\n",
    ":::{figure}../figures/05-least_squares.jpg\n",
    ":label:least_squares_fig\n",
    ":alt:Least squares\n",
    ":width: 400px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    "To prove the above geometrically intuitive fact (see [](#least_squares_fig)), we need to decompose $\\vv b$ into its orthogonal projection onto Col$(A)$, which we denote by $\\hat{\\vv b}$, and the element in its orthogonal complement Col$(A)$, which we denote by $\\vv e$. Recall $\\vv b, \\hat{\\vv b}, \\vv e\\in \\mathbb{R}^m$ and Col$(A) \\subset \\mathbb{R}^m$. \n",
    "\n",
    "We then have that\n",
    "$$\n",
    "\\vv r = A \\vv x - \\vv b = \\left(A \\vv x - \\hat{\\vv b}\\right) - \\vv e.\n",
    "$$\n",
    "Since $A \\vv x, \\hat{\\vv b} \\in $Col$(A)$, so is $A \\vv x -  \\hat{\\vv b}$ (why?), and thus we have decomposed $\\vv r$ into components lying in Col$(A)$ and Col$(A)^{\\perp}$. Using our generalized Pythagorean theorem, it then follows that \n",
    "$$\n",
    "\\|A \\vv x - \\vv b\\|^2 = \\|\\vv r\\|^2 = \\|A \\vv x - \\hat{\\vv b}\\|^2 + \\|\\vv e\\|^2.\n",
    "$$\n",
    "The above expression can be made as small as possible be choosing $\\hat{\\vv x}$ such that $A\\hat{\\vv x} = \\hat{\\vv b}$, which always has a solution (why?) leaving the residual error\n",
    "$\\|\\vv e\\|^2 = \\|\\vv b - \\hat{\\vv b}\\|^2$, ie, the component of $\\vv b$ that is orthogonal to Col$(A)$.\n",
    "\n",
    "This gives us a nice geometric interpretation of the lest squares solution $\\hat{\\vv x}$, but how should we compute it? We now recall from [here](../03_Ch_4_Orthogonality/054-proj_subspace.ipynb#thm_orth_fund) that Col$(A)^{\\perp} = $Null$(A^{\\top})$. So, we therefore have that $\\vv e \\in $Null$(A^{\\top})$. This means that\n",
    "$$\n",
    "A^{\\top} \\vv e=A^{\\top} \\left(\\vv b - \\hat{\\vv b}\\right) = A^{\\top} \\left(\\vv b - A \\hat{\\vv x}\\right) = 0.\n",
    "$$\n",
    "or, equivalently that\n",
    "\\begin{equation}\n",
    "\\label{norm_eqn}\n",
    "A^{\\top} A \\hat{\\vv x} = A^{\\top} \\vv b. \\ (\\textrm{NE})\n",
    "\\end{equation}\n",
    "The above equations are the _normal equations_ associated with the lest squares problem specified by $A$ and $\\vv b$. We have just informally argued that the set of least squares solutions $\\hat{\\vv x}$ coincide with the set of solutions to the [normal equations (NE)](#norm_eqn): this is in fact true, and can be proven (we wont do that here).\n",
    "\n",
    "Thus, we have reduced solving a least squares problem to our favorite problem, solving a system of linear equations! One question you might have is when do the normal equations (NE) have a _unique solution_? The answer, perhaps unsurprisingly, is when the columns of\n",
    "are linearly independent, and hence form a basis for Col$(A)$. The following theorem is a useful summary of our discussion thus fur:\n",
    "\n",
    ":::{prf:theorem}\n",
    ":label: least_squares_thm\n",
    "Let $A\\in \\mathbb{R}^{m \\times n}$ be an $m \\times n$ matrix. Then the following statements are logically equivalent, i.e., any one being true implies all the other are true):\n",
    "\n",
    "(i) The least squares problem **minimize $\\|A \\vv x - \\vv b\\|^2$** has a unique sdution for any $\\vv b \\in \\mathbb{R}^m$;\n",
    "\n",
    "(ii) The columns of $A$ are linearly independent;\n",
    "\n",
    "(iii) the matrix $A^{\\top}A$ is invertible.\n",
    "\n",
    "When these are true, the unique least squares solution is given by\n",
    "\\begin{equation}\n",
    "\\label{least_squares_thm_eqn}\n",
    " \\hat{\\vv x} = \\left(A^{\\top}A\\right)^{-1}A^{\\top} \\vv b. \\ (\\textrm{XLS})\n",
    "\\end{equation}\n",
    ":::\n",
    "\n",
    ":::{note}\n",
    "The [formula (XLS)](#least_squares_thm_eqn) is useful mainly for theoretical purposes and for hand calculations when $A^{\\top}A$ is a $2 \\times 2$ matrix. Computational approaches are typically based on QR factorizations of $A$ (the QR factorization we saw in class for square matrices can be easily extended to tall matrices with more rows than columns).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{prf:example} \n",
    ":label: ex_ALA_5_12\n",
    "Consider the linear system\n",
    "\\begin{align*}\n",
    "x_1 + 2x_2 &= 1, \\\\\n",
    "3x_1 - x_2 + x_3 &= 0, \\\\\n",
    "-x_1 + 2x_2 + x_3 &= -1, \\\\\n",
    "x_1 - x_2 - 2x_3 &= 2, \\\\\n",
    "2x_1 + x_2 - x_3 &= 2.\n",
    "\\end{align*}\n",
    "consisting of 5 equations in 3 unknowns. The coeﬃcient matrix and right-hand side are\n",
    "$$\n",
    "A = \\bm\n",
    "1 & 2 & 0 \\\\\n",
    "3 & -1 & 1 \\\\\n",
    "-1 & 2 & 1 \\\\\n",
    "1 & -1 & -2 \\\\\n",
    "2 & 1 & -1\n",
    "\\em, \\quad\n",
    "\\mathbf{b} = \\bm\n",
    "1 \\\\ 0 \\\\ -1 \\\\ 2 \\\\ 2\n",
    "\\em\n",
    "$$\n",
    "\n",
    "A direct application of Gaussian Elimination shows that $\\vv b \\in $img$A$, and so the system is incompatible — it has no solution. Of course, to apply the least squares method, we are not required to check this in advance. If the system has a solution, it is the least squares solution too, and the least squares method will ﬁnd it.\n",
    "\n",
    "Let us ﬁnd the least squares solution based on the Euclidean norm, uisng the [XLS formula](#least_squares_thm_eqn).\n",
    "$$\n",
    "K = A^T A = \\bm\n",
    "16 & -2 & -2 \\\\\n",
    "-2 & 11 & 2 \\\\\n",
    "-2 & 2 & 7\n",
    "\\em, \\quad\n",
    "\\mathbf{f} = A^T \\mathbf{b} = \\bm\n",
    "8 \\\\ 0 \\\\ -7\n",
    "\\em\n",
    "$$\n",
    "Solving the $3 \\times 3$ system of normal equations $K \\vv x = \\vv f$ by Gaussian Elimination, we ﬁnd\n",
    "$$\n",
    "\\mathbf{x}^* = K^{-1}\\mathbf{f} \\approx \\bm .4119 & .2482 & -.9532 \\em^T\n",
    "$$\n",
    "to be the least squares solution to the system. The least squares error is\n",
    "$$\n",
    "\\|\\mathbf{b} - A\\mathbf{x}^*\\|^2 \\approx \\| \\bm -.0917, .0342, .1313, .0701, .0252 \\em ^T\\|^2 \\approx .03236.\n",
    "$$\n",
    "which is reasonably small — indicating that the system is, roughly speaking, not too\n",
    "incompatible.\n",
    "\n",
    "An alternative strategy is to begin by orthonormalizing the columns of $A$ using Gram–\n",
    "Schmidt. We can then apply the [orthogonal projection formula](./054-proj_subspace.ipynb#orth_basis_proj_eqn) to construct the\n",
    "same least squares solution. We suggest you to try this strategy as an exercise.\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex_LAA_1\n",
    "\n",
    "Find a least-squares solution of the inconsistent system $A\\mathbf{x} = \\mathbf{b}$ for\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "4 & 0 \\\\\n",
    "0 & 2 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix}, \\quad\n",
    "\\mathbf{b} = \\begin{bmatrix}\n",
    "2 \\\\\n",
    "0 \\\\\n",
    "11\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    ":::{solution} ex_LAA_1\n",
    ":class: dropdown\n",
    "\n",
    "To use [normal equations (NE)](#norm_eqn), compute:\n",
    "\n",
    "$$\n",
    "A^TA = \\begin{bmatrix}\n",
    "4 & 0 & 1 \\\\\n",
    "0 & 2 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "4 & 0 \\\\\n",
    "0 & 2 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "17 & 1 \\\\\n",
    "1 & 5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^T\\mathbf{b} = \\begin{bmatrix}\n",
    "4 & 0 & 1 \\\\\n",
    "0 & 2 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "2 \\\\\n",
    "0 \\\\\n",
    "11\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "19 \\\\\n",
    "11\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then the equation $A^TA\\mathbf{x} = A^T\\mathbf{b}$ becomes\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "17 & 1 \\\\\n",
    "1 & 5\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "19 \\\\\n",
    "11\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Row operations can be used to solve this system, but since $A^TA$ is invertible and $2 \\times 2$, it is probably faster to compute\n",
    "\n",
    "$$\n",
    "(A^TA)^{-1} = \\frac{1}{84}\n",
    "\\begin{bmatrix}\n",
    "5 & -1 \\\\\n",
    "-1 & 17\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and then to solve $A^TA\\mathbf{x} = A^T\\mathbf{b}$ as\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{\\mathbf{x}} &= (A^TA)^{-1}A^T\\mathbf{b} \\\\\n",
    "&= \\frac{1}{84}\n",
    "\\begin{bmatrix}\n",
    "5 & -1 \\\\\n",
    "-1 & 17\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "19 \\\\\n",
    "11\n",
    "\\end{bmatrix} = \n",
    "\\frac{1}{84}\n",
    "\\begin{bmatrix}\n",
    "84 \\\\\n",
    "168\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "2\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    ":::\n",
    "::::\n",
    "\n",
    "\n",
    "::::{exercise}\n",
    ":label: ex_LAA_2\n",
    "\n",
    "Find a least-squares solution of $A\\mathbf{x} = \\mathbf{b}$ for\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "1 & 1 & 0 & 0 \\\\\n",
    "1 & 1 & 0 & 0 \\\\\n",
    "1 & 0 & 1 & 0 \\\\\n",
    "1 & 0 & 1 & 0 \\\\\n",
    "1 & 0 & 0 & 1 \\\\\n",
    "1 & 0 & 0 & 1\n",
    "\\end{bmatrix}, \\quad\n",
    "\\mathbf{b} = \\begin{bmatrix}\n",
    "-3 \\\\\n",
    "-1 \\\\\n",
    "0 \\\\\n",
    "2 \\\\\n",
    "5 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    ":::{solution} ex_LAA_2\n",
    ":class: dropdown\n",
    "\n",
    "Compute\n",
    "\n",
    "$$\n",
    "A^TA = \n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & 1 & 1 & 1 \\\\\n",
    "1 & 1 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 0 & 0 \\\\\n",
    "1 & 1 & 0 & 0 \\\\\n",
    "1 & 0 & 1 & 0 \\\\\n",
    "1 & 0 & 1 & 0 \\\\\n",
    "1 & 0 & 0 & 1 \\\\\n",
    "1 & 0 & 0 & 1\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "6 & 2 & 2 & 2 \\\\\n",
    "2 & 2 & 0 & 0 \\\\\n",
    "2 & 0 & 2 & 0 \\\\\n",
    "2 & 0 & 0 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^T\\mathbf{b} = \n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & 1 & 1 & 1 \\\\\n",
    "1 & 1 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "-3 \\\\\n",
    "-1 \\\\\n",
    "0 \\\\\n",
    "2 \\\\\n",
    "5 \\\\\n",
    "1\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "4 \\\\\n",
    "-4 \\\\\n",
    "2 \\\\\n",
    "6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The augmented matrix for $A^TA\\mathbf{x} = A^T\\mathbf{b}$ is\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "6 & 2 & 2 & 2 & 4 \\\\\n",
    "2 & 2 & 0 & 0 & -4 \\\\\n",
    "2 & 0 & 2 & 0 & 2 \\\\\n",
    "2 & 0 & 0 & 2 & 6\n",
    "\\end{bmatrix} \\sim\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 1 & 3 \\\\\n",
    "0 & 1 & 0 & -1 & -5 \\\\\n",
    "0 & 0 & 1 & -1 & -2 \\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The general solution is $x_1 = 3 - x_4$, $x_2 = -5 + x_4$, $x_3 = -2 + x_4$, and $x_4$ is free. So the general least-squares solution of $A\\mathbf{x} = \\mathbf{b}$ has the form\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}} = \n",
    "\\begin{bmatrix}\n",
    "3 \\\\\n",
    "-5 \\\\\n",
    "-2 \\\\\n",
    "0\n",
    "\\end{bmatrix} + x_4\n",
    "\\begin{bmatrix}\n",
    "-1 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    ":::\n",
    "::::\n",
    "\n",
    "\n",
    ":::::{exercise}\n",
    ":label: ex_LAA_3\n",
    "\n",
    "Given $A$ and $\\mathbf{b}$ as in [](#ex_LAA_1), determine the least-squares error in the least-squares solution of $A\\mathbf{x} = \\mathbf{b}$.\n",
    "\n",
    "::::{solution} ex_LAA_3\n",
    ":class: dropdown \n",
    "\n",
    "From [](#ex_LAA_1),\n",
    "\n",
    "$$\n",
    "\\mathbf{b} = \\begin{bmatrix}\n",
    "2 \\\\\n",
    "0 \\\\\n",
    "11\n",
    "\\end{bmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "A\\hat{\\mathbf{x}} = \\begin{bmatrix}\n",
    "4 & 0 \\\\\n",
    "0 & 2 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "2\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "4 \\\\\n",
    "4 \\\\\n",
    "3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\n",
    "\\mathbf{b} - A\\hat{\\mathbf{x}} = \n",
    "\\begin{bmatrix}\n",
    "2 \\\\\n",
    "0 \\\\\n",
    "11\n",
    "\\end{bmatrix} - \n",
    "\\begin{bmatrix}\n",
    "4 \\\\\n",
    "4 \\\\\n",
    "3\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "-2 \\\\\n",
    "-4 \\\\\n",
    "8\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{b} - A\\hat{\\mathbf{x}}\\| = \\sqrt{(-2)^2 + (-4)^2 + 8^2} = \\sqrt{84}\n",
    "$$\n",
    "\n",
    "The least-squares error is $\\sqrt{84}$. For any $\\mathbf{x}$ in $\\mathbb{R}^2$, the distance between $\\mathbf{b}$ and the vector $A\\mathbf{x}$ is at least $\\sqrt{84}$. See [](#ex_LAA_3_fig). Note that the least-squares solution $\\hat{\\mathbf{x}}$ itself does not appear in the figure.\n",
    "\n",
    ":::{figure}../figures/05-ex_LAA_3.jpg\n",
    ":label:ex_LAA_3_fig\n",
    ":alt:ex_LAA_3\n",
    ":width: 300px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    "::::\n",
    ":::::\n",
    "\n",
    "\n",
    "::::{exercise}\n",
    ":label: exact\n",
    "\n",
    "Find a least-squares solution of $A\\mathbf{x} = \\mathbf{b}$ for\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "1 & -2 \\\\\n",
    "5 & 3\n",
    "\\end{bmatrix}, \\quad\n",
    "\\mathbf{b} = \\begin{bmatrix}\n",
    "8 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    ":::{solution} exact\n",
    ":class: dropdown\n",
    "\n",
    "Compute\n",
    "\n",
    "$$\n",
    "A^TA = \n",
    "\\begin{bmatrix}\n",
    "26 & 13 \\\\ 13 & 13\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^T\\mathbf{b} = \n",
    "\\begin{bmatrix}\n",
    "1 & 5 \\\\\n",
    "-2 & 3\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "8 \\\\ 1\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "13 \\\\ -13\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The augmented matrix for $A^TA\\mathbf{x} = A^T\\mathbf{b}$ is\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "26 & 13 & 13 \\\\ 13 & 13 & -13\n",
    "\\end{bmatrix} \\sim\n",
    "\\begin{bmatrix}\n",
    "26 & 13 & 13 \\\\ 0 & 6.5 & -19.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Using backsubstitution, the solution to the above system is $x_2 = \\frac{-19.5}{6.5} = -3$, $x_1 = \\frac{13 - 13x_2}{26} = 2$. So the least-squares solution of $A\\mathbf{x} = \\mathbf{b}$ is \n",
    "\n",
    "$$\n",
    "\\vv x = \n",
    "\\begin{bmatrix}\n",
    "2 \\\\ -3\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The least squares error is computed as below\n",
    "\n",
    "$$\n",
    "\\mathbf{b} - A\\hat{\\mathbf{x}} = \n",
    "\\begin{bmatrix}\n",
    "8 \\\\ 1\n",
    "\\end{bmatrix} - \n",
    "\\begin{bmatrix}\n",
    "1 & -2 \\\\\n",
    "5 & 3\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "2 \\\\ -3\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "8 \\\\ 1\n",
    "\\end{bmatrix} - \\begin{bmatrix}\n",
    "8 \\\\ 1\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0 \\\\ 0\n",
    "\\end{bmatrix} \\Rightarrow \\|\\mathbf{b} - A\\hat{\\mathbf{x}}\\| = 0.\n",
    "$$\n",
    "\n",
    "Hence, $\\hat{\\vv x}$ is an exact solution for the equation $A \\vv x = \\vv b$, which we found out by solving least squares! Therefore, if an exact solution exists for $A \\vv x = \\vv b$, then, our least squares solution strategy indeed finds it!\n",
    "\n",
    "\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python break!\n",
    "\n",
    "In the following code, we show how to use `np.linalg.lstsq` in Python to solve the least squares problem, and also how to obtain the solution by solving a linear system (`np.linalg.solve`) as illustrated in [](#ex_ALA_5_12). If there is more than one solution to the least squares problem, then the two strategies ( `np.linalg.lstsq` and `np.linalg.solve`) might possibly return different solutions $\\hat{\\vv x}$ because each NumPy function uses a different numerical strategy to obtain $\\hat{\\vv x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A: \n",
      " [[ 1 -2]\n",
      " [ 5  3]] \n",
      "b:  [8 1]\n",
      "\n",
      "lstsq function\n",
      "\n",
      "Solution (x): \n",
      " [ 2. -3.] \n",
      "Residual:  []\n",
      "\n",
      "solving a linear system\n",
      "\n",
      "Solution (x): \n",
      " [ 2. -3.] \n",
      "Residual:  0.0\n",
      "\n",
      "A: \n",
      " [[1 1 0 0]\n",
      " [1 1 0 0]\n",
      " [1 0 1 0]\n",
      " [1 0 1 0]\n",
      " [1 0 0 1]\n",
      " [1 0 0 1]] \n",
      "b:  [-3 -1  0  2  5  1]\n",
      "\n",
      "lstsq function\n",
      "\n",
      "Solution (x): \n",
      " [ 0.5 -2.5  0.5  2.5] \n",
      "Residual:  []\n",
      "\n",
      "solving a linear system\n",
      "\n",
      "Solution (x): \n",
      " [-6.  4.  7.  9.] \n",
      "Residual:  11.999999999999998\n",
      "\n",
      "A: \n",
      " [[4 0]\n",
      " [0 2]\n",
      " [1 1]] \n",
      "b:  [ 2  0 11]\n",
      "\n",
      "lstsq function\n",
      "\n",
      "Solution (x): \n",
      " [1. 2.] \n",
      "Residual:  [84.]\n",
      "\n",
      "solving a linear system\n",
      "\n",
      "Solution (x): \n",
      " [1. 2.] \n",
      "Residual:  84.0\n"
     ]
    }
   ],
   "source": [
    "# Least squares\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def least_squares_linalg(A, b):\n",
    "\n",
    "    print(\"\\nA: \\n\", A, \"\\nb: \", b)\n",
    "\n",
    "    print(\"\\nlstsq function\\n\")\n",
    "    \n",
    "    x, residual, rank, sing_val = np.linalg.lstsq(A, b, rcond=None)\n",
    "    # residual = 0 if rank of A < size of x (or) number of rows of A <= size of x \n",
    "    print(\"Solution (x): \\n\", x, \"\\nResidual: \", residual)\n",
    "\n",
    "def least_squares(A, b):\n",
    "    print(\"\\nsolving a linear system\\n\")\n",
    "\n",
    "    x = np.linalg.solve(A.T @ A, A.T @ b)\n",
    "    \n",
    "    residual = np.linalg.norm(A@x- b)**2\n",
    "    \n",
    "    print(\"Solution (x): \\n\", x, \"\\nResidual: \", residual)\n",
    "\n",
    "A = np.array([[1, -2],\n",
    "              [5, 3]])\n",
    "b = np.array([8, 1])\n",
    "\n",
    "least_squares_linalg(A, b)\n",
    "least_squares(A, b)\n",
    "\n",
    "A1 = np.array([[1, 1, 0, 0],\n",
    "              [1, 1, 0, 0],\n",
    "              [1, 0, 1 , 0],\n",
    "              [1, 0, 1, 0],\n",
    "              [1, 0, 0, 1],\n",
    "              [1, 0, 0, 1]])\n",
    "b1 = np.array([-3, -1, 0, 2, 5, 1])\n",
    "\n",
    "# Notice the difference in both the solutions\n",
    "least_squares_linalg(A1, b1)\n",
    "least_squares(A1, b1)\n",
    "\n",
    "A2 = np.array([[4, 0], [0, 2],[1, 1]])\n",
    "b2 = np.array([2, 0, 11])\n",
    "\n",
    "least_squares_linalg(A2, b2)\n",
    "least_squares(A2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/03_Ch_4_Orthogonality/055-least_squares.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
