{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 7.2 Repeated Eigen Values, Jordan Forms, and Linear Dynamical Systems\n",
    "subject: Dynamics\n",
    "subtitle: multiplicity of eigen values\n",
    "short_title: 7.2 Repeated Eigen Values, Jordan Forms, and Linear Dynamical Systems\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: Jordan matrices, algebraic multiplicity, geometric multiplicity \n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/06_Ch_7_Dynamics/082-rep_eig_jord_LDS.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 13 - Complex and Repeated Eigenvalues Revisited, Jordan Blocks, Matrix Exponential.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be found in ALA 8.6, 10.1 and 10.3.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- examples of matrices with repeated eigen values\n",
    "- what are Jordan Forms\n",
    "- algebraic and geometric multiplicity of eigen values\n",
    "- how to solve linear dynamical systems with repeated eigen values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeated Eigenvalues, Jordan Forms, and Linear Dynamical Systems\n",
    "\n",
    "Let's revisit the matrix $A = \\begin{bmatrix} 2 & 1 \\\\ 0 & 2 \\end{bmatrix}$ we saw in the previous lecture. This matrix has an eigenvalue $\\lambda = 2$ of algebraic multiplicity 2: ($\\det(A-\\lambda I) = (\\lambda-2)^2 = 0$ $\\Leftrightarrow \\lambda_1 = \\lambda_2 = 2$) but geometric multiplicity 1, i.e., only one linearly independent eigenvector\n",
    "$$\n",
    "\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix},\n",
    "$$\n",
    "exists. How can we solve $\\dot{\\mathbf{x}} = A\\mathbf{x}$ in this case? Taking the approach that we've seen so far, we would write a candidate solution as\n",
    "$$\n",
    "\\mathbf{x}(t) = c_1 e^{2t} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "But this won't work! What if $\\mathbf{x}(0) = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$? There is no $c_1 \\in \\mathbb{R}$ such that $\\mathbf{x}(0) = \\begin{bmatrix} c_1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$. Does this mean no solution to $\\dot{\\mathbf{x}} = A\\mathbf{x}$ exists? This would be deeply unsettling! The issue here is that we are \"missing\" an eigenvector. To remedy this, we'll introduce the idea of a _generalized eigenvector_. We will only consider 2$\\times$2 matrices, in which case a generalized eigenvector $\\mathbf{v}_2$ for an eigenvalue $\\lambda$ with eigenvector $\\mathbf{v}_1$ is given by the solution to the linear system:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{lin_sys_eig}\n",
    "(A - \\lambda I)\\mathbf{v}_2 = \\mathbf{v}_1.\n",
    "\\end{equation}\n",
    "\n",
    "For our example, we compute $\\mathbf{v}_2$ by solving:\n",
    "\n",
    "\\begin{align*}\n",
    "\\left(\\begin{bmatrix}\n",
    "2 & 1 \\\\\n",
    "0 & 2\n",
    "\\end{bmatrix} - 2\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\\right) \\begin{bmatrix}\n",
    "v_{21} \\\\\n",
    "v_{22}\n",
    "\\end{bmatrix} &= \\begin{bmatrix}\n",
    "0 & 1 \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "v_{21} \\\\\n",
    "v_{22}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "v_{22} \\\\\n",
    "0\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix} = \\mathbf{v}_1 \\\\[10pt]\n",
    "&\\Rightarrow v_{22} = 1 \\text{ and } v_{21} \\text{ is free. We set } v_{21} = 0 \\text{ and find} \\\\[5pt]\n",
    "&\\mathbf{v}_2 = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix} \\text{ (any choice for } v_{21} \\text{ would work, this is just a convenient choice).}\n",
    "\\end{align*}\n",
    "\n",
    "Now, how can we construct a solution using $\\mathbf{v}_2$? If we try the strategy we used for eigenvalue/vector pairs, things do not quite work out:\n",
    "\n",
    "If $\\mathbf{x}_2(t) = e^{2t}\\mathbf{v}_2$ then $\\dot{\\mathbf{x}}_2(t) = 2e^{2t}\\mathbf{v}_2 = 2\\mathbf{x}_2$ but $A\\mathbf{x}_2(t) = A(e^{2t}\\mathbf{v}_2) = e^{2t}(2\\mathbf{v}_2 + \\mathbf{v}_1) = 2\\mathbf{x}_2 + \\mathbf{v}_1e^{2t}$,\n",
    "\n",
    "where we used the fact that the generalized eigenvector $\\mathbf{v}_2$ satisfies\n",
    "\n",
    "$A\\mathbf{v}_2 = \\lambda\\mathbf{v}_2 + \\mathbf{v}_1$,\n",
    "\n",
    "which is obtained by rearranging [](#lin_sys_eig). So we'll have to try something else. Let's see if\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_2(t) = e^{2t}\\mathbf{v}_2 + te^{2t}\\mathbf{v}_1\n",
    "$$\n",
    "\n",
    "does better. This guess is made because we need to find a way to have $e^{2t}\\mathbf{v}_1$ appear in $\\dot{\\mathbf{x}}$.\n",
    "\n",
    "First we compute \n",
    "$$\n",
    "\\dot{\\mathbf{x}}_2 &= 2e^{2t}\\mathbf{v}_2 + e^{2t}\\mathbf{v}_1 + 2te^{2t}\\mathbf{v}_1 \\\\\n",
    "                    &= 2(e^{2t}\\mathbf{v}_2 + te^{2t}\\mathbf{v}_1) + e^{2t}\\mathbf{v}_1 \\\\\n",
    "                    &= 2\\mathbf{x}_2 + e^{2t}\\mathbf{v}_1\n",
    "                    $$\n",
    "\n",
    "This looks promising! Now let's check\n",
    "\n",
    "\\begin{align*}\n",
    "A\\mathbf{x}_2(t) = A(e^{2t}\\mathbf{v}_2 + te^{2t}\\mathbf{v}_1) &= 2e^{2t}\\mathbf{v}_2 + e^{2t}\\mathbf{v}_1 + 2te^{2t}\\mathbf{v}_1 \\\\\n",
    "&= 2(e^{2t}\\mathbf{v}_2 + te^{2t}\\mathbf{v}_1) + e^{2t}\\mathbf{v}_1 \\\\\n",
    "&= 2\\mathbf{x}_2 + e^{2t}\\mathbf{v}_1.\n",
    "\\end{align*}\n",
    "\n",
    "Success! We therefore can write solutions to our initial value problem as linear combinations of\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{x}_1(t) &= e^{2t}\\mathbf{v}_1 \\quad \\text{and} \\quad \\mathbf{x}_2(t) = e^{2t}\\mathbf{v}_2 + te^{2t}\\mathbf{v}_1, \\\\\n",
    "\\text{i.e.,} \\quad \\mathbf{x}(t) &= (c_1 + c_2t)e^{2t}\\mathbf{v}_1 + c_2e^{2t}\\mathbf{v}_2.\n",
    "\\end{align*}\n",
    "\n",
    "Let's check if we can find $c_1$ and $c_2$ so that $\\mathbf{x}(0) = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{x}(0) = c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 = \\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\Rightarrow c_1 = 0, \\, c_2 = 1\n",
    "\\end{align*}\n",
    "\n",
    "and $\\mathbf{x}(t) = \\begin{bmatrix} te^{2t} \\\\ e^{2t} \\end{bmatrix}$ is the solution to our initial value problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2$\\times$2 Jordan Blocks\n",
    "\n",
    "In the complete matrix setting, we saw that we could diagonalize the matrix $A$ using a similarity transformation defined by the eigenvectors of $A$, i.e., for $V = [\\mathbf{v}_1 \\, \\mathbf{v}_2 \\, \\ldots \\, \\mathbf{v}_n]$, we have that\n",
    "\n",
    "$$\n",
    "A = V^{-1}\\Lambda V, \\text{ or equivalently, } A = V\\Lambda V^{-1}, \\quad \\Lambda = \\text{diag}(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n).\n",
    "$$\n",
    "\n",
    "We saw that this was very useful when solving systems of linear equations.\n",
    "\n",
    "In the case of incomplete matrices, similarity transformations defined in terms of _generalized eigenvectors_ and _Jordan blocks_ play an analogous role.\n",
    "\n",
    "For example, consider the matrix $A = \\begin{bmatrix} 1 & 1 \\\\ -1 & 3 \\end{bmatrix}$. This matrix has a repeated eigenvalue at $\\lambda = 2$, and one eigenvector ${\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}}$. We therefore compute the generalized eigenvector by solving $(A-\\lambda I)\\mathbf{v}_2 = \\mathbf{v}_1$:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} -1 & 1 \\\\ -1 & 1 \\end{bmatrix}\\begin{bmatrix} v_{21} \\\\ v_{22} \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\Rightarrow -v_{21} + v_{22} = 1\n",
    "$$\n",
    "\n",
    "One solution is $\\mathbf{v}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$. We construct our similarity transformation as before, and set $V = \\bm \\mathbf{v}_1 & \\mathbf{v}_2 \\em = \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix}$, and compute $V^{-1} = \\begin{bmatrix} 1 & 0 \\\\ -1 & 1 \\end{bmatrix}$\n",
    "\n",
    "Let's see what happens if we compute $V^{-1}AV$. In the complete case, this would give us a diagonal matrix. In this case, we get\n",
    "$$\n",
    "\\begin{bmatrix} 1 & 0 \\\\ -1 & 1 \\end{bmatrix}\\begin{bmatrix} 1 & 1 \\\\ -1 & 3 \\end{bmatrix}\\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 \\\\ 0 & 2 \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "which we'll recognize as our previous example! It turns out that **all** $2 \\times 2$ matrices with $\\lambda=2$ having algebraic multiplicity 2 and geometric multiplicity 1 are similar to the _Jordan Block_\n",
    "\n",
    "$$\n",
    "J = \\begin{bmatrix} 2 & 1 \\\\ 0 & 2 \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "and this similarity transformation is defined by $V = \\bm \\mathbf{v}_1 & \\mathbf{v}_2 \\em$ composed of the eigenvector $\\mathbf{v}_1$ and generalized eigenvector $\\mathbf{v}_2$ of the original matrix.\n",
    "\n",
    "We can generalize this idea to any $2 \\times 2$ matrix with only one eigenvector:\n",
    "\n",
    ":::{prf:theorem}\n",
    ":label: algeb_geo_mult_thm\n",
    "Let $A \\in \\mathbb{R}^{2\\times2}$ have eigenvalue $\\lambda$ with algebraic multiplicity 2 and geometric multiplicity 1. Let $\\mathbf{v}_1$ and $\\mathbf{v}_2$ satisfy\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{algeb_geo_mult_thm_eqn}\n",
    "(A-\\lambda I)\\mathbf{v}_1 = \\mathbf{0} \\quad \\text{and} \\quad (A-\\lambda I)\\mathbf{v}_2 = \\mathbf{v}_1.\n",
    "\\end{equation}\n",
    "\n",
    "Then $A = VJ_\\lambda V^{-1}$, where $V = \\bm \\mathbf{v}_1 & \\mathbf{v}_2\\em$ and $J_\\lambda$ is the Jordan Block\n",
    "\n",
    "$$\n",
    "J_\\lambda = \\begin{bmatrix} \\lambda & 1 \\\\ 0 & \\lambda \\end{bmatrix}\n",
    "$$\n",
    ":::\n",
    "\n",
    "\n",
    "Using [this theorem](#algeb_geo_mult_thm), we can conclude, much in the same way we did for diagonalizable $A$, that if $A = VJ_\\lambda V^{-1}$, then\n",
    "\n",
    "$$\n",
    "\\mathbf{x}(t) = (c_1 + c_2t)e^{\\lambda t}\\mathbf{v}_1 + c_2e^{\\lambda t}\\mathbf{v}_2\n",
    "$$\n",
    "\n",
    "is a general solution to $\\dot{\\mathbf{x}} = A\\mathbf{x}$.\n",
    "\n",
    "\n",
    ":::{note}\n",
    "This is a very specific instantiation of the Jordan Canonical Form of a matrix. You will learn more about the Jordan Canonical Form and its implications on differential equations in ESE 2100. For those interested in the fully general theorem statement, see ALA 8.6, Theorem 8.51.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/06_Ch_7_Dynamics/082-rep_eig_jord_LDS.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
