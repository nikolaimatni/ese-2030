{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 7.3 Matrix Exponential\n",
    "subject: Dynamics\n",
    "subtitle: power series of a matrix\n",
    "short_title: 7.3 Matrix Exponential\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: power series, general solution, linear dynamical systems\n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/06_Ch_7_Dynamics/083-Matrix_exponential.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 13 - Complex and Repeated Eigenvalues Revisited, Jordan Blocks, Matrix Exponential.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be found in ALA 10.4.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- how to define the matrix exponential as a power series,\n",
    "- how to solve linear ODEs with the matrix exponential,\n",
    "- how to compute the matrix exponential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Matrix Exponential\n",
    "\n",
    "We've seen four cases for eigenvalues/eigenvectors and their relationship to solutions of initial value problems defined by $\\dot{\\mathbf{x}} = A\\mathbf{x}$ and $\\mathbf{x}(0)$ given:\n",
    "\n",
    "1) real distinct eigenvalues, solved by diagonalization;\n",
    "2) real repeated eigenvalues with algebraic multiplicity = geometric multiplicity, also solved by diagonalization;\n",
    "3) complex distinct eigenvalues, solved by diagonalization and applying Euler's formula to define real-valued eigenfunctions;\n",
    "4) repeated eigenvalues with algebraic multiplicity > geometric multiplicity, solved by Jordan decomposition using generalized eigenvectors.\n",
    "\n",
    "While correct, the fact that there are four different cases we need to consider is somewhat unsatisfying. In this section, we show that by appropriately defining a _matrix exponential_, we can provide a unified treatment of all the aforementioned settings.\n",
    "\n",
    "We start by recalling the power series definition for the scalar exponential $e^x$, for $x \\in \\mathbb{R}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{ps}\n",
    "e^x = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots = \\sum_{k=0}^{\\infty} \\frac{x^k}{k!}, \\quad (\\text{PS})\n",
    "\\end{equation}\n",
    "\n",
    "where we recall that $k! = 1 \\cdot 2 \\cdots (k-1) \\cdot k$. We know that for the scalar initial value problem $\\dot{x} = ax$, the solution is $x(t) = e^{at}x(0)$, where $e^{at}$ can be computed via ([PS](#ps)) by setting $x = at$.\n",
    "\n",
    "Wouldn't it be cool if we could do something similar for the vector valued initial value problem defined by $\\dot{\\mathbf{\\vv x}} = A\\mathbf{\\vv x}$? Does there exist a function, call it $e^{At}$, so that $\\mathbf{x}(t) = e^{At}\\mathbf{x}(0)$? How would we even begin to define such a thing?\n",
    "\n",
    "Let's do the \"obvious\" thing and start with the definition ([PS](#ps)), and replace the scalar $x$ with a matrix $X$ to obtain the _matrix exponential of X_:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{MPS}\n",
    "e^X = I + X + \\frac{X^2}{2!} + \\frac{X^3}{3!} + \\cdots = \\sum_{k=0}^{\\infty} \\frac{X^k}{k!}, \\quad (\\text{MPS})\n",
    "\\end{equation}\n",
    "\n",
    "Although we can't prove it yet, it can be shown that ([MPS](#MPS)) converges for any $X$, so this is a well defined object. Does ([MPS](#MPS)) help with solving $\\dot{\\mathbf{\\vv x}} = A\\mathbf{\\vv x}$? Let's try the test solution $\\mathbf{\\vv x}(t) = e^{At}\\mathbf{\\vv x}(0)$ â€” this is exactly what we did for the scalar setting, but we replace $e^{at}$ with $e^{At}$. Is this a solution to $\\dot{\\mathbf{\\vv x}} = A\\mathbf{\\vv x}$?\n",
    "\n",
    "First, we compute $A\\mathbf{\\vv x}(t) = Ae^{At}\\mathbf{\\vv x}(0)$. Next, we need to compute $\\frac{d}{dt}e^{At}\\mathbf{\\vv x}(0)$. But how do we do this? We will rely on ([MPS](#MPS)):\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d}{dt} e^{At} \\vv x(0) &= \\frac{d}{dt} \\left(I + At + \\frac{(At)^2}{2!} + \\frac{(At)^3}{3!} + \\cdots\\right) \\\\\n",
    "\n",
    "&= \\frac{d}{dt}I + \\frac{d}{dt}At + \\frac{d}{dt}\\frac{A^2t^2}{2!} + \\frac{d}{dt}\\frac{A^3t^3}{3!} + \\cdots \\\\\n",
    "\n",
    "&= 0 + A + A^2t + A^3\\frac{t^2}{2} + \\cdots \\\\\n",
    "\n",
    "&= A + A^2t + \\frac{A^3t^2}{2!} + \\frac{A^4t^3}{3!} + \\cdots \\\\\n",
    "\n",
    "&= A\\left(I + At + \\frac{A^2t^2}{2!} + \\frac{A^3t^3}{3!} + \\cdots\\right) \\\\\n",
    "\n",
    "&= A e^{At} \\vv x(0).\n",
    "\\end{equation}\n",
    "\n",
    "This worked, and we have found a general solution to $\\dot{\\vv x} =  \\vv Ax$ defined in terms of the matrix exponential!\n",
    "\n",
    ":::{prf:theorem}\n",
    ":label: MPS_thm\n",
    "Consider the initial value problem $\\dot{\\vv x} = A \\vv x$, with $\\vv x(0)$ specified. Its\n",
    "solution is given by $\\vv x(t) = e^{At} \\vv x(0)$, where $e^{At}$ is defined according to the matrix power series [MPS](#MPS).\n",
    ":::\n",
    "\n",
    "This is very satisfying, as now our scalar and vector-valued problems have similar looking solutions defined in terms of appropriate exponential functions. The only thing that remains is to compute $e^{At}$! How do we do this? This is where all of the work we've done on diagonalization and Jordan forms really pays off!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Matrix Exponential\n",
    "\n",
    "### Case 1: Real eigenvalues, diagonalizable $A$\n",
    "\n",
    "Suppose that $A \\in \\mathbb{R}^{n\\times n}$ and has eigenvalues $\\lambda_1, \\lambda_2, \\ldots, \\lambda_n $ with corresponding linearly independent eigenvectors $ \\vv v_1, \\vv v_2, \\ldots,\\vv v_n$. Then we can write\n",
    "\n",
    "$$\n",
    "A = V \\Lambda V^{-1}, \\text{ for } V = \\bm \\vv v_1, \\vv v_2, \\ldots, \\vv v_n\\em \\text{ and } \\Lambda = \\text{diag}(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n).\n",
    "$$\n",
    "\n",
    "To compute $e^{At}$ we need to compute powers $(At)^k$. Let's work a few of these out using $A = V\\Lambda V^{-1}$:\n",
    "\n",
    "\\begin{align*}\n",
    "(At)^0 = I, \\quad At = V\\Lambda V^{-1}t, \\quad A^2t^2 &= (V\\Lambda V^{-1})(V\\Lambda V^{-1})t^2,\n",
    "& A^3t^3 &= (V\\Lambda V^{-1})A^2t^3  \\\\\n",
    "&= V\\Lambda^2 V^{-1}t^2 & &=(V\\Lambda V^{-1})(V\\Lambda^2 V^{-1})t^3  \\\\\n",
    "& & &= V\\Lambda^3 V^{-1}t^3\n",
    "\\end{align*}\n",
    "\n",
    "There is a pattern: $(At)^k = V \\Lambda^k V^{-1} t^k$.  This is nice, since computing powers of diagonal matrices is easy:\n",
    "$$\n",
    "\\Lambda^k = \\begin{bmatrix}\n",
    "\\lambda_1 & & \\\\\n",
    "& \\ddots & \\\\\n",
    "& & \\lambda_n\n",
    "\\end{bmatrix}^k = \\begin{bmatrix}\n",
    "\\lambda_1^k & & \\\\\n",
    "& \\ddots & \\\\\n",
    "& & \\lambda_n^k\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Let's plug these expressions into ([MPS](#MPS)):\n",
    "\n",
    "\\begin{equation}\n",
    "e^{At} &= I + At + \\frac{A^2t^2}{2!} + \\frac{A^3t^3}{3!} + \\cdots \\\\\n",
    "&= VV^{-1} + V\\Lambda V^{-1}t + V\\Lambda^2 V^{-1}\\frac{t^2}{2!} + V\\Lambda^3 V^{-1}\\frac{t^3}{3!} + \\cdots \\\\\n",
    "\n",
    "&= V\\left(I + \\Lambda t + \\frac{\\Lambda^2 t^2}{2!} + \\frac{\\Lambda^3 t^3}{3!} + \\cdots\\right)V^{-1} \\quad \\text{(factor out } V(\\cdot)V^{-1}\\text{)} \\\\\n",
    "\n",
    "&= V\\left(\\text{diag}\\left(1+\\lambda_1t+\\frac{\\lambda_1^2t^2}{2!}+\\frac{\\lambda_1^3t^3}{3!}, \\ldots, 1+\\lambda_nt+\\frac{\\lambda_n^2t^2}{2!}+\\frac{\\lambda_n^3t^3}{3!}\\right)\\right)V^{-1} \\\\\n",
    "\n",
    "&= V \\begin{bmatrix}\n",
    "e^{\\lambda_1 t} & & \\\\\n",
    "& \\ddots & \\\\\n",
    "& & e^{\\lambda_n t}\n",
    "\\end{bmatrix} V^{-1} \\quad \\text{(we recognize } 1+\\lambda_i t+\\frac{\\lambda_i^2t^2}{2!}+\\cdots \\text{ as (PS))} \n",
    "\\end{equation}\n",
    "\n",
    "That's very nice! We diagonalize $A$, then exponentiate its eigenvalues to compute $e^{At}$.\n",
    "Let's plug this back in to $\\vv x(t) = e^{At} \\vv x(0)$:\n",
    "$$\n",
    "\\vv x(t) = V \\begin{bmatrix}\n",
    "e^{\\lambda_1 t} & & \\\\\n",
    "& \\ddots & \\\\\n",
    "& & e^{\\lambda_n t}\n",
    "\\end{bmatrix} V^{-1} \\vv x(0).\n",
    "$$\n",
    "\n",
    "Now, if we let $ \\vv c = V^{-1}\\vv x(0)$, we can write\n",
    "$$\n",
    "\\vv x(t) = \\bm \\vv v_1 \\cdots \\vv v_n\\em \\begin{bmatrix}\n",
    "e^{\\lambda_1 t} & & \\\\\n",
    "& \\ddots & \\\\\n",
    "& & e^{\\lambda_n t}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "c_1 \\\\ \\vdots \\\\ c_n\n",
    "\\end{bmatrix} = c_1 e^{\\lambda_1 t}\\vv v_1 + \\cdots + c_n e^{\\lambda_n t} \\vv v_n,\n",
    "$$\n",
    "recovering our previous solution, with the exact formula $ \\vv c = V^{-1} \\vv x(0)$ we saw previously for the coefficients $c_1, \\ldots, c_n$!.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: Imaginary Eigenvalues\n",
    "\n",
    "We focus on the $2 \\times 2$ case with ${A = \\begin{bmatrix} 0 & \\omega \\\\ -\\omega & 0 \\end{bmatrix} = \\omega \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}}$. In this case, we will compute the power series directly.\n",
    "\\begin{align*}\n",
    "A &= \\omega \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}, & A^2 &= \\omega^2 \\begin{bmatrix} -1 & 0 \\\\ 0 & -1 \\end{bmatrix}, & A^3 &= \\omega^3 \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}, & A^4 &= \\omega^4 \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\\\\n",
    "\n",
    "&= \\omega J, & &= \\omega^2 J^2, & &= \\omega^3 J^3, & &= \\omega^4 J^4 \\\\\n",
    "\n",
    "A^5 &= \\omega^5 J^5 = \\omega^5 J, & A^6 &= \\omega^6 J^6 = J^2, & A^7 &= \\omega^7 J^7 = \\omega^7 J^3, & A^8 &= \\omega^8 J^8 = \\omega^8 J^4,\n",
    "\\end{align*}\n",
    "etc. So putting this together in computing $e^{At}$  we get:\n",
    "\n",
    "$$\n",
    "e^{At} = \\begin{bmatrix} 1 - \\frac{1}{2!}t^2 \\omega^2 + \\cdots & t \\omega - \\frac{1}{3!}t^3 \\omega^3 + \\cdots \\\\\n",
    "-t \\omega + \\frac{1}{3!}t^3 \\omega^3 + \\cdots & 1 - \\frac{1}{2!}t^2 \\omega^2 + \\cdots \\end{bmatrix} = \\begin{bmatrix} \\cos \\omega t & \\sin \\omega t \\\\ -\\sin \\omega t & \\cos \\omega t \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "where we used the [power series](https://en.wikipedia.org/wiki/Power_series) for $\\sin \\omega t$ and $\\cos \\omega t$ in the last equality.\n",
    "As expected, the matrix $A = \\omega \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}$ has a matrix exponential which defines a rotation, at rate $\\omega$, so that\n",
    "$$\n",
    "\\vv x(t) = \\begin{bmatrix} \\cos \\omega t & \\sin \\omega t \\\\ -\\sin \\omega t & \\cos \\omega t \\end{bmatrix} \\vv x(0).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3: Complex Eigenvalues\n",
    "\n",
    "Let's generalize our previous example to $A = \\begin{bmatrix} 6 & \\omega \\\\ -\\omega & 6 \\end{bmatrix}$.The matrix $A$ has complex conjugate eigenvalues $\\lambda_1 = 6 + i\\omega$ and $\\lambda_2 = 6 - i\\omega$. We will again compute the power series directly. To do so, we will use the following very useful fact:\n",
    "\n",
    "::::{prf:theorem}\n",
    ":label: e_AB_commute_thm\n",
    "If $A$ and $B$ commute ($AB = BA$), then $e^{A}e^B = e^{A+B}$.\n",
    "\n",
    ":::{prf:proof} Proof of [](#e_AB_commute_thm)\n",
    ":label: proof-e_AB_commute_thm\n",
    ":class: dropdown\n",
    "\n",
    "Assume $A, B$ commute, i.e., $AB = BA$. Expanding $e^{A}e^B$ using the power series definition, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "e^Ae^B &= \\left(\\sum_{a = 0}^{\\infty}{\\frac{A^a}{a!}}\\right)\\left(\\sum_{b =0}^{\\infty}{\\frac{B^b}{b!}}\\right)\\\\\n",
    "&= \\sum_{a = 0}^{\\infty}{\\frac{A^a}{a!}\\left(\\sum_{b =0}^{\\infty}{\\frac{B^b}{b!}}\\right)}\\\\\n",
    "&= \\sum_{p = 0}^{\\infty}{\\sum_{a=0}^{p}{ \\frac{A^a}{a!}\\cdot \\frac{B^{p - a}}{(p-a)!} }}\n",
    "\\end{align*}\n",
    "\n",
    "The last equality in particular follows by grouping terms with the same values of $a + b$. We also expand $e^{A + B}$ using the power series definition,\n",
    "\n",
    "\\begin{align*}\n",
    "e^{A + B} &= \\sum_{p = 0}^{\\infty}{\\frac{(A + B)^p}{p!}}\\\\\n",
    "    &= \\sum_{p = 0}^{\\infty}{\\sum_{a = 0}^{p}{\\frac{\\binom{p}{a}A^aB^{k - a}}{p!}}}\\\\\n",
    "    &= \\sum_{p = 0}^{\\infty}{\\sum_{a = 0}^{p}{\\frac{\\left(\\frac{p!}{a!(p - a)!}\\right)A^aB^{p - a}}{p!}}}\\\\\n",
    "    &= \\sum_{p = 0}^{\\infty}{\\sum_{a=0}^{p}{ \\frac{A^a}{a!}\\cdot \\frac{B^{p - a}}{(p-a)!} }}\n",
    "\\end{align*}\n",
    "\n",
    "Here, the second equality follows from the binomial expansion and commutativity of $A, B$. We see that the expressions for $e^Ae^B$ and $e^{A + B}$ are equal. Hence $AB = BA$ implies $e^{A}e^B = e^{A+B}$.\n",
    "\n",
    "<!-- Assume $ A $ and $ B $ commute, i.e., $ AB = BA $. We will first show that $ e^{A+B} = e^A e^B $.\n",
    "\n",
    "First, recall the series expansion for the matrix exponential: $e^X = \\sum_{n=0}^{\\infty} \\frac{X^n}{n!}$. Then,\n",
    "$$\n",
    "e^A = \\sum_{n=0}^{\\infty} \\frac{A^n}{n!} \\quad \\text{and} \\quad e^B = \\sum_{n=0}^{\\infty} \\frac{B^n}{n!}\n",
    "$$\n",
    "Consider the product $ e^A e^B $:\n",
    "$$\n",
    "e^A e^B = \\left( \\sum_{m=0}^{\\infty} \\frac{A^m}{m!} \\right) \\left( \\sum_{n=0}^{\\infty} \\frac{B^n}{n!} \\right)\n",
    "$$\n",
    "Since $ A $ and $ B $ commute, the terms in the product can be reordered:\n",
    "$$\n",
    "e^A e^B = \\sum_{k=0}^{\\infty} \\frac{(A+B)^k}{k!} = e^{A+B}\n",
    "$$\n",
    "\n",
    "Hence, $ e^{A+B} = e^A e^B $ if $ AB = BA $.\n",
    "\n",
    "Now assume $ e^{A+B} = e^A e^B $. We need to show that $ A $ and $ B $ commute.\n",
    "\n",
    "Consider the matrix $ C(t) = e^{At} e^{Bt} $. Differentiating $ C(t) $ with respect to $ t $ gives:\n",
    "$$\n",
    "\\frac{d}{dt} C(t) = A e^{At} e^{Bt} + e^{At} B e^{Bt} = A C(t) + C(t) B\n",
    "$$\n",
    "\n",
    "Using the product rule:\n",
    "\\[\n",
    "\\frac{d}{dt} C(t) = A C(t) + C(t) B\n",
    "\\]\n",
    "\n",
    "Since \\( e^{A+B} = e^A e^B \\), we have:\n",
    "\\[\n",
    "C(t) = e^{(A+B)t}\n",
    "\\]\n",
    "\n",
    "Differentiating \\( e^{(A+B)t} \\) with respect to \\( t \\) gives:\n",
    "\\[\n",
    "\\frac{d}{dt} e^{(A+B)t} = (A+B) e^{(A+B)t}\n",
    "\\]\n",
    "\n",
    "Comparing the two derivatives:\n",
    "\\[\n",
    "A e^{At} e^{Bt} + e^{At} B e^{Bt} = (A+B) e^{(A+B)t}\n",
    "\\] -->\n",
    "<!-- \n",
    "This implies:\n",
    "\\[\n",
    "A e^{(A+B)t} + B e^{(A+B)t} = (A+B) e^{(A+B)t}\n",
    "\\]\n",
    "\n",
    "For this equality to hold for all \\( t \\), \\( A \\) and \\( B \\) must commute:\n",
    "\\[\n",
    "AB = BA\n",
    "\\]\n",
    "\n",
    "Therefore, \\( e^{A+B} = e^A e^B \\) if and only if \\( A \\) and \\( B \\) commute. -->\n",
    ":::\n",
    "::::\n",
    "\n",
    "We will strategically use this fact. First, defining $J = \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}$ we note that we can write $A = 6I + \\omega J$.Importantly, $6I$ and  $\\omega J$ commute as $(6I)(\\omega J) = (\\omega J)(6I) = \\omega 6J$. Therefore, \n",
    "$$\n",
    "e^{At} = e^{(6I + \\omega J)t} = e^{6It} e^{\\omega Jt} = \\begin{bmatrix} e^{6t} & 0 \\\\ 0 & e^{6t} \\end{bmatrix} \\begin{bmatrix} \\cos \\omega t & \\sin \\omega t \\\\ -\\sin \\omega t & \\cos \\omega t \\end{bmatrix} = e^{6t} \\begin{bmatrix} \\cos \\omega t & \\sin \\omega t \\\\ -\\sin \\omega t & \\cos \\omega t \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 4: Jordan Block\n",
    "\n",
    "Assume $A = V \\begin{bmatrix} \\lambda & 1 \\\\ 0 & \\lambda \\end{bmatrix} V^{-1}$, for $V = \\bm \\vv v_1 & \\vv v_2\\em$ an eigenvector and generalized eigenvector of A.\n",
    "\n",
    "Then following the same argument as in Case 1, we have that $e^{At} = V e^{\\begin{bmatrix} \\lambda & 1 \\\\ 0 & \\lambda \\end{bmatrix}t} V^{-1}$. To compute $e^{\\begin{bmatrix} \\lambda & 1 \\\\ 0 & \\lambda \\end{bmatrix}t}$, we note $\\begin{bmatrix} \\lambda & 1 \\\\ 0 & \\lambda \\end{bmatrix}t = \\lambda It + t\\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix}$, and that these two terms commute. Hence: $e^{\\begin{bmatrix} \\lambda & 1 \\\\ 0 & \\lambda \\end{bmatrix}t} = e^{\\begin{bmatrix} \\lambda & 0 \\\\ 0 & \\lambda \\end{bmatrix}t} e^{t\\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix}}$. We note that\n",
    "$$\n",
    "e^{\\begin{bmatrix} \\lambda & 0 \\\\ 0 & \\lambda \\end{bmatrix}t} = \\begin{bmatrix} e^{\\lambda t} & 0 \\\\ 0 & e^{\\lambda t} \\end{bmatrix} \\text{ and } e^{\\begin{bmatrix} 0 & t \\\\ 0 & 0 \\end{bmatrix}} &= \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} + \\begin{bmatrix} 0 & t \\\\ 0 & 0 \\end{bmatrix} \\ \\text{ (higher powers }=0) \\\\\n",
    "&= \\begin{bmatrix} 1 & t \\\\ 0 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Allowing us to conclude that $e^{\\begin{bmatrix} \\lambda & 1 \\\\ 0 & \\lambda \\end{bmatrix}t} = \\begin{bmatrix} e^{\\lambda t} & te^{\\lambda t} \\\\ 0 & e^{\\lambda t} \\end{bmatrix}$, and that\n",
    "\n",
    "$$\n",
    "\\vv x(t) = e^{At} \\vv x(0) &= \\bm \\vv v_1 & \\vv v_2\\em \\begin{bmatrix} e^{\\lambda t} & te^{\\lambda t} \\\\ 0 & e^{\\lambda t} \\end{bmatrix} V^{-1} \\vv x(0), \\quad \\text{and letting } \\vv c = V^{-1}\\vv x(0) \\\\\n",
    "&= \\bm \\vv v_1 & \\vv v_2\\em \\begin{bmatrix} c_1 e^{\\lambda t} + c_2 te^{\\lambda t} \\\\ c_2 e^{\\lambda t} \\end{bmatrix} = \\left(c_1 e^{\\lambda t} + c_2 te^{\\lambda t}\\right)\\vv v_1 + c_2 e^{\\lambda t} \\vv v_2,\n",
    "$$\n",
    "which we recognize from our previous section on Jordan Blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/06_Ch_7_Dynamics/083-Matrix_exponential.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
