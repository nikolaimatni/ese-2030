{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 7.3 Matrix Exponential\n",
    "subject: Dynamics\n",
    "subtitle: \n",
    "short_title: 7.3 Matrix Exponential\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: linear systems, \n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/06_Ch_7_Dynamics/083-Matrix_exponential.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 13 - Complex and Repeated Eigenvalues Revisited, Jordan Blocks, Matrix Exponential.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be found in ALA .\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- the matrix exponential expressed as a power series\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Exponential\n",
    "\n",
    "We've seen four cases for eigenvalues/eigenvectors and their relationship to solutions of initial value problems defined by $\\dot{\\mathbf{x}} = A\\mathbf{x}$ and $\\mathbf{x}(0)$ given:\n",
    "\n",
    "1) real distinct eigenvalues, solved by diagonalization;\n",
    "2) real repeated eigenvalues with algebraic multiplicity = geometric multiplicity, also solved by diagonalization;\n",
    "3) complex distinct eigenvalues, solved by diagonalization and applying Euler's formula to define real-valued eigenfunctions;\n",
    "4) repeated eigenvalues with algebraic multiplicity > geometric multiplicity, solved by Jordan decomposition using generalized eigenvectors.\n",
    "\n",
    "While correct, the fact that there are four different cases we need to consider is somewhat unsatisfying. In this section, we show that by appropriately defining a _matrix exponential_, we can provide a unified treatment of all the aforementioned settings.\n",
    "\n",
    "We start by recalling the power series definition for the scalar exponential $e^x$, for $x \\in \\mathbb{R}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{ps}\n",
    "e^x = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots = \\sum_{k=0}^{\\infty} \\frac{x^k}{k!}, \\quad (\\text{PS})\n",
    "\\end{equation}\n",
    "\n",
    "where we recall that $k! = 1 \\cdot 2 \\cdots (k-1) \\cdot k$. We know that for the scalar initial value problem $\\dot{x} = ax$, the solution is $x(t) = e^{at}x(0)$, where $e^{at}$ can be computed via ([PS](#ps)) by setting $x = at$.\n",
    "\n",
    "Wouldn't it be cool if we could do something similar for the vector valued initial value problem defined by $\\dot{\\mathbf{\\vv x}} = A\\mathbf{\\vv x}$? Does there exist a function, call it $e^{At}$, so that $\\mathbf{x}(t) = e^{At}\\mathbf{x}(0)$? How would we even begin to define such a thing?\n",
    "\n",
    "Let's do the \"obvious\" thing and start with the definition ([PS](#ps)), and replace the scalar $x$ with a matrix $X$ to obtain the _matrix exponential of X_:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{MPS}\n",
    "e^X = I + X + \\frac{X^2}{2!} + \\frac{X^3}{3!} + \\cdots = \\sum_{k=0}^{\\infty} \\frac{X^k}{k!}, \\quad (\\text{MPS})\n",
    "\\end{equation}\n",
    "\n",
    "Although we can't prove it, it can be shown that ([MPS](#MPS)) converges for any $X$, so this is a well defined object. Does ([MPS](#MPS)) help with solving $\\dot{\\mathbf{\\vv x}} = A\\mathbf{\\vv x}$? Let's try the test solution $\\mathbf{\\vv x}(t) = e^{At}\\mathbf{\\vv x}(0)$ â€” this is exactly what we did for the scalar setting, but we replace $e^{at}$ with $e^{At}$. Is this a solution to $\\dot{\\mathbf{\\vv x}} = A\\mathbf{\\vv x}$?\n",
    "\n",
    "First, we compute $A\\mathbf{\\vv x}(t) = Ae^{At}\\mathbf{\\vv x}(0)$. Next, we need to compute $\\frac{d}{dt}e^{At}\\mathbf{\\vv x}(0)$. But how do we do this? We will rely on ([MPS](#MPS)):\n",
    "\n",
    "\\frac{d}{dt} e^{At} x(0) = \\frac{d}{dt} (I + At + \\frac{(At)^2}{2!} + \\frac{(At)^3}{3!} + \\cdots)\n",
    "\n",
    "= \\frac{d}{dt}I + \\frac{d}{dt}At + \\frac{d}{dt}\\frac{A^2t^2}{2!} + \\frac{d}{dt}\\frac{A^3t^3}{3!} + \\cdots\n",
    "\n",
    "= 0 + A + A^2t + A^3t^2 + \\cdots\n",
    "\n",
    "= A + A^2t + \\frac{A^3t^2}{2!} + \\frac{A^4t^3}{3!} + \\cdots\n",
    "\n",
    "= A(I + At + \\frac{A^2t^2}{2!} + \\frac{A^3t^3}{3!} + \\cdots)\n",
    "\n",
    "= A e^{At} x(0).\n",
    "\n",
    "\\text{This works, and we have found a general solution to } \\dot{x} = Ax \\text{ defined}\n",
    "\\text{in terms of the matrix exponential!}\n",
    "\n",
    "\\textbf{Theorem:} \\text{Consider the initial value problem } \\dot{x} = Ax, \\text{ with } x(0) \\text{ specified. Its}\n",
    "\\text{solution is given by } x(t) = e^{At} x(0), \\text{ where } e^{At} \\text{ is defined according}\n",
    "\\text{to the matrix power series (MPS).}\n",
    "\n",
    "\\text{This is very satisfying, as now our scalar and vector-valued problems have similar}\n",
    "\\text{looking solutions defined in terms of appropriate exponential functions. The only thing}\n",
    "\\text{that remains is to compute } e^{At}. \\text{ How do we do this? This is where all}\n",
    "\\text{of the work we've done on diagonalization and Jordan forms really pays off!}\n",
    "\n",
    "\\textbf{Case 1:} \\text{ Real eigenvalues, diagonalizable A}\n",
    "\n",
    "\\text{Suppose that } A \\in \\mathbb{R}^{n\\times n} \\text{ and has eigenvalues } \\lambda_1, \\lambda_2, \\ldots, \\lambda_n \\text{ with corresponding linearly}\n",
    "\\text{independent eigenvectors } v_1, v_2, \\ldots, v_n. \\text{ Then we can write}\n",
    "\n",
    "A = V \\Lambda V^{-1}, \\text{ for } V = [v_1, v_2, \\ldots, v_n] \\text{ and } \\Lambda = \\text{diag}(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n).\n",
    "\n",
    "\\text{To compute } e^{At} \\text{ we need to compute powers } (At)^k. \\text{ Let's work a few of}\n",
    "\\text{these out using } A = V\\Lambda V^{-1}:\n",
    "\n",
    "(At)^0 = I, \\quad At = V\\Lambda V^{-1}t, \\quad A^2t^2 = (V\\Lambda V^{-1})(V\\Lambda V^{-1})t^2 = V\\Lambda^2 V^{-1}t^2,\n",
    "\\quad A^3t^3 = (V\\Lambda V^{-1})(V\\Lambda^2 V^{-1})t^3 = V\\Lambda^3 V^{-1}t^3\n",
    "\n",
    "\\text{There is a pattern: } (At)^k = V \\Lambda^k V^{-1} t^k. \\text{ This is nice, since computing}\n",
    "\\text{powers of diagonal matrices is easy:}\n",
    "\n",
    "\\Lambda^k = \\begin{bmatrix}\n",
    "\\lambda_1 & & \\\\\n",
    "& \\ddots & \\\\\n",
    "& & \\lambda_n\n",
    "\\end{bmatrix}^k = \\begin{bmatrix}\n",
    "\\lambda_1^k & & \\\\\n",
    "& \\ddots & \\\\\n",
    "& & \\lambda_n^k\n",
    "\\end{bmatrix}.\n",
    "\n",
    "\\text{Let's plug these expressions into (MPS):}\n",
    "\n",
    "e^{At} = I + At + \\frac{A^2t^2}{2!} + \\frac{A^3t^3}{3!} + \\cdots\n",
    "\n",
    "= VV^{-1} + V\\Lambda V^{-1}t + V\\Lambda^2 V^{-1}\\frac{t^2}{2!} + V\\Lambda^3 V^{-1}\\frac{t^3}{3!} + \\cdots\n",
    "\n",
    "= V(I + \\Lambda t + \\frac{\\Lambda^2 t^2}{2!} + \\frac{\\Lambda^3 t^3}{3!} + \\cdots)V^{-1} \\quad \\text{(factor out } V(\\cdot)V^{-1}\\text{)}\n",
    "\n",
    "= V(\\text{diag}(1+\\lambda_1t+\\frac{\\lambda_1^2t^2}{2!}+\\frac{\\lambda_1^3t^3}{3!}, \\ldots, 1+\\lambda_nt+\\frac{\\lambda_n^2t^2}{2!}+\\frac{\\lambda_n^3t^3}{3!}))V^{-1}\n",
    "\n",
    "= V \\begin{bmatrix}\n",
    "e^{\\lambda_1 t} & & \\\\\n",
    "& \\ddots & \\\\\n",
    "& & e^{\\lambda_n t}\n",
    "\\end{bmatrix} V^{-1} \\quad \\text{(we recognize } 1+\\lambda t+\\frac{\\lambda^2t^2}{2!}+\\cdots \\text{ as (PS))}\n",
    "\n",
    "\\text{That's very nice! We diagonalize A, then exponentiate its eigenvalues to compute } e^{At}.\n",
    "\\text{Let's plug this back in to } x(t) = e^{At} x(0):\n",
    "\n",
    "x(t) = V \\begin{bmatrix}\n",
    "e^{\\lambda_1 t} & & \\\\\n",
    "& \\ddots & \\\\\n",
    "& & e^{\\lambda_n t}\n",
    "\\end{bmatrix} V^{-1} x(0).\n",
    "\n",
    "\\text{Now, if we let } c = V^{-1}x(0), \\text{ we can write}\n",
    "\n",
    "x(t) = [v_1 \\cdots v_n] \\begin{bmatrix}\n",
    "e^{\\lambda_1 t} & & \\\\\n",
    "& \\ddots & \\\\\n",
    "& & e^{\\lambda_n t}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "c_1 \\\\ \\vdots \\\\ c_n\n",
    "\\end{bmatrix} = c_1 e^{\\lambda_1 t} v_1 + \\cdots + c_n e^{\\lambda_n t} v_n,\n",
    "\n",
    "\\text{recovering our previous solution, with the exact formula } c = V^{-1} x(0) \\text{ we saw previously}\n",
    "\\text{for the coefficients } c_1, \\ldots, c_n.\n",
    "\n",
    "\\textbf{Case 2: Imaginary Eigenvalues}\n",
    "\n",
    "\\text{We focus on the 2x2 case with } A = \\begin{bmatrix} 0 & \\omega \\\\ -\\omega & 0 \\end{bmatrix} = \\omega \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}. \\text{ In this case,}\n",
    "\\text{we will compute the power series directly.}\n",
    "\n",
    "A = \\omega \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}, A^2 = \\omega^2 \\begin{bmatrix} -1 & 0 \\\\ 0 & -1 \\end{bmatrix}, A^3 = \\omega^3 \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}, A^4 = \\omega^4 \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\n",
    "\n",
    "= \\omega J, = \\omega^2 J^2, = \\omega^3 J^3, = \\omega^4 J^4\n",
    "\n",
    "A^5 = \\omega^5 J^5 = \\omega^5 J, A^6 = \\omega^6 J^6 = J^2, A^7 = \\omega^7 J^7 = \\omega^7 J^3, A^8 = \\omega^8 J^8 = \\omega^8 J^4,\n",
    "\n",
    "\\text{etc. So putting this together in computing } e^{At} \\text{ we get:}\n",
    "\n",
    "e^{At} = \\begin{bmatrix} 1 - \\frac{1}{2!}(\\omega t)^2 + \\cdots & \\omega t - \\frac{1}{3!}(\\omega t)^3 + \\cdots \\\\\n",
    "-\\omega t + \\frac{1}{3!}(\\omega t)^3 - \\cdots & 1 - \\frac{1}{2!}(\\omega t)^2 + \\cdots \\end{bmatrix} = \\begin{bmatrix} \\cos \\omega t & \\sin \\omega t \\\\ -\\sin \\omega t & \\cos \\omega t \\end{bmatrix},\n",
    "\n",
    "\\text{where we used the power series for } \\sin \\omega t \\text{ and } \\cos \\omega t \\text{ in the last equality.}\n",
    "\\text{As expected, the matrix } A = \\omega \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix} \\text{ has a matrix exponential which defines}\n",
    "\\text{a rotation, at rate } \\omega, \\text{ so that}\n",
    "\n",
    "x(t) = \\begin{bmatrix} \\cos \\omega t & \\sin \\omega t \\\\ -\\sin \\omega t & \\cos \\omega t \\end{bmatrix} x(0).\n",
    "\n",
    "\\textbf{Case 3: Complex Eigenvalues}\n",
    "\n",
    "\\text{Let's generalize our previous example to } A = \\begin{bmatrix} 6 & \\omega \\\\ -\\omega & 6 \\end{bmatrix}. \\text{ The matrix } A \\text{ has}\n",
    "\\text{complex conjugate eigenvalues } \\lambda_1 = 6 + i\\omega \\text{ and } \\lambda_2 = 6 - i\\omega. \\text{ We will again compute}\n",
    "\\text{the power series directly. To do so, we will use the following very useful fact:}\n",
    "\n",
    "\\text{Fact: } e^{A+B} = e^A e^B \\text{ if and only if } AB = BA, \\text{ that is, if and only if A and B}\n",
    "\\text{commute}\n",
    "\n",
    "\\textbf{ONLINE NOTES: PLEASE PROVIDE PROOF}\n",
    "\n",
    "\\text{We will strategically use this fact. First, defining } J = \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}, \\text{ we note we}\n",
    "\\text{can write } A = 6I + \\omega J. \\text{ Importantly, } 6I \\text{ and } \\omega J \\text{ commute as } (6I)(\\omega J) = (\\omega J)(6I) = \\omega 6J\n",
    "\n",
    "\\text{Therefore, } e^{At} = e^{(6I + \\omega J)t} = e^{6It} e^{\\omega Jt} = \\begin{bmatrix} e^{6t} & 0 \\\\ 0 & e^{6t} \\end{bmatrix} \\begin{bmatrix} \\cos \\omega t & \\sin \\omega t \\\\ -\\sin \\omega t & \\cos \\omega t \\end{bmatrix} = e^{6t} \\begin{bmatrix} \\cos \\omega t & \\sin \\omega t \\\\ -\\sin \\omega t & \\cos \\omega t \\end{bmatrix}\n",
    "\n",
    "\\textbf{Case 4: Jordan Block}\n",
    "\n",
    "\\text{Assume } A = V \\begin{bmatrix} \\lambda & 1 \\\\ 0 & \\lambda \\end{bmatrix} V^{-1}, \\text{ for } V = [v_1 \\, v_2] \\text{ an eigenvector and generalized eigenvector}\n",
    "\\text{of A.}\n",
    "\n",
    "\\text{Then following the same argument as case 2, we have that } e^{At} = V e^{\\begin{bmatrix} \\lambda & 1 \\\\ 0 & \\lambda \\end{bmatrix}t} V^{-1}.\n",
    "\n",
    "\\text{To compute } e^{\\begin{bmatrix} \\lambda & 1 \\\\ 0 & \\lambda \\end{bmatrix}t}, \\text{ we note } \\begin{bmatrix} \\lambda & 1 \\\\ 0 & \\lambda \\end{bmatrix}t = \\lambda It + t\\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix}, \\text{ and that these two}\n",
    "\\text{terms commute. Hence: } e^{\\begin{bmatrix} \\lambda & 1 \\\\ 0 & \\lambda \\end{bmatrix}t} = e^{\\lambda It} e^{t\\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix}} \\text{ We note that}\n",
    "\n",
    "e^{\\lambda It} = \\begin{bmatrix} e^{\\lambda t} & 0 \\\\ 0 & e^{\\lambda t} \\end{bmatrix} \\text{ and } e^{t\\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix}} = I + t\\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix} \\text{ (higher powers }=0)\n",
    "= \\begin{bmatrix} 1 & t \\\\ 0 & 1 \\end{bmatrix}.\n",
    "\n",
    "\\text{Allowing us to conclude that } e^{\\begin{bmatrix} \\lambda & 1 \\\\ 0 & \\lambda \\end{bmatrix}t} = \\begin{bmatrix} e^{\\lambda t} & te^{\\lambda t} \\\\ 0 & e^{\\lambda t} \\end{bmatrix}, \\text{ and that}\n",
    "\n",
    "x(t) = e^{At} x(0) = [v_1 \\, v_2] \\begin{bmatrix} e^{\\lambda t} & te^{\\lambda t} \\\\ 0 & e^{\\lambda t} \\end{bmatrix} V^{-1} x(0), \\quad \\text{and letting } c = V^{-1}x(0)\n",
    "\n",
    "= [v_1 \\, v_2] \\begin{bmatrix} c_1 e^{\\lambda t} + c_2 te^{\\lambda t} \\\\ c_2 e^{\\lambda t} \\end{bmatrix} = (c_1 e^{\\lambda t} + c_2 te^{\\lambda t})v_1 + c_2 e^{\\lambda t} v_2,\n",
    "\n",
    "\\text{which we recognize from our previous section on Jordan Blocks.}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/06_Ch_7_Dynamics/083-Matrix_exponential.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
