{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 7.3 Matrix Exponential\n",
    "subject: Dynamics\n",
    "subtitle: \n",
    "short_title: 7.3 Matrix Exponential\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: linear systems, \n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/06_Ch_7_Dynamics/083-Matrix_exponential.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 13 - Complex and Repeated Eigenvalues Revisited, Jordan Blocks, Matrix Exponential.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be found in ALA .\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- the matrix exponential expressed as a power series\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Exponential\n",
    "\n",
    "We've seen four cases for eigenvalues/eigenvectors and their relationship to solutions of initial value problems defined by $\\dot{\\mathbf{x}} = A\\mathbf{x}$ and $\\mathbf{x}(0)$ given:\n",
    "\n",
    "1) real distinct eigenvalues, solved by diagonalization;\n",
    "2) real repeated eigenvalues with algebraic multiplicity = geometric multiplicity, also solved by diagonalization;\n",
    "3) complex distinct eigenvalues, solved by diagonalization and applying Euler's formula to define real-valued eigenfunctions;\n",
    "4) repeated eigenvalues with algebraic multiplicity > geometric multiplicity, solved by Jordan decomposition using generalized eigenvectors.\n",
    "\n",
    "While correct, the fact that there are four different cases we need to consider is somewhat unsatisfying. In this section, we show that by appropriately defining a _matrix exponential_, we can provide a unified treatment of all the aforementioned settings.\n",
    "\n",
    "We start by recalling the power series definition for the scalar exponential $e^x$, for $x \\in \\mathbb{R}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{ps}\n",
    "e^x = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots = \\sum_{k=0}^{\\infty} \\frac{x^k}{k!}, \\quad (\\text{PS})\n",
    "\\end{equation}\n",
    "\n",
    "where we recall that $k! = 1 \\cdot 2 \\cdots (k-1) \\cdot k$. We know that for the scalar initial value problem $\\dot{x} = ax$, the solution is $x(t) = e^{at}x(0)$, where $e^{at}$ can be computed via ([PS](#ps)) by setting $x = at$.\n",
    "\n",
    "Wouldn't it be cool if we could do something similar for the vector valued initial value problem defined by $\\dot{\\mathbf{\\vv x}} = A\\mathbf{\\vv x}$? Does there exist a function, call it $e^{At}$, so that $\\mathbf{x}(t) = e^{At}\\mathbf{x}(0)$? How would we even begin to define such a thing?\n",
    "\n",
    "Let's do the \"obvious\" thing and start with the definition ([PS](#ps)), and replace the scalar $x$ with a matrix $X$ to obtain the _matrix exponential of X_:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{MPS}\n",
    "e^X = I + X + \\frac{X^2}{2!} + \\frac{X^3}{3!} + \\cdots = \\sum_{k=0}^{\\infty} \\frac{X^k}{k!}, \\quad (\\text{MPS})\n",
    "\\end{equation}\n",
    "\n",
    "Although we can't prove it, it can be shown that ([MPS](#MPS)) converges for any $X$, so this is a well defined object. Does ([MPS](#MPS)) help with solving $\\dot{\\mathbf{\\vv x}} = A\\mathbf{\\vv x}$? Let's try the test solution $\\mathbf{\\vv x}(t) = e^{At}\\mathbf{\\vv x}(0)$ â€” this is exactly what we did for the scalar setting, but we replace $e^{at}$ with $e^{At}$. Is this a solution to $\\dot{\\mathbf{\\vv x}} = A\\mathbf{\\vv x}$?\n",
    "\n",
    "First, we compute $A\\mathbf{\\vv x}(t) = Ae^{At}\\mathbf{\\vv x}(0)$. Next, we need to compute $\\frac{d}{dt}e^{At}\\mathbf{\\vv x}(0)$. But how do we do this? We will rely on ([MPS](#MPS)):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/06_Ch_7_Dynamics/083-Matrix_exponential.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
