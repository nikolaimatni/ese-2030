{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 7.4 Invariant Subspaces\n",
    "subject: Dynamics\n",
    "subtitle: staying inside a region\n",
    "short_title: 7.4 Invariant Subspaces\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: \n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/06_Ch_7_Dynamics/084-Invariant_Sub.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 14 - Invariant Subspaces, Inhomogeneous Systems, and Applications to Mechanical Systems.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be found in ALA 8.4 and 10.4.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- the definition and some examples of invariant subspaces\n",
    "-  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invariant Subspaces: Definition and Examples\n",
    "\n",
    "Invariant subspaces of linear maps play a key role in dynamical systems, linear iterative systems (like Markov chains, which we'll see next lecture), and control systems. Perhaps not surprisingly, the theory of invariant subspaces is built on eigenvalues and eigenvectors.\n",
    "\n",
    "We start by defining an _invariant subspace with respect to a linear transformation_. \n",
    "\n",
    ":::{prf:definition} Invariant Subspace\n",
    ":label: inv_sub_defn\n",
    "Let $L:V\\to V$ be a **linear transformation** on a vector space $V$. A subspace $W\\subset V$ is said to be _invariant_ if $L(\\vv w)\\in W$ for any $\\vv w\\in W$.\n",
    ":::\n",
    "\n",
    "Intuitively, an invariant subspace $W\\subset V$ is like Vegas: what happens in $W$ stays in $W$! Let's see some simple examples before developing a more general theory.\n",
    "\n",
    ":::{prf:example}\n",
    ":label: simple_eg\n",
    "First some not so interesting examples:\n",
    "1. If $W = V$ is the entire space, or $W = \\{\\vv 0\\}$, then it is invariant under any linear map $L$.    \n",
    "2. For $L=I$ the identity map, any subspace $W\\subset V$ is invariant.\n",
    "3. Both the [kernel](../01_Ch_2_Vector_Spaces_and_Bases/035-kernel_image.ipynb#null_defn) and [image](../01_Ch_2_Vector_Spaces_and_Bases/035-kernel_image.ipynb#col_space_defn) of $L$ are invariant subspaces:\n",
    "   - If $\\vv w\\in \\text{ker } L$ then $L(\\vv w)=\\vv 0\\in \\ker L$ \\newline\n",
    "   - If $\\vv w\\in \\text{img } L$ then $L(\\vv w)\\in \\text{img }L$ (by definition of img $L$)\n",
    "\n",
    ":::\n",
    "\n",
    ":::{prf:example}\n",
    ":label: eg_2\n",
    "Let $V=\\mathbb{R}^2$ and $L(x,y)=\\begin{bmatrix} 2x \\\\ 3y \\end{bmatrix}$. What are the invariant subspaces of $V$ under $L$? Besides $W=\\mathbb{R}^2$ and $W=\\{0\\}$, the only other type of subspace in $\\mathbb{R}^2$ is a line. If W is the line spanned by the vector $\\vv w=\\bm a \\\\ b \\em \\neq \\vv 0$, then $L(\\vv w)=\\bm 2a \\\\ 3b\\em\\in W$ if and only if there exists $c\\in\\mathbb{R}$ s.t. $\\bm 2a \\\\ 3b\\em=c\\bm a \\\\ b\\em$, which is only possible if either $a=0$ or $b=0$. Thus the only invariant subspaces of L are either the x-axis or the y-axis.\n",
    ":::\n",
    "\n",
    ":::{prf:example}\n",
    ":label: eg_3\n",
    "Let $V=\\mathbb{R}^2$ and $L(x,y)=(x+3y,y)$. Let's see what lines $W$, spanned by $\\vv w=\\bm a \\\\ b \\em\\neq \\vv 0$, are invariant under L by solving for a $c\\in\\mathbb{R}$ s.t.\n",
    "$$\n",
    "L(\\vv w)=\\begin{bmatrix} a+3b \\\\ b \\end{bmatrix} = c\\begin{bmatrix} a \\\\ b \\end{bmatrix}\n",
    "$$\n",
    "which is only possible if $b=0$, i.e., the x-axis is the only invariant 1d subspace.\n",
    ":::\n",
    "\n",
    ":::{prf:example}\n",
    ":label: eg_4\n",
    "Let $V=\\mathbb{R}^2$ and $L(x,y)=(-y,x)$, a counterclockwise rotation by $90^\\circ$. You should be able to convince yourself geometrically that no 1d subspace can be invariant under such a transformation.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing Transforms using Matrix\n",
    "\n",
    "Since we will focus on cases where $V=\\mathbb{R}^n$, our linear transformations will be defined by matrices $A:\\mathbb{R}^n \\to \\mathbb{R}^n$: $L(\\vv x)=A\\vv x$. In this case, we can characterize 1-d invariant subspaces very clearly.\n",
    "\n",
    "::::{prf:proposition}\n",
    ":label: eig_vec_prop\n",
    "A 1d subspace is invariant under $L(\\vv x)=A\\vv x$ if and only if $W=\\text{span}\\{\\vv v\\}$ where $\\vv v$ is an eigenvector of $A$.\n",
    "\n",
    ":::{prf:proof} Proof of [](#eig_vec_prop)\n",
    ":label: proof-eig_vec_prop\n",
    ":class: dropdown\n",
    "Let $W = \\text{span}\\{\\vv v\\}$ for some $\\vv v \\neq 0$. Then $A \\vv v \\in W$ if and only if $A \\vv v = \\lambda \\vv v$ for some scalar $\\lambda$. But this means that $\\vv v$ is an eigenvector of $A$ with eigenvalue $\\lambda$.\n",
    ":::\n",
    "::::\n",
    "\n",
    ":::{note}\n",
    "[This](#eig_vec_prop) tells us that if $A$ has $n$ linearly independent eigenvectors $\\vv v_1,\\ldots,\\vv v_n$ (arising from either distinct or repeated eigenvalues), then every  one dimensional subspace of $A$ is of the form $W_i = \\text{span}\\{\\vv v_i\\}$.\n",
    "\n",
    "For complex matrices $A$, we can use these 1d invariant subspaces to build all other $k$-dimensional invariant subspaces, but these may be complex!\n",
    ":::\n",
    "\n",
    ":::{prf:theorem}\n",
    ":label: \n",
    "If $A \\in \\mathbb{R}^{n \\times n}$ is a [complete matrix](../05_Ch_6_Eigenvalues_and_Eigenvectors/077-diagonalization.ipynb#eigenbasis-defn), then every $k$-dimensional complex invariant subspace is spanned by $k$ linearly independent eigenvectors of $A$.\n",
    ":::\n",
    "\n",
    "We won't formally prove this result, but instead give a hint as to why it might be true. Suppose $W = \\text{span}\\{\\vv v_1,\\ldots,\\vv v_k\\}$ for $\\vv v_1,\\ldots,\\vv v_k$ linearly independent eigenvectors of $A$. Then any $\\vv w \\in W$ can be written as\n",
    "$$\n",
    "\\vv w = c_1\\vv v_1 + \\cdots + c_k\\vv v_k\n",
    "$$\n",
    "for $c_1,\\ldots,c_k \\in \\mathbb{C}$. Then\n",
    "$$\n",
    "A \\vv w = A(c_1\\vv v_1 + \\cdots + c_k\\vv v_k) = c_1A\\vv v_1 + \\cdots + c_kA\\vv v_k = c_1\\lambda_1\\vv v_1 + \\cdots + c_k\\lambda_k\\vv v_k \\in \\text{span}\\{\\vv v_1,\\ldots,\\vv v_k\\} = W\n",
    "$$\n",
    "and hence $W$ is invariant under the map $\\vv x \\mapsto A\\vv x$. The challenge is to show the other direction, that if $W$ is invariant under $A$ then $W$ must be spanned by $k$ eigenvectors of $A$. We refer interested readers to proof of Thm 8.3 in ALA 8.4.\n",
    "\n",
    ":::{note}\n",
    "Some final comments before we see an example:\n",
    "1. If $A$ is a complex real matrix with all real eigenvalues, then the above tells us that all real invariant subspaces are spanned by eigenvectors of $A$.\n",
    "2. If $A$ is real and complete, and has complex conjugate eigenvectors $\\vv v_t = \\vv x \\pm i \\vv y$, then the real invariant subspaces are spanned by $\\text{Re}\\{\\vv v_t\\} = \\vv x$ and $\\text{Im}\\{\\vv v_t\\} = \\vv y$ using a similar argument as to the one we used to find real solutions to $ \\dot{\\vv x} = A\\vv x$ when $A$ had complex conjugate eigenvalues.\n",
    "\\item A slightly modified argument can be applied to incomplete matrices using Jordan blocks and generalized eigenvectors; we can't cover these in this course, but if you're curious, you can check out ALA 8.6.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section*{Example: Consider the rotation (permutation) matrix:}\n",
    "\n",
    "\\[A = \\begin{pmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{pmatrix}\\]\n",
    "\n",
    "It has one real eigenvalue, $\\lambda_1 = 1$, and two complex conjugate eigenvalues, $\\lambda_2 = \\frac{1}{2} + i\\frac{\\sqrt{3}}{2}$ and $\\lambda_3 = \\frac{1}{2} - i\\frac{\\sqrt{3}}{2}$. The corresponding eigenvectors are:\n",
    "\n",
    "\\[v_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad\n",
    "v_2 = \\begin{pmatrix} -\\frac{1}{2} + i\\frac{\\sqrt{3}}{2} \\\\ -\\frac{1}{2} - i\\frac{\\sqrt{3}}{2} \\\\ 1 \\end{pmatrix}, \\quad\n",
    "v_3 = \\begin{pmatrix} -\\frac{1}{2} - i\\frac{\\sqrt{3}}{2} \\\\ -\\frac{1}{2} + i\\frac{\\sqrt{3}}{2} \\\\ 1 \\end{pmatrix}\\]\n",
    "\n",
    "The complex invariant subspaces are spanned by $c_1v_1$, $c_2v_2$, or $c_3v_3$. There is a single 1d real invariant subspace spanned by $v_1 = (1,1,1)$, and a single 2d real invariant subspace spanned by $\\text{Re}\\{v_2\\} = (-\\frac{1}{2}, -\\frac{1}{2}, 1)$ and $\\text{Im}\\{v_2\\} = (\\frac{\\sqrt{3}}{2}, -\\frac{\\sqrt{3}}{2}, 0)$ which is the orthogonal complement to $v_1$. We can interpret $v_1$ as the axis of rotation, and $A$ acts as a 2d rotation on its orthogonal complement.\n",
    "\n",
    "Online notes: would be good to have a maple/lib worksheet illustrating this.\n",
    "\n",
    "\\section*{Invariant Subspaces and Linear Dynamical Systems (HLA 10.4)}\n",
    "\n",
    "Here we give a very brief preview of the role of invariant subspaces in dynamical systems. You will see this in much more detail in ESE 520.\n",
    "\n",
    "We call a subset $S \\subset \\mathbb{R}^n$ invariant for $\\dot{x} = Ax$ if, whenever $x(0) = b \\in S$ then the solution $x(t) \\in S$ for all $t \\geq 0$. It turns out, invariant subspaces of $A$ precisely characterize these subsets:\n",
    "\n",
    "\\begin{proposition}\n",
    "If $S \\subset \\mathbb{R}^n$ is an invariant subspace of $A$, then it is invariant under $\\dot{x} = Ax$.\n",
    "\\end{proposition}\n",
    "\n",
    "The proof follows from our solution $x(t) = e^{At}x(0) = e^{At}b$. Using (MPS), we have:\n",
    "\n",
    "\\[x(t) = e^{At}b = \\sum_{k=0}^{\\infty} \\frac{t^k}{k!} A^k b. \\quad (*)\\]\n",
    "\n",
    "But if $b \\in S$, then $Ab \\in S$, $A^2b \\in S$, and in general $A^kb \\in S$ for any $k \\geq 0$. Since every term in $(*)$ belongs to $S$, then so does their (infinite) sum, hence $x(t) \\in S$.\n",
    "\n",
    "This is because a subspace is a closed set, and so we are allowed to take infinite sums."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus on the case of complete matrices $A$ with real eigenvalues and eigenvectors; extensions to the general case are similar, and rely on using Jordan blocks and taking Real/Imaginary parts of complex eigenvectors.\n",
    "\n",
    "Suppose $A \\in \\mathbb{R}^{n \\times n}$ is complete with real eigenvalues/eigenvectors $(\\lambda_1, v_1), \\ldots, (\\lambda_n, v_n)$. Let's split the eigenvectors according to whether $\\lambda_i < 0$, $\\lambda_i = 0$, $\\lambda_i > 0$, and define the following invariant subspaces:\n",
    "\n",
    "\\begin{itemize}\n",
    "\\item The stable subspace $S \\subset \\mathbb{R}^n$ spanned by $v_i$ such that $\\lambda_i < 0$\n",
    "\\item The center subspace $C \\subset \\mathbb{R}^n$ spanned by $v_i$ such that $\\lambda_i = 0$\n",
    "\\item The unstable subspace $U \\subset \\mathbb{R}^n$ spanned by $v_i$ such that $\\lambda_i > 0$\n",
    "\\end{itemize}\n",
    "\n",
    "These subspaces are important, as they describe the long-term behavior of solutions to initial value problems. Without formally studying this observation, we make the following comments:\n",
    "\n",
    "\\begin{itemize}\n",
    "\\item If there are no eigenvalues of the specified type, we set the corresponding subspace to $\\{0\\}$.\n",
    "\\item $S \\cap C = S \\cap U = C \\cap U = \\{0\\}$, i.e., their pairwise intersections are trivial.\n",
    "\\item $S + C + U = \\mathbb{R}^n$, i.e., any vector $v \\in \\mathbb{R}^n$ can be uniquely written as a sum $v = s + c + u$ with $s \\in S$, $c \\in C$, $u \\in U$.\n",
    "\\end{itemize}\n",
    "\n",
    "Remembering that to each eigenvalue/vector pair $(\\lambda_i, v_i)$ we can associate an eigenfunction $x_i(t) = e^{\\lambda_i t}v_i$, we can characterize the following long-term behavior of solutions to $\\dot{x} = Ax$:\n",
    "\n",
    "\\begin{theorem}\n",
    "Let $A \\in \\mathbb{R}^{n \\times n}$ be a complete matrix with real eigenvalues/eigenvectors. Let $0 \\neq b \\in \\mathbb{R}^n$ and $x(t)$ be a solution to $\\dot{x} = Ax$, $x(0) = b$. Then $b$, and hence $x(t)$, are in:\n",
    "\n",
    "\\begin{enumerate}\n",
    "\\item The stable subspace $S$ if and only if $\\|x(t)\\| \\to 0$ as $t \\to \\infty$\n",
    "\\item The center subspace $C$ if and only if $x(t) = b$ for all $t \\in \\mathbb{R}$\n",
    "\\item The unstable subspace $U$ if and only if $\\|x(t)\\| \\to \\infty$ as $t \\to \\infty$\n",
    "\\end{enumerate}\n",
    "\\end{theorem}\n",
    "\n",
    "This theorem tells us what subsets of $\\mathbb{R}^n$ from which we should pick initial conditions $x(0) = b$ if we want our solutions to decay to zero (stable), not move (center), or blow up to infinity (unstable). This has very important applications in analyzing the behavior of dynamical systems, which we'll explore in the case study salad."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section*{Example}\n",
    "\n",
    "The matrix $A = \\begin{pmatrix} -2 & 1 & 0 \\\\ 1 & -1 & 1 \\\\ 0 & 1 & -2 \\end{pmatrix}$ has eigenvalue/vector pairs\n",
    "\n",
    "$\\lambda_1 = 0, v_1 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}, \\quad \n",
    "\\lambda_2 = -2, v_2 = \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix}, \\quad\n",
    "\\lambda_3 = -3, v_3 = \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix}.$\n",
    "\n",
    "Thus the stable subspace is spanned by $v_2$ and $v_3$, whose nonzero solutions tend to 0 as $t \\to \\infty$ (exponentially quickly), the center subspace is the line spanned by $v_1$, all of whose solutions are constant. In this case, no eigenvalues are positive and hence the unstable subspace is trivial: $U = \\{0\\}$.\n",
    "\n",
    "\\section*{Inhomogeneous Systems (HLA 10.4)}\n",
    "\n",
    "Many systems of interest can be modeled by first-order inhomogeneous linear systems of ordinary differential equations of the form:\n",
    "\n",
    "\\[\\dot{x} = Ax + f, \\quad (SYS)\\]\n",
    "\n",
    "where $A \\in \\mathbb{R}^{n \\times n}$, $x(t) \\in \\mathbb{R}^n$ is the solution, and $f(t) \\in \\mathbb{R}^n$ is a known vector-valued function of $t$ acting as an external force on the system.\n",
    "\n",
    "We rely on our work on general linear systems to know that the solution to (SYS) will have the general form given by the superposition:\n",
    "\n",
    "\\[x(t) = x^*(t) + z(t),\\]\n",
    "\n",
    "where $x^*(t)$ is a particular solution representing a response to the forcing, and $z(t)$ is a solution to the corresponding homogeneous system $\\dot{z} = Az$.\n",
    "\n",
    "Since we already know that $z(t) = e^{At}z(0)$, all that's left to work out is the particular solution $x^*(t)$ in response to the forcing $f(t)$.\n",
    "\n",
    "We'll start, as before, by recalling the scalar case. To solve $\\dot{x} = ax + f$, we set $x(t) = e^{at}v(t)$, where $v(t)$ is a function to be determined. Differentiating, we compute\n",
    "\n",
    "\\[\\dot{x} = ae^{at}v(t) + e^{at}\\dot{v} = ax + e^{at}\\dot{v},\\]\n",
    "\n",
    "from which we conclude $x(t)$ satisfies $\\dot{x} = ax + f$ if and only if $\\dot{v} = e^{-ta}f$, i.e., \n",
    "if and only if $v(t) = \\int_0^t e^{-sa}f(s)ds + v(0)$, where $v(0) = e^{-ta}x(0)$.\n",
    "\n",
    "Perhaps unsurprisingly, we can extend this method to the vector setting via the matrix exponential:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replace our initial guess with $x(t) = e^{At}v(t)$, where $v(t)$ is a vector-valued function to be determined. We compute the derivative $\\dot{x}$ as:\n",
    "\n",
    "\\[\\dot{x} = \\frac{d}{dt}(e^{At}v) = \\frac{d}{dt}(e^{At})v + e^{At}\\frac{dv}{dt} = Ae^{At}v + e^{At}\\dot{v} = Ax + e^{At}\\dot{v}\\]\n",
    "\n",
    "From which we conclude that $\\dot{v} = e^{-At}f$. We can integrate both sides to obtain, via the Fundamental Theorem of Calculus, that:\n",
    "\n",
    "\\[v(t) = v(0) + \\int_0^t e^{-As}f(s)ds, \\quad \\text{where } v(0) = e^{-tA}x(0)\\]\n",
    "\n",
    "This is the last piece we needed to write the general solution to the inhomogeneous initial value problem:\n",
    "\n",
    "\\begin{theorem}\n",
    "The solution to the initial value problem\n",
    "\\[\\dot{x} = Ax + f, \\quad x(0) = b \\quad \\text{is} \\quad x(t) = e^{(t-0)A}b + \\int_0^t e^{(t-s)A}f(s)ds\\]\n",
    "\\end{theorem}\n",
    "\n",
    "Example: Online notes please transcribe Example 10.3b, but replace u with x for notation, and compute $e^{At}$ via diagonalization. Make sure to highlight $f(t) = \\begin{bmatrix} 0 \\\\ e^t \\end{bmatrix}$\n",
    "\n",
    "\\section*{Application to Mechanical Systems (ALA 10.6)}\n",
    "\n",
    "We'll start with the simplest possible mechanical system: a single mass connected to a fixed support by a spring. Assuming no external force, Newton's Law of Force = Mass $\\times$ Acceleration results in the following homogeneous second order scalar equation:\n",
    "\n",
    "\\[m\\ddot{p} + kp = 0 \\quad (SM), \\quad m > 0 \\text{ mass} \\\\ k > 0 \\text{ spring constant}\\]\n",
    "\n",
    "where $p(t) \\in \\mathbb{R}$ is the mass' position over time, $\\dot{p}$ its velocity, and $\\ddot{p}$ its acceleration. Our first order of business is to convert (SM) to a first-order system. To do so, we define the vector $x \\in \\mathbb{R}^2$ as\n",
    "\n",
    "\\[x = \\begin{pmatrix} p \\\\ \\dot{p} \\end{pmatrix}, \\quad \\text{i.e., we stack position and velocity.}\\]\n",
    "\n",
    "Then $\\dot{x} = \\begin{pmatrix} \\dot{p} \\\\ \\ddot{p} \\end{pmatrix} = \\begin{pmatrix} \\dot{p} \\\\ -\\frac{k}{m}p \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ -\\frac{k}{m} & 0 \\end{pmatrix} \\begin{pmatrix} p \\\\ \\dot{p} \\end{pmatrix} =: Ax$.\n",
    "\n",
    "Thus, the solutions to (SM) can be obtained by reading off the first component of $x(t) = (p(t), \\dot{p}(t))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix $A$ has imaginary eigenvalues $\\lambda_{\\pm} = \\pm i \\sqrt{\\frac{k}{m}}$ (what physical intuition might have suggested this?), with corresponding eigenvectors\n",
    "\n",
    "\\[\n",
    "v_{\\pm} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\pm i \\begin{pmatrix} 0 \\\\ \\sqrt{\\frac{m}{k}} \\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "Therefore, solutions are weighted sums of\n",
    "\n",
    "\\begin{align*}\n",
    "x_{\\pm}(t) &= e^{\\pm i\\sqrt{\\frac{k}{m}}t} \\left(\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\pm i \\begin{pmatrix} 0 \\\\ \\sqrt{\\frac{m}{k}} \\end{pmatrix}\\right) \\\\\n",
    "&= \\begin{pmatrix} \\cos(\\sqrt{\\frac{k}{m}}t) \\\\ \\pm \\sin(\\sqrt{\\frac{k}{m}}t) \\end{pmatrix} \\pm i \\begin{pmatrix} \\sin(\\sqrt{\\frac{k}{m}}t) \\\\ \\cos(\\sqrt{\\frac{k}{m}}t) \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "but, really we want real solutions, we write the general solution as\n",
    "\n",
    "\\begin{align*}\n",
    "x(t) &= c_1 \\text{Re}\\{x_+(t)\\} + c_2 \\text{Im}\\{x_+(t)\\} \\\\\n",
    "&= c_1 \\begin{pmatrix} \\cos(\\sqrt{\\frac{k}{m}}t) \\\\ \\sin(\\sqrt{\\frac{k}{m}}t) \\end{pmatrix} + c_2 \\begin{pmatrix} \\sin(\\sqrt{\\frac{k}{m}}t) \\\\ \\cos(\\sqrt{\\frac{k}{m}}t) \\end{pmatrix} \\\\\n",
    "&= \\begin{pmatrix} \\cos(\\sqrt{\\frac{k}{m}}t) & \\sin(\\sqrt{\\frac{k}{m}}t) \\\\ -\\sin(\\sqrt{\\frac{k}{m}}t) & \\cos(\\sqrt{\\frac{k}{m}}t) \\end{pmatrix} \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} = \\begin{pmatrix} \\cos(\\sqrt{\\frac{k}{m}}t) & \\sin(\\sqrt{\\frac{k}{m}}t) \\\\ -\\sin(\\sqrt{\\frac{k}{m}}t) & \\cos(\\sqrt{\\frac{k}{m}}t) \\end{pmatrix} \\begin{pmatrix} p(0) \\\\ v(0) \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "where $x(0) = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} = \\begin{pmatrix} p(0) \\\\ v(0) \\end{pmatrix}$. Thus we see that a single mass connected to a fixed support by a spring oscillates with frequency $\\sqrt{\\frac{k}{m}}$ in the absence of external forces like friction or damping.\n",
    "\n",
    "Note: we could also have computed $x(t) = e^{At}x(0)$ by directly computing the matrix exponential from the power series definition (MRS).\n",
    "\n",
    "Next, we modify our problem to make it a bit more realistic to add friction, which is proportional to $\\dot{p}$. Newton's law then becomes:\n",
    "\n",
    "\\[\n",
    "m\\ddot{p} + b\\dot{p} + kp = 0.\n",
    "\\]\n",
    "\n",
    "Defining $x = (p,\\dot{p})$ as before, our first order reformulation becomes:\n",
    "\n",
    "\\[\n",
    "\\dot{x} = \\begin{pmatrix} 0 & 1 \\\\ -\\frac{k}{m} & -\\frac{b}{m} \\end{pmatrix} \\begin{pmatrix} p \\\\ \\dot{p} \\end{pmatrix} = Ax.\n",
    "\\]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvalues of $A$ are the solutions to the characteristic equation:\n",
    "\n",
    "\\[\n",
    "m\\lambda^2 + b\\lambda + k = 0. \\tag{*}\n",
    "\\]\n",
    "\n",
    "There are three possible cases:\n",
    "\n",
    "Overdamped: If $b > 2\\sqrt{mk}$ then the equation (*) has two negative real roots\n",
    "\n",
    "\\[\n",
    "\\lambda_{\\pm} = -\\frac{b \\pm \\sqrt{b^2 - 4mk}}{2m},\n",
    "\\]\n",
    "\n",
    "and thus the solution is given by a linear combination of two decaying exponentials\n",
    "\n",
    "\\[\n",
    "x(t) = c_1 e^{\\lambda_+ t} v_+ + c_2 e^{\\lambda_- t} v_-,\n",
    "\\]\n",
    "\n",
    "for $v_{\\pm}$ the corresponding real eigenvectors of $\\lambda_{\\pm}$. Such a system has so much friction that it no longer oscillates!\n",
    "\n",
    "Underdamped: If $0 < b < 2\\sqrt{mk}$, then (*) has two complex-conjugate roots:\n",
    "\n",
    "\\[\n",
    "\\lambda_{\\pm} = -\\frac{b}{2m} \\pm i\\frac{\\sqrt{4mk-b^2}}{2m} =: -\\mu \\pm ir.\n",
    "\\]\n",
    "\n",
    "With a little bit of effort, we can compute the matrix exponential here as\n",
    "\n",
    "\\[\n",
    "e^{At} = e^{-\\mu t} \\begin{bmatrix} \\cos rt & \\sin rt \\\\ -\\sin rt & \\cos rt \\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "so that $x(t) = e^{At} \\begin{bmatrix} p(0) \\\\ \\dot{p}(0) \\end{bmatrix}$. In contrast to the undamped\n",
    "\n",
    "setting, we see here that oscillations continue at fixed frequency $r = \\sqrt{\\frac{k}{m} - \\frac{b^2}{4m^2}}$,\n",
    "\n",
    "while the entire trajectory also decays exponentially at rate $\\mu = \\frac{b}{2m}$. Thus\n",
    "\n",
    "for small friction, we eventually stop moving, but continue to oscillate as we\n",
    "\n",
    "go to the origin.\n",
    "\n",
    "Critically damped: The borderline case occurs when $b = b_c = 2\\sqrt{mk}$, in which case\n",
    "our matrix $A$ has one eigenvalue $\\lambda = -\\frac{b}{2m}$, and is similar to the Jordan block\n",
    "\n",
    "\\[\n",
    "J_2 = \\begin{bmatrix} -\\frac{b}{2m} & 1 \\\\ 0 & -\\frac{b}{2m} \\end{bmatrix}\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the techniques we saw last class, we compute the matrix exponential\n",
    "\n",
    "\\[\n",
    "e^{At} = \\begin{bmatrix}\n",
    "e^{\\lambda t} & te^{\\lambda t} \\\\\n",
    "\\lambda e^{\\lambda t} & e^{\\lambda t} + \\lambda te^{\\lambda t}\n",
    "\\end{bmatrix} \\quad \\text{(please double check!)}\n",
    "\\]\n",
    "\n",
    "and thus our solution is\n",
    "\n",
    "\\[\n",
    "\\begin{bmatrix}\n",
    "p(t) \\\\\n",
    "\\dot{p}(t)\n",
    "\\end{bmatrix} = e^{At} \\begin{bmatrix}\n",
    "p(0) \\\\\n",
    "\\dot{p}(0)\n",
    "\\end{bmatrix} = e^{\\lambda t} \\left(\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "\\lambda\n",
    "\\end{bmatrix} p(0) + \n",
    "\\begin{bmatrix}\n",
    "t \\\\\n",
    "1 + \\lambda t\n",
    "\\end{bmatrix} \\dot{p}(0)\n",
    "\\right).\n",
    "\\]\n",
    "\n",
    "Even though the formula looks quite different, the qualitative behavior is very similar to the overdamped case, as when $t \\to \\infty$, the exponential decay $e^{-\\frac{b}{2m}t}$ governs the system's behavior. This corresponds to a non-vibrating solution with the slowest possible decay rate -- any further reduction in $b$ will allow for a damped slowly oscillatory vibration to appear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/06_Ch_7_Dynamics/084-Invariant_Sub.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
