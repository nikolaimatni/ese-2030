{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 3.5 Clustering and the K-Means Algorithm\n",
    "subject: Inner Products and Norms\n",
    "subtitle: An example of unsupervised machine learning\n",
    "short_title: 3.5 Clustering and K-Means\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: Clutering, K-Means\n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/02_Ch_3_Inner_Products_and_Norms/044-clustering.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 06 - Clustering and K-means.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be found in VMLS Chapter 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- the clustering problem\n",
    "- the centroid, or mean, of a group of vectors\n",
    "- the K-means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Clustering Problem\n",
    "\n",
    "## An Informal Example of Clustering (VMLS 4.1)\n",
    "\n",
    "Suppose we have $N$ vectors $\\vv{x_1}, ..., \\vv{x_N} \\in V$. The goal of clustering is to group the vectors into $k$ groups, $k$ groups or *clusters* of vectors that are close to each other, as measured by the [distance](#general_distance_defn) between pairs of them.\n",
    "\n",
    "Normally, the number of groups $k$ is much smaller than the total number of vectors $N$. Typical values in practice for $k$ range from a handful (2-3) to hundreds, and $N$ ranges from hundreds to billions.\n",
    "\n",
    "The figure below shows a simple example with $N = 300$ vectors in $\\mathbb{R}^2$, shown as small circles. The right picture shows that is easily seen: these vectors can be clustererd into $k = 3$ groups in a way that \"looks right\" (we will quantify this idea soon).\n",
    "\n",
    ":::{figure}../figures/04-3_clusters.png\n",
    ":label:Grouping vectors into 3 clusters\n",
    ":alt: A collection of $300$ vectors in $\\mathbb{R}^n$ grouped into $3$ clusters by eye\n",
    ":width: 500px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    "This is example is a bit silly; for vectors in $\\mathbb{R}^2$, clustering is easy; just make a scatter plot and use your eyes. In almost all applications, vectors live in $\\mathbb{R}^n$ with $n$ much bigger than 2. Another silly aspect is how cleanly points split into clusters; real data is messy, and often many points lie between clusters. Finally, in real examples, it is not always obvious how many clusters $k$ there are.\n",
    "\n",
    "## Applications of Clustering\n",
    "\n",
    "Despite all of this, we'll see clustering can still be incredibly useful in practice. Before we dive into more details, let's highlight a few common applications where clustering is useful:\n",
    "\n",
    "* **Topic discovery.** Suppose $\\vv{x_i}$ are word histograms associated with $N$ documents (a word histogram $\\vv x$ has entries $x_i$ which count the number of times word $i$ appears in a document). Clustering will partition the $N$ documents into $k$ groups, which can be interpreted as groups of documents with the same or similar topics, genre, or author. This is sometimes called *automatic topic discovery*.\n",
    "\n",
    "* **Customer market segmentation.** Suppose the vector $\\vv{x_i} \\in \\mathbb{R}^n$ gives the dollar values of $n$ items purchased by customer $i$ in the past year. A clustering algorithm groups the customers into $k$ market segments, which are groups of customers with similar purchasing patterns.\n",
    "\n",
    "Other examples include patient, zip code, student, and survey response clustering, as well as identifying weatehr zones, daily energy use patterns, and financial sectors. See pp. 70-71 of VLMS for more details.\n",
    "\n",
    "## A Clustering Objective (VLMS 4.2)\n",
    "\n",
    "Our goal now is to formalize the ideas described above, and introduce a quantitative measure of \"how good a clustering\" is.\n",
    "\n",
    "### Specifying Cluster Assignments\n",
    "\n",
    "We specify a clutering of vectors by assigning each vector to a group. We label the groups $1, ..., k$ and assign each of the $N$ vectors $\\vv{x_1}, .., \\vv{x_n}$ to a group via the vector $\\vv c \\in \\mathbb{R}^N$, with $c_i$ being the group number that $\\vv{x_i}$ has been assigned to. \n",
    "\n",
    "For example, if $N = 5$ and $k = 3$, then\n",
    "\n",
    "\\begin{align*}\n",
    "    c = \\bm 3\\\\1\\\\1\\\\1\\\\2 \\em \n",
    "\\end{align*}\n",
    "assigns $\\vv{x_1}$ to group 3; $\\vv{x_2}, \\vv{x_3}, \\vv{x_4}$ to group 1; $\\vv{x_5}$ to group 2.\n",
    "\n",
    "We will also describe clusters by the sets of indices for each group, with $G_j$ being the set of indices associated with group $j$. For our simple example, we have\n",
    "\n",
    "\\begin{align*}\n",
    "    G_1 = \\{ 2, 3, 4 \\}, \\quad G_2 = \\{5\\}, \\quad G_3 = \\{1\\}\n",
    "\\end{align*}\n",
    "\n",
    "In general, we have that $G_j = \\{ i \\mid c_i = j\\}$.\n",
    "\n",
    "### Group Representatives \n",
    "\n",
    "Each group is assigned a *group representative* $\\vv{z_1}, ..., \\vv{z_k} \\in V$. Note that these representatives can be any vector, and need not be one of the given vectors $\\vv{x_1}, ..., \\vv{x_N}$. A good clustering will have each representative close to vectors in its associated group, i.e.,\n",
    "\n",
    "\\begin{align*}\n",
    "    \\text{dist}(\\vv{x_i}, \\vv{z_{c_i}}) = \\| \\vv{x_i} - \\vv{z_{c_i}} \\|\n",
    "\\end{align*}\n",
    "is small for all $i = 1,..., N$. Note that according to our notation, $\\vv{x_i}$ is in group $c_i$, so $\\vv{z_{c_i}}$ is the group representative against which $\\vv{x_i}$ should be measured.\n",
    "\n",
    "### A Clustering Objective\n",
    "\n",
    "We now define a *clustering objective* that assigns a score, or cost, to a choice of clustering and representatives:\n",
    "\n",
    ":::{prf:definition} The Clustering Objective\n",
    ":label: clustering-objective-defn\n",
    "We will define the clustering cost associated with choice of group representantives $\\vv{x_1}, ..., \\vv{x_k}$ and a choice of assignments $c_1, ..., c_N$ as\n",
    "\n",
    "\\begin{align*}\n",
    "    J_{\\text{clust}} := \\frac{\\| \\vv{x_1} - \\vv{z_{c_1}} \\|^2 + \\| \\vv{x_2} - \\vv{z_{c_2}} \\|^2 + ... + \\| \\vv{x_N} - \\vv{z_{c_N}} \\|^2}{N}\n",
    "\\end{align*}\n",
    "\n",
    "This computes the mean square distance from the vectors to their associated representatives. The smaller $J_{\\text{clust}}$ is, the \"better\" the clustering. (What does it mean if $J_{\\text{clust}} = 0$?)\n",
    ":::\n",
    "\n",
    "A clustering is said to be *optimal* if the choice of group assignments $c_1, ..., c_N$ and group representatives $\\vv{z_1}, ..., \\vv{z_N}$ lead to the smallest achievable clustering objective $J_{\\text{clust}}$; in this case, these choices are said to *minimize the objective $J_{\\text{clust}}$*. Unfortunately, except for very small problems, it is computationally prohibitive to find an optimal clustering.\n",
    "\n",
    "Forutunately, the *k-means algorithm* we will introduce next can be run efficiently on very large problems, and often finds very good clusterings that achieve objective values $J_{\\text{clust}}$ near the smallest possible value. Because k-means finds *suboptimal* solutions, we call it a *heuristic*. Heuristics are often looked down on in more theory oriented circles because they cannot guarantee the quality of their solutions, but as we will see, they often work incredibly well in practice!\n",
    "\n",
    "# The K-Means Algorithm\n",
    "\n",
    "The idea behind k-means is to break down the overall hard problem of choosing the best representatives and clusterings at the same time into two subproblems we can easily solve effectively. While we can't yet solve the problem of jointly choosing group assignments of group representatives to minimize $J_{\\text{clust}}$, we know how to solve for one component when the other is fixed.\n",
    "\n",
    "## Partitioning Vectors with Representatives Fixed\n",
    "\n",
    "Pretend for a moment that we have already found group representatives $\\vv{z_1}, ..., \\vv{z_k}$, and our task is to pick the group assignments $c_1, ..., c_N$ which lead to the smallest possible $J_{\\text{clust}}$. This problem is actually very easy! We will use the idea of [nearest neighbors](#distance-nearest_neighbors) we saw in the previous section.\n",
    "\n",
    "Notice that the objective [$J_{\\text{clust}}$](#clustering-objective-defn) is a sum of $N$ terms, with one term for each vector $\\vv{x_i}$. Further, the choice of $c_i$ (i.e., the group to which we assign $\\vv{x_i}$) only affects the term \n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac 1 N \\| \\vv{x_i} - \\vv{z_{c_i}} \\|^2\n",
    "\\end{align*}\n",
    "\n",
    "in $J_{\\text{clust}}$. This means we should choose $c_i$ to make this term smallest since it doesn't affect any other terms in our clustering objective. To minimize $\\| \\vv{x_i} - \\vv{z_{c_i}} \\|$, we pick the $c_i$ so that\n",
    "\n",
    "\\begin{align*}\n",
    "    \\| \\vv{x_i} - \\vv{z_{c_i}} \\| \\leq \\| \\vv{x_i} - \\vv{z_{c_j}} \\| \\quad \\text{for $j = 1, ..., k$.}\n",
    "\\end{align*}\n",
    "\n",
    "This should look familiar! Modulo our new notation, we should asign $\\vv{x_i}$ to its [nearest neighbor](#distance-nearest_neighbors) among the representatives.\n",
    "\n",
    "## Optimizing the Group Representatives with Assignments Fixed\n",
    "\n",
    "Now we flip things around, and assume each vector $\\vv{x_i}$ has been assigned to a group $c_i$. How should we pick group representatives $\\vv{z_1}, ..., \\vv{z_k}$ to minimize $J_{\\text{clust}}$? We start be rearranging our objectives into $k$ sums, one for each group:\n",
    "\n",
    "\\begin{align*}\n",
    "    J_{\\text{clust}} = J_1 + J_2 + ... + J_k\n",
    "\\end{align*}\n",
    "\n",
    "where $J_j := \\frac 1 N \\sum_{i \\in G_j}{\\| \\vv{x_i} - \\vv{z_j} \\|^2}$ is the contribution to $J_{\\text{clust}}$ from the vectors in group $j$. The sum notation here means we should include terms $\\| \\vv{x_i} - \\vv{z_j} \\|^2$ in our sum if $i \\in G_j$ (i.e., if $\\vv{x_i}$ has been assigned to group $j$).\n",
    "\n",
    "The choice of representative $\\vv{z_j}$ only affects the term $J_j$, so we can choose $\\vv{z_j}$ to minimize $J_j$. You can check, e.g., using vector calculus, that the best choice is to pick $\\vv{z_j}$ to be the *average (or centroid)* of the vectors in group $j$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\vv{z_j} = \\frac{1}{|G_j|} \\sum_{i \\in G_j}{\\vv{x_i}}\n",
    "\\end{align*}\n",
    "\n",
    "Here $|G_j|$ is the *cardinality of the set $G_j$*, and denotes the number of elements in the set $G_j$, i.e., the size of group $j$.\n",
    "\n",
    "## Pseudocode for the k-means Algorithm\n",
    "\n",
    "While we can't yet solve the problem of jointly choosing group assignments & group representatives $J_{\\text{clust}}$, we know how to solve for one component when the other is fixed. \n",
    "\n",
    "The k-means algorithm produces an approximate solution to the clustering problem by *iterating* between the two subroutines. A key feature of this approach is that $J_{\\text{clust}}$ gets better or stays the same with each iteration, meaning it is guaranteed to converge, to a (likely suboptimal) solution.\n",
    "\n",
    "We next state the pseudocode for the k-means algorithm.\n",
    "\n",
    ":::{prf:algorithm} The K-Means Algorithm\n",
    ":label: k-means-alg\n",
    "\n",
    "**Inputs** A list of $N$ vectors $\\vv{x_1}, ..., \\vv{x_N}$, and an initial list of $k$ group representative vectors $\\vv{z_1}, ..., \\vv{z_k}$\n",
    "\n",
    "**Output** An approximate solution to the clustering problem\n",
    "\n",
    "* Repeat until convergence:\n",
    "  * *Parition the vectors into $k$ groups.* For each vector $i = 1, ..., N$, assign $\\vv{x_i}$ to the group associated with the nearest representative.\n",
    "  * *Update representatives*. For each group $j = 1, ..., k$, set $\\vv{z_j}$ to be the mean of the vectors in group $j$.\n",
    ":::\n",
    "\n",
    "One iteration of the k-means algorithm is illustrated below:\n",
    "\n",
    ":::{figure}../figures/04-kmeans.png\n",
    ":label:Grouping vectors into 3 clusters\n",
    ":alt: A collection of $300$ vectors in $\\mathbb{R}^n$ grouped into $3$ clusters by eye\n",
    ":width: 500px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    "Left: vectors $\\vv{x_1}, ..., \\vv{x_N}$ are assigned to the nearest representative $\\vv{z_j}$.\n",
    "\n",
    "Right: the representatives are updated to the centroids of the new groups.\n",
    "\n",
    ":::{warning} Some comments and clarifications\n",
    "* Ties can be broken by assigning $\\vv{x_i}$ to the tied group of smallest $j$ (or any other *deterministic* rule â€“ assigning at random can affect convergece of the algorithm).\n",
    "\n",
    "* If at any time a group is empty, it (and its representative) are simply dropped for the rest of the execution.\n",
    "\n",
    "* If group assignments don't change during an iteration, then the representatives will also stay the same: this is what we mean by the algorithm *converging*.\n",
    "\n",
    "* there are many ways to initialize the group representatives: a common approach is to pick them at random from the $\\vv{x_i}$.\n",
    ":::\n",
    "\n",
    "\n",
    ":::{warning} Warning: K-means is a heuristic!\n",
    "* There is no guarantee that the solutions that k-means produces are globally optimal!\n",
    "\n",
    "* For the implementation of k-means given above, there is also no guarantee that it will converge to a solution quickly; it might take a long time (even exponentially long in the number of clusters) to converge for some inputs.\n",
    ":::\n",
    "\n",
    "#### Python break!\n",
    "\n",
    "The ```scikit-learn``` library in Python gives an implementation of the k-means algorithm. Documentation is available [here](#https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans).\n",
    "\n",
    "In the following code, we generate a sample clustering problem with $N = 600$ and $k = 3$. We then run k-means, plot some intermediate clusterings, and plot the final clustering. We also plot the rate of convergence (k-means loss vs. number of iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeans\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m2030\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(2030)\n",
    "\n",
    "# Sample data\n",
    "sample1 = np.random.multivariate_normal(\n",
    "    mean=np.array([2, 0]),\n",
    "    cov=np.array([[2, 0], [0, 2]]),\n",
    "    size=300\n",
    ")\n",
    "\n",
    "sample2 = np.random.multivariate_normal(\n",
    "    mean=np.array([-1, 4]),\n",
    "    cov=np.array([[2, 0.5], [0.5, 1]]),\n",
    "    size=180\n",
    ")\n",
    "\n",
    "sample3 = np.random.multivariate_normal(\n",
    "    mean=np.array([-3, -2]),\n",
    "    cov=np.array([[1, -0.2], [-0.2, 2]]),\n",
    "    size=120\n",
    ")\n",
    "\n",
    "# Generates a dataset with data sampled from 3 normal distributions.\n",
    "data = np.concatenate((sample1, sample2, sample3), axis=0)\n",
    "initial_representatives = [[5, 0], [4.5, 0], [4, 0]]\n",
    "\n",
    "# Define RGB colors for the clusters.\n",
    "colors = np.array([\n",
    "    [1, 0, 0], \n",
    "    [0, 1, 0], \n",
    "    [0, 0, 1]  \n",
    "])\n",
    "\n",
    "fig, axes = plt.subplots(5, 1, figsize=(20, 20))\n",
    "\n",
    "# For demonstration purposes, we will purposely limit the number of k-means iterations\n",
    "# and show you the outputs along the way!\n",
    "num_iterations = [1, 2, 6]\n",
    "for i in range(len(num_iterations)):\n",
    "    kmeans = KMeans(n_clusters=3, random_state=0, init=initial_representatives, max_iter=num_iterations[i]).fit(data)\n",
    "\n",
    "    ax = axes[i]\n",
    "    ax.scatter(data[:, 0], data[:, 1], c=colors[kmeans.labels_], s=5)\n",
    "    ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='x', c='black', s=100)\n",
    "    ax.set_title(f'K-Means with {num_iterations[i]} Iterations')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Now, we allow the algorithm to run until convergence and plot the clusters.\n",
    "kmeans = KMeans(n_clusters=3, random_state=0, init=initial_representatives).fit(data)\n",
    "\n",
    "ax = axes[3]\n",
    "ax.scatter(data[:, 0], data[:, 1], c=colors[kmeans.labels_], s=5)\n",
    "ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='x', c='black', s=100)\n",
    "ax.set_title(f'Final Clustering')\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "# We also run the code again for a bunch of different iteration numbers, and plot the \n",
    "# rate of convergence to the solution.\n",
    "costs = []\n",
    "for i in range(kmeans.n_iter_):\n",
    "    kmeans = KMeans(n_clusters=3, random_state=0, init=initial_representatives, max_iter=i+1).fit(data)\n",
    "    costs.append(kmeans.inertia_ / data.shape[0])\n",
    "\n",
    "ax = axes[4]\n",
    "ax.plot(range(1, kmeans.n_iter_ + 1), costs, marker='o', linestyle='-', color='b')\n",
    "ax.set_title('Rate of Convergence of K-means')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Cost (Mean Squared Distance)')\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xticks(range(1, kmeans.n_iter_ + 1))\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Automated Topic Discovery\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/02_Ch_3_Inner_Products_and_Norms/044-clustering.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
