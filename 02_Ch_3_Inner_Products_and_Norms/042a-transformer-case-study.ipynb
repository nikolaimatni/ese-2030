{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1940a536-1a33-471c-b6c8-b4bbb78f114c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "title: \"3.3 Case Study: ChatGPT, Transformers, and Inner Products\"\n",
    "subject: Inner Products and Norms\n",
    "subtitle: Attention mechanisms are fancy inner products\n",
    "short_title: \"3.3 Case Study: ChatGPT and Inner Products\"\n",
    "authors:\n",
    "  - name: Renukanandan Tumu\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nandant@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: sample notes, ese 2030, linear algebra\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb5d88c-e8e1-4be6-aa17-80f32a96f291",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/02_Inner_Products_and_Norms/Case_Study_On_Transformers/048-transformer-case-study.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a417db5-0891-4e82-9c09-89ca997c687c",
   "metadata": {},
   "source": [
    "## What are transformers?\n",
    "\n",
    "Transformers are a popular architecture for machine-learning models today, powering most notably, [Large Language Models (LLMs)](https://openai.com/index/better-language-models/). We will examine how the inner product makes up its core."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cc9bd4-0ed7-4a40-8770-3d6f5603b763",
   "metadata": {},
   "source": [
    "First, let's look at the archictecture itself. The Transformer was first proposed by Vaswani et. al. in [*Attention is all you need*](https://arxiv.org/abs/1706.03762v7). At the core of the transformer is the Attention Mechanism. \n",
    "\n",
    "The Transformer was originally proposed to help with machine translation, to be able to translate large documents like books from one language to another. This is one of the core topics of study of Natural Language Processing or NLP. At the time the Transformer was developed, the state of the art approach was the [LSTM](https://dl.acm.org/doi/10.1162/neco.1997.9.8.1735). Unfortunately when translating an entire book, these approaches were unable to keep context from earlier in the book, or even in a sentence. This presented challenges when translating from languages that flip the order of the verb and the subject.\n",
    "\n",
    "## Machine Translation Basics\n",
    "\n",
    "In order for us to try operating on words, we need to find mathematical representations for them. The way that words are represented in Machine Translation is through vectors. We can build dictionaries that consist of these vectors, and create a library of these keys $K$. We can also have a set of vectors that represent our target language, in a set of values $V$. For our example, $K$ can have a set of vectors that represent the meaning of some words, like 'with', 'above', and 'below'. If we are translating to french, we would use the corresponding words in French 'avec', 'au-dessus', 'ci-dessous'. Because we have only 3 keys and 3 values, we can represent them as vectors in $\\mathbb{R}^3$ using One Hot Encoding. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabf4879-a3b4-4be0-bdee-5bacd23c1ae2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What does Attention do?\n",
    "\n",
    "Attention helps us relate words from our query $Q$, to a set of keys $K$, into a domain of values $V$. \n",
    "A key matrix $K$ in the translation example would be a dictionary of words in the source language, English. The value matrix $V$ would be a dictionary of words in the target language, French, where each word in the $K$ aligns with its corresponding word in $V$.\n",
    "The query is the word or set of words that we are looking to translate.\n",
    "\n",
    "The dot product here can be thought of as a lookup table, identifying elements with closely matching vectors. We can think of this in the same way we think of cosine similarity, where the dot product tells us how \"aligned\" the vectors are. We can think of this as a lookup table, identifying which parts of the key matrix \"match\" our query.\n",
    "\n",
    "## The Attention Mechanism\n",
    "\n",
    "The core of the attention mechanism is the dot product. Let's look at the formula for Attention piecewise, using a simple example.\n",
    "### Definitions\n",
    "```{math}\n",
    "\\begin{gather}\n",
    "Q = \\begin{bmatrix}\\text{with} \\\\ \\text{below}\\end{bmatrix} = \\begin{bmatrix}\n",
    "0 & 5 & 0 \\\\\n",
    "0 & 0 & 5\n",
    "\\end{bmatrix}\\\\\n",
    "\n",
    "K = \\begin{bmatrix}\n",
    "\\text{above} &\n",
    "\\text{below} &\n",
    "\\text{with}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "5 & 0 & 0 \\\\\n",
    "0 & 0 & 5 \\\\\n",
    "0 & 5 & 0 \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\n",
    "V = \\begin{bmatrix}\n",
    "\\text{au-dessus} &\n",
    "\\text{ci-dessous} &\n",
    "\\text{avec}\n",
    "\\end{bmatrix}= \n",
    "\\begin{bmatrix}\n",
    "5 & 0 & 0 \\\\\n",
    "0 & 5 & 0 \\\\\n",
    "0 & 0 & 5 \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\end{gather}\n",
    "```\n",
    "In code, this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "094abcde-6ce1-4a1b-a793-53f3cf9a11e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Q = np.array([[0,5,0],[0,0,5]])\n",
    "K = np.array([\n",
    "    [5,0,0],\n",
    "    [0,0,5],\n",
    "    [0,5,0]\n",
    "])\n",
    "V = np.array([\n",
    "    [5, 0, 0],\n",
    "    [0, 5, 0],\n",
    "    [0, 0, 5]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a014057d-3619-4e3c-ac3e-e9e8c1fc2500",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Part 1: Dot-Product\n",
    "\n",
    "```{math}\n",
    "QK^T = A\n",
    "```\n",
    "We'll organize this dot product so we can see what components lead to which items in $A$\n",
    "```math\n",
    "\\begin{aligned}\n",
    "\\begin{array}{lll}\n",
    " \\\\\n",
    " \\\\\n",
    " \\\\\n",
    "\\end{array}&\n",
    "\\left[\\begin{array}{lll}\n",
    "k_{1,1} & k_{2,1} & k_{3,1}  \\\\\n",
    "k_{1,2} & k_{2,2} & k_{3,2}  \\\\\n",
    "k_{1,3} & k_{2,3} & k_{3,3}  \\\\\n",
    "\\end{array}\\right] \\\\\n",
    "\\left[\\begin{array}{lll}\n",
    "q_{1,1} & q_{1,2} & q_{1,3}  \\\\\n",
    "q_{2,1} & q_{2,2} & q_{2,3}  \\\\\n",
    "\\end{array}\\right]&\\left[\\begin{array}{lll}\n",
    "a_{1,1} & a_{1,2} & a_{1,3} \\\\\n",
    "a_{2,1} & a_{2,2} & a_{2,3} \\\\\n",
    "\\end{array}\\right] \\\\\n",
    "&\n",
    "\\end{aligned}\n",
    "```\n",
    "Now we can replace these placeholders with data from our example.\n",
    "```math\n",
    "\\begin{aligned}\n",
    "\\begin{array}{lll}\n",
    " \\\\\n",
    " \\\\\n",
    " \\\\\n",
    "\\end{array}&\n",
    "\\left[\\begin{array}{lll}\n",
    "5 & 0 & 0 \\\\\n",
    "0 & 0 & 5 \\\\\n",
    "0 & 5 & 0 \\\\\n",
    "\\end{array}\\right] \\\\\n",
    "\\left[\\begin{array}{lll}\n",
    "0 & 5 & 0 \\\\\n",
    "0 & 0 & 5\n",
    "\\end{array}\\right]&\\left[\\begin{array}{lll}\n",
    "0 & 0 & 5 \\\\\n",
    "0 & 5 & 0 \\\\\n",
    "\\end{array}\\right] \\\\\n",
    "&\n",
    "\\end{aligned} \\\\\n",
    "A = \\left[\\begin{array}{lll}\n",
    "0 & 0 & 25 \\\\\n",
    "0 & 25 & 0 \\\\\n",
    "\\end{array}\\right]\n",
    "```\n",
    "This forms the core of the attention mechanism: matching items in the query to items in our database of keys. This relation is done entirely through the dot-product. We can see that the matrix $A$ takes higher values when the query is similar to the corresponding key in the database $K$, and lower values when the query and database are dissimilar. \n",
    "In code, this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39525504-e62d-40d7-9996-54142d91d058",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, 25],\n",
       "       [ 0, 25,  0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = Q@K.T\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39713bf-efa6-4205-8ae1-1e361bf6682f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 2: Scaling\n",
    "The next step is to scale the output by a factor of $d_k$, where $d_k$ is the dimension of the keys. For us, this value is $3$, because each key is in $\\mathbb{R}^3$. The formula here is:\n",
    "```{math}\n",
    "\\begin{gather}\n",
    "\\frac{A}{d_k} = B \\\\\n",
    "\\left[\\begin{array}{lll}\n",
    "0 & 0 & 25 \\\\\n",
    "0 & 25 & 0 \\\\\n",
    "\\end{array}\\right] / d_k = \\left[\\begin{array}{lll}\n",
    "0 & 0 & 8.3 \\\\\n",
    "0 & 8.3 & 0 \\\\\n",
    "\\end{array}\\right]\n",
    "\n",
    "\\end{gather}\n",
    "```\n",
    "In code, this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d476e01-1b3f-41e5-a102-92b45f38b495",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 8.33333333],\n",
       "       [0.        , 8.33333333, 0.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = A/3\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3882dd7-8323-45a6-aa26-e9f78be33e12",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 3: Softmax\n",
    "\n",
    "The softmax operator is a nonlinear operator which maps a vector of values so that they all lie between 0 and 1, and all sum to 1. The intuition is that it is a smooth version of the maximum operator. The formula for the softmax for a vector $z \\in \\mathbb{R}^K$ is below:\n",
    "$$\n",
    "\\text{softmax}\\left(z\\right)_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\n",
    "$$\n",
    "\n",
    "Our formula is:\n",
    "```{math}\n",
    "C = \\text{softmax}(B)\n",
    "```\n",
    "Using the softmax, we are creating a sort of lookup table to identify which part of the key database best represents the content in the query. \n",
    "\n",
    "In code this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f78d362a-b1d3-4a8c-99ef-edb75188f842",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.40253977e-04, 2.40253977e-04, 9.99519492e-01],\n",
       "       [2.40253977e-04, 9.99519492e-01, 2.40253977e-04]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "C = softmax(B, axis=1)\n",
    "print(np.around(C,3))\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c02e42-1242-4266-976c-02dd150ad96c",
   "metadata": {},
   "source": [
    "Interpreting this result, we can see that the top query corresponds most to the third key, and the second query corresponds most to the second key.\n",
    "\n",
    "## Part 4: Values\n",
    "\n",
    "Finally, we get our result by multiplying the matrix $C$ with our values $V$, where each item in $V$ corresponds to an item in $K$, our key database. Our formula for this is:\n",
    "\n",
    "```{math}\n",
    "D = CV \\\\\n",
    "D = \\left[\\begin{array}{lll}\n",
    "0.0002 & 0.0002 & 0.999 \\\\\n",
    "0.0002 & 0.999 & 0.0002 \\\\\n",
    "\\end{array}\\right] \\left[\\begin{array}{lll}\n",
    "5 & 0 & 0 \\\\\n",
    "0 & 5 & 0 \\\\\n",
    "0 & 0 & 5 \\\\\n",
    "\\end{array}\\right] \\\\\n",
    "D \\approx \\left[\\begin{array}{lll}\n",
    "0 & 0 & 5 \\\\\n",
    "0 & 5 & 0\n",
    "\\end{array}\\right]\n",
    "```\n",
    "This completes the translation by relating the keys in our database to their translated counterparts. In code this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c3ee123-7aa1-48cb-9108-3b34cf5a6b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.20126988e-03, 1.20126988e-03, 4.99759746e+00],\n",
       "       [1.20126988e-03, 4.99759746e+00, 1.20126988e-03]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = C@V\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f73770-7078-42ff-89c6-d49a7ae0790d",
   "metadata": {},
   "source": [
    "Now that we have calculated the Attention mechanism for our queries, we match the result to the values which are most similar. For this example, these keys are $[0,0,5]$, corresponding to \"avec\", and $[0,5,0]$, corresponding to \"ci-dessous\". Checking with our definitions from above, these are correctly translated words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a2a97c-3b9a-484e-a160-63e0cb1aee2e",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "If we combine the 4 steps we just completed, we get the full definition for Attention, which is below:\n",
    ":::{prf:definition} Attention Definition\n",
    "$$ \\text{Attention}(Q, K, V) =  \\text{softmax}\\left(\\frac{QK^T}{d_k}\\right)V$$\n",
    "$Q$ is the query, $K$ are the keys, and $V$ are values, and $d_k$ is the dimension of the keys.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71414b22-3ff7-4271-9774-edac77eff327",
   "metadata": {},
   "source": [
    "# Looking at real words\n",
    "To do this, we will use a library `gensim` which provides a pretrained set of word embeddings. We will download word embeddings from twitter for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d0a6d1e-43c7-4301-a0a0-87f2ea533128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89469254-ab20-4190-a01c-f16e754dd21e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98231aa-d8ca-4163-895b-920847efa05f",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can look up words, like screen, television, and remote using the function calls below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47d64856-c65e-463d-a3c9-81578e06634e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Screen:  [ 0.31098   -0.53336    0.9123    -0.15256    0.89117   -0.10692\n",
      "  0.78658   -0.19013    0.93285    0.52754    0.31475    0.63839\n",
      " -3.4433    -0.65918    0.055317   1.3083     0.4009     0.0071616\n",
      " -0.27728    0.017544  -0.86985   -0.60072    1.3789     0.25096\n",
      " -1.4383   ]\n",
      "rock:  [ 0.021641 -0.54484   0.78102  -0.10052   0.10482   0.53319   1.0586\n",
      "  0.90491   0.2594   -0.73547  -0.12972  -0.59679  -3.4979   -0.61679\n",
      " -0.15259   0.072474 -0.45453  -0.14681   0.09392   0.32735  -0.68834\n",
      " -0.020972  0.21344  -0.63178   1.3292  ]\n",
      "Remote:  [ 0.1125    0.38049   0.40254   0.089511  0.58018   0.067418  0.41842\n",
      " -0.46138   1.8756    0.99621   0.19743   0.27248  -2.7099   -0.65497\n",
      "  0.54752   1.0199    0.58964   0.1559    0.93753  -0.28045  -0.8659\n",
      " -0.88299   0.85855  -0.39055  -1.0604  ]\n"
     ]
    }
   ],
   "source": [
    "screen = glove_vectors['screen']\n",
    "rock = glove_vectors['rock']\n",
    "remote = glove_vectors['remote']\n",
    "print('Screen: ',screen)\n",
    "print('rock: ',rock)\n",
    "print('Remote: ',remote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb723747-191d-4a79-b0da-c69b51703e19",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we can calculate the cosine similarity based on the formula:\n",
    "$$\n",
    "w\\cdot v = \\|w\\| \\|v\\| \\cos(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ab17641-8c27-4ebe-811f-3c6fdd66826c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5682209"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screen@rock/(np.linalg.norm(screen)*np.linalg.norm(rock))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47428b03-a649-4f7b-814d-8b44b729547d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4561119"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote@rock/(np.linalg.norm(remote)*np.linalg.norm(rock))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4dd8837-5a02-4203-9a08-4de2a2e52a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86155677"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screen@remote/(np.linalg.norm(screen)*np.linalg.norm(remote))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea3afe0-79e2-4ff4-80b9-e723b60dbc2d",
   "metadata": {},
   "source": [
    "Based on this dataset, we can see that screen and remote are the most similar, remote and rock are the least similar, and screen and rock are not very similar either. The library also provides the ability to search the database for the most similar words to a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28702a77-df2e-47de-b436-f5597a0236d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('keyboard', 0.9022769331932068),\n",
       " ('monitor', 0.9000157713890076),\n",
       " ('plug', 0.8990083932876587),\n",
       " ('engine', 0.8985315561294556),\n",
       " ('switch', 0.896084725856781),\n",
       " ('device', 0.888617753982544),\n",
       " ('charger', 0.8855589032173157),\n",
       " ('counter', 0.8843674063682556),\n",
       " ('automatic', 0.878197431564331),\n",
       " ('sync', 0.877406656742096)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors.most_similar('remote')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42e962c-2028-4f4f-9bc7-30c50d3d4283",
   "metadata": {},
   "source": [
    "The most similar words to 'remote' in this dataset are shown above. Dot products and cosine similarity are key components of technologies which underpin the latest innovations in Large Language Models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bb0100-8000-473e-9f10-842d63f020ca",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/02_Inner_Products_and_Norms/Case_Study_On_Transformers/048-transformer-case-study.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
