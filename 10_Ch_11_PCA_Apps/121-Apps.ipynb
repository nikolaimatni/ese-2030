{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 11.1 Applications\n",
    "subject:  PCA\n",
    "subtitle: \n",
    "short_title: 11.1 Applications\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: \n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/10_Ch_11_PCA_Apps/121-Apps.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 19 - Principal Component Analysis with Applications to Imaging and Data Compression.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\title{Principal Component Analysis with Applications to Image Processing and Statistics}\n",
    "\n",
    "\n",
    "We start with a motivating application from satellite imagery analysis. The Landsat satellites are a pair of imaging satellites that record images of terrain and coastlines. These satellites cover almost every square mile of the Earth's surface every 16 days.\n",
    "\n",
    "Satellite sensors acquire seven simultaneous images of any given region, with each sensor recording energy from separate wavelength bands: three in the visible light spectrum and four in the infrared and thermal bands.\n",
    "\n",
    "Each image is digitized and stored as a rectangular array of numbers, with each number representing the signal strength for that pixel. Each of the seven images is one channel of a \\textit{multispectral image}.\n",
    "\n",
    "The seven Landsat images of a given region typically contain a lot of redundant information, as some features will appear across most channels. However, other features, because of their color or temperature, may only appear in one or two channels. A goal of multispectral image processing is to find a way that combines all seven channels of information better than studying each image separately.\n",
    "\n",
    "One approach, called Principal Component Analysis (PCA), seeks to find a special linear combination of the data that finds a weighted combination of all seven images into just one or two images. Importantly, we want these one or two composite images to preserve as much of the original variance (features) as possible. In particular, features should be more visible in the composite images than any of the original individual ones.\n",
    "\n",
    "This idea, which we'll explore in detail today, is illustrated with some Landsat imagery taken over Portland Valley Nevada.\n",
    "\n",
    "\\begin{figure}[h]\n",
    "\\centering\n",
    "\\begin{subfigure}{.3\\textwidth}\n",
    "  \\includegraphics[width=\\linewidth]{spectral_band1.jpg}\n",
    "  \\caption{Spectral band 1: Visible blue}\n",
    "\\end{subfigure}\n",
    "\\begin{subfigure}{.3\\textwidth}\n",
    "  \\includegraphics[width=\\linewidth]{spectral_band4.jpg}\n",
    "  \\caption{Spectral band 4: Near infrared}\n",
    "\\end{subfigure}\n",
    "\\begin{subfigure}{.3\\textwidth}\n",
    "  \\includegraphics[width=\\linewidth]{spectral_band7.jpg}\n",
    "  \\caption{Spectral band 7: Mid-infrared}\n",
    "\\end{subfigure}\n",
    "\n",
    "\\begin{subfigure}{.3\\textwidth}\n",
    "  \\includegraphics[width=\\linewidth]{principal_component1.jpg}\n",
    "  \\caption{Principal component 1: 61.5\\%}\n",
    "\\end{subfigure}\n",
    "\\begin{subfigure}{.3\\textwidth}\n",
    "  \\includegraphics[width=\\linewidth]{principal_component2.jpg}\n",
    "  \\caption{Principal component 2: 5.3\\%}\n",
    "\\end{subfigure}\n",
    "\\begin{subfigure}{.3\\textwidth}\n",
    "  \\includegraphics[width=\\linewidth]{principal_component3.jpg}\n",
    "  \\caption{Principal component 3: 1.2\\%}\n",
    "\\end{subfigure}\n",
    "\\caption{Landsat imagery of Portland Valley Nevada}\n",
    "\\end{figure}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images from three Landsat spectral bands are shown in (a)-(c); their total information is ``projected'' into the three principal components in (d)-(f). The first component, (d), ``explains'' 93.5\\% of the scene features (or variance) found in the original data. In this way, we could compress all of the original data to the single image (d) with only a 6.5\\% loss of scene variance.\n",
    "\n",
    "PCA can in general be applied to any data that consists of lists of measurements made on a collection of objects or individuals, including data mining, machine learning, image processing, speech recognition, facial recognition, and health informatics. As we'll see next, the way in which these ``special combinations'' of measurements are computed are via the singular vectors of an \\textbf{observation matrix}.\n",
    "\n",
    "\\section*{Observation Matrix, Mean, and Covariance}\n",
    "\n",
    "Let $\\mathbf{x}_i \\in \\mathbb{R}^p$ denote an observation vector obtained from measurement $i$, and suppose that $i=1,\\ldots,N$ measurements are obtained. The observation matrix $X \\in \\mathbb{R}^{p \\times N}$ is a $p \\times N$ matrix with $i^{th}$ column equal to the $i^{th}$ measurement vector $\\mathbf{x}_i$:\n",
    "\n",
    "\\[\n",
    "X = [\\mathbf{x}_1 \\; \\mathbf{x}_2 \\; \\cdots \\; \\mathbf{x}_N] \\in \\mathbb{R}^{p \\times N}\n",
    "\\]\n",
    "\n",
    "\\textbf{Example:} Suppose that $\\mathbf{x}_i \\in \\mathbb{R}^2$ is a two dimensional data given by the weight and height of the $i^{th}$ student at Penn: $\\mathbf{x}_i = (w_i, h_i) \\in \\mathbb{R}^2$. Then if measurements are obtained from $N$ students, the observation matrix $X \\in \\mathbb{R}^{2 \\times N}$ has the form:\n",
    "\n",
    "\\[\n",
    "X = \\begin{bmatrix}\n",
    "w_1 & w_2 & \\cdots & w_N \\\\\n",
    "h_1 & h_2 & \\cdots & h_N\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "The set of observation vectors can be visualized as a two-dimensional scatter plot.\n",
    "\n",
    "\\begin{figure}[h]\n",
    "\\centering\n",
    "\\includegraphics[width=0.5\\textwidth]{scatter_plot.png}\n",
    "\\caption{A scatter plot of observation vectors $\\mathbf{x}_1,\\ldots,\\mathbf{x}_N$.}\n",
    "\\end{figure}\n",
    "\n",
    "\\textbf{Example:} The three images (a)-(c) above can be thought of as one image composed of three spectral components, as each image gives information about the same region. We can capture this mathematically by associating a vector in $\\mathbb{R}^3$ to each pixel (one coordinate of the vector) that lists the intensity for that pixel in the three spectral bands. Typically the image is 2000 $\\times$ 2000 pixels, so there are 4 million pixels in the image. The observation matrix for this data is a matrix with 3 rows and 4 million columns. The data can thus be visualized as a scatter plot in $\\mathbb{R}^3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualized as a scatter plot of 4 million points in $\\mathbb{R}^3$ (see Figure below for a synthetic example).\n",
    "\n",
    "\\begin{figure}[h]\n",
    "\\centering\n",
    "\\includegraphics[width=0.6\\textwidth]{spectral_scatter_plot.png}\n",
    "\\caption{A scatter plot of spectral data for a satellite image.}\n",
    "\\label{fig:spectral_scatter}\n",
    "\\end{figure}\n",
    "\n",
    "\\section*{Mean and Covariance}\n",
    "\n",
    "To understand PCA, we need to understand some basic concepts from statistics. We will review the mean and covariance of a set of observations. For our purposes, these will simply be things we can compute from the data, but you should be aware that these are well-motivated quantities from a statistical perspective. You will learn more about this in ESE 2010, STAT 4300 or ESE 5020.\n",
    "\n",
    "Let's start with an observation matrix $X \\in \\mathbb{R}^{p\\times N}$, with columns $\\mathbf{x}_1,\\ldots,\\mathbf{x}_N \\in \\mathbb{R}^p$. The sample mean $\\mathbf{m}$ of the observation vectors is given by\n",
    "\n",
    "\\[\n",
    "\\mathbf{m} = \\frac{1}{N}(\\mathbf{x}_1 + \\cdots + \\mathbf{x}_N) = \\frac{1}{N}\\sum_{j=1}^N \\mathbf{x}_j.\n",
    "\\]\n",
    "\n",
    "Another name for the sample mean is the centroid of the data, which we encountered when we learned about the k-means algorithm.\n",
    "\n",
    "Since PCA is interested in directions of (maximal) variation in our data, it makes sense to subtract off the mean $\\mathbf{m}$, as it captures the average behavior of our data set. To that end, define the centered observations to be\n",
    "\n",
    "\\[\n",
    "\\hat{\\mathbf{x}}_j = \\mathbf{x}_j - \\mathbf{m}, \\quad j=1,\\ldots,N,\n",
    "\\]\n",
    "\n",
    "and the centered or de-meaned observation matrix\n",
    "\n",
    "\\[\n",
    "\\hat{X} = [\\hat{\\mathbf{x}}_1 \\; \\hat{\\mathbf{x}}_2 \\; \\cdots \\; \\hat{\\mathbf{x}}_N].\n",
    "\\]\n",
    "\n",
    "For example, Fig. 3 below shows a centered version of the weight/height data illustrated in Fig. 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{figure}[h]\n",
    "\\centering\n",
    "\\includegraphics[width=0.4\\textwidth]{weight_height_scatter.png}\n",
    "\\caption{Weight-height data in mean-deviation form.}\n",
    "\\label{fig:weight_height_scatter}\n",
    "\\end{figure}\n",
    "\n",
    "Finally, we define the sample covariance matrix $S \\in \\mathbb{R}^{p\\times p}$ as\n",
    "\n",
    "\\[\n",
    "S = \\frac{1}{N} \\hat{X}\\hat{X}^T.\n",
    "\\]\n",
    "\n",
    "Since any matrix of the form $AA^T$ is positive semidefinite (can you see why?), so is S. Note sometimes $\\frac{1}{N-1}$ is used instead of $\\frac{1}{N}$ for statistical considerations beyond the scope of this course. (It leads to S being an unbiased estimator of the \"true\" covariance of the data). We will just use $\\frac{1}{N}$.\n",
    "\n",
    "\\textbf{Example:} Three measurements are made on each of four individuals in a random sample from a population. The observation vectors are:\n",
    "\n",
    "\\[\n",
    "\\mathbf{x}_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 4 \\end{pmatrix}, \\quad\n",
    "\\mathbf{x}_2 = \\begin{pmatrix} 2 \\\\ 5 \\\\ 7 \\end{pmatrix}, \\quad\n",
    "\\mathbf{x}_3 = \\begin{pmatrix} 7 \\\\ 8 \\\\ 9 \\end{pmatrix}, \\quad\n",
    "\\mathbf{x}_4 = \\begin{pmatrix} 6 \\\\ 2 \\\\ 5 \\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "The sample mean is $\\mathbf{m} = \\frac{1}{4}(\\mathbf{x}_1+\\mathbf{x}_2+\\mathbf{x}_3+\\mathbf{x}_4) = \\begin{pmatrix} 4 \\\\ 4 \\\\ 6.25 \\end{pmatrix}$.\n",
    "\n",
    "The centered observations $\\hat{\\mathbf{x}}_j = \\mathbf{x}_j - \\mathbf{m}$ are then\n",
    "\n",
    "\\[\n",
    "\\hat{\\mathbf{x}}_1 = \\begin{pmatrix} -3 \\\\ -3 \\\\ -2.25 \\end{pmatrix}, \\quad\n",
    "\\hat{\\mathbf{x}}_2 = \\begin{pmatrix} -2 \\\\ 1 \\\\ 0.75 \\end{pmatrix}, \\quad\n",
    "\\hat{\\mathbf{x}}_3 = \\begin{pmatrix} 3 \\\\ 4 \\\\ 2.75 \\end{pmatrix}, \\quad\n",
    "\\hat{\\mathbf{x}}_4 = \\begin{pmatrix} 2 \\\\ -2 \\\\ -1.25 \\end{pmatrix},\n",
    "\\]\n",
    "\n",
    "and the centered observation matrix is\n",
    "\n",
    "\\[\n",
    "\\hat{X} = \\begin{pmatrix}\n",
    "-3 & -2 & 3 & 2 \\\\\n",
    "-3 & 1 & 4 & -2 \\\\\n",
    "-2.25 & 0.75 & 2.75 & -1.25\n",
    "\\end{pmatrix}.\n",
    "\\]\n",
    "\n",
    "The sample covariance matrix is\n",
    "\n",
    "\\[\n",
    "S = \\frac{1}{4} \\hat{X}\\hat{X}^T = \\begin{pmatrix}\n",
    "7.5 & 4.5 & 0 \\\\\n",
    "4.5 & 6 & -6 \\\\\n",
    "0 & -6 & 24\n",
    "\\end{pmatrix}.\n",
    "\\]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be wondering what the entries $s_{ij}$ of the covariance matrix S mean. Let's take a bit of a closer look. We'll consider a small example where the observations $\\mathbf{x}_i \\in \\mathbb{R}^2$ are two dimensional, and assume we have $N=3$ observations. Let the first measurement be $a \\in \\mathbb{R}$ and the second be $b \\in \\mathbb{R}$, so that $\\mathbf{x}_i = (a_i, b_i) \\in \\mathbb{R}^2$ and the centered observation is $\\hat{\\mathbf{x}}_i = (\\hat{a}_i, \\hat{b}_i) \\in \\mathbb{R}^2$. Our centered observation matrix is then\n",
    "\n",
    "\\[\n",
    "\\hat{X} = [\\hat{a}_1 \\; \\hat{a}_2 \\; \\hat{a}_3] = [\\hat{\\mathbf{a}}^T]\n",
    "           [\\hat{b}_1 \\; \\hat{b}_2 \\; \\hat{b}_3]   [\\hat{\\mathbf{b}}^T],\n",
    "\\]\n",
    "\n",
    "where we defined $\\hat{\\mathbf{a}} = (\\hat{a}_1, \\hat{a}_2, \\hat{a}_3) \\in \\mathbb{R}^3$ and $\\hat{\\mathbf{b}} = (\\hat{b}_1, \\hat{b}_2, \\hat{b}_3)$ as the vectors in $\\mathbb{R}^3$ containing all of the centered first and second measurements, respectively.\n",
    "\n",
    "Then, we can write our sample covariance matrix as:\n",
    "\n",
    "\\[\n",
    "S = \\frac{1}{3} \\hat{X}\\hat{X}^T = \\frac{1}{3} [\\hat{\\mathbf{a}}^T] [\\hat{\\mathbf{a}} \\; \\hat{\\mathbf{b}}] = \n",
    "\\begin{bmatrix}\n",
    "\\|\\hat{\\mathbf{a}}\\|^2 & \\hat{\\mathbf{a}}^T\\hat{\\mathbf{b}} \\\\\n",
    "\\hat{\\mathbf{a}}^T\\hat{\\mathbf{b}} & \\|\\hat{\\mathbf{b}}\\|^2\n",
    "\\end{bmatrix}.\n",
    "\\]\n",
    "\n",
    "The diagonal entry $s_{11} = \\frac{\\|\\hat{\\mathbf{a}}\\|^2}{3}$ is called the variance of measurement 1.\n",
    "\n",
    "Expanding it out:\n",
    "\n",
    "\\[\n",
    "s_{11} = \\|\\hat{\\mathbf{a}}\\|^2 = \\frac{1}{3}(\\hat{a}_1^2 + \\hat{a}_2^2 + \\hat{a}_3^2)\n",
    "= \\frac{1}{3}((a_1-m_1)^2 + (a_2-m_1)^2 + (a_3-m_1)^2)\n",
    "\\]\n",
    "\n",
    "We see that $s_{11}$ captures how much the first measurement $a_i$ deviates from its mean value $m_1$, on average, i.e., it measures how much $a_i$ varies relative to its mean. Similarly, $s_{22} = \\|\\hat{\\mathbf{b}}\\|^2$ is the variance of measurement 2.\n",
    "\n",
    "Now let's look at the off-diagonal term $s_{12} = s_{21} = \\frac{\\hat{\\mathbf{a}}^T\\hat{\\mathbf{b}}}{3}$. Recall from our work on inner products that $\\hat{\\mathbf{a}}^T\\hat{\\mathbf{b}} = \\|\\hat{\\mathbf{a}}\\| \\|\\hat{\\mathbf{b}}\\| \\cos \\theta$, where $\\theta$ is the angle between $\\hat{\\mathbf{a}}$ and $\\hat{\\mathbf{b}}$. We can view\n",
    "\n",
    "\\[\n",
    "\\cos \\theta = \\frac{\\hat{\\mathbf{a}}^T\\hat{\\mathbf{b}}}{\\|\\hat{\\mathbf{a}}\\| \\|\\hat{\\mathbf{b}}\\|}\n",
    "\\]\n",
    "\n",
    "as a measure of how well aligned, or correlated, $\\hat{\\mathbf{a}}$ and $\\hat{\\mathbf{b}}$ are. If $\\hat{\\mathbf{a}}$ and $\\hat{\\mathbf{b}}$ are parallel, $\\cos \\theta = 1$ or $-1$, and if $\\hat{\\mathbf{a}}$ and $\\hat{\\mathbf{b}}$ are perpendicular, $\\cos \\theta = 0$. This lets us interpret $s_{12} = \\frac{\\hat{\\mathbf{a}}^T\\hat{\\mathbf{b}}}{3}$, which is proportional to $\\cos \\theta$, as a measure of how similarly $\\hat{\\mathbf{a}}$ and $\\hat{\\mathbf{b}}$ behave. If $\\hat{\\mathbf{a}}^T\\hat{\\mathbf{b}}$ is positive, this means $\\hat{\\mathbf{a}}$ and $\\hat{\\mathbf{b}}$ tend to move up or down together; if it is negative they tend to move in opposite directions; and if it is small (or zero), $\\hat{\\mathbf{a}}$ and $\\hat{\\mathbf{b}}$ tend to move independently of each other. Since $s_{12}$ captures how the 1st and 2nd measurements vary with each other, it is called their covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\textbf{Finally, always we worked out the concepts for $x_i \\in \\mathbb{R}^p$ and $j=1,2,3,\\ldots$. These concepts extend naturally to the general setting:}\n",
    "\n",
    "\\begin{itemize}\n",
    "    \\item $S_{ij} = \\text{variance of measurement } i \\text{ across measurements } j=1,\\ldots,N$\n",
    "    \\item $S_{kl} = \\text{covariance of measurements } k \\text{ and } l \\text{ across measurements } j=1,\\ldots,N$\n",
    "\\end{itemize}\n",
    "\n",
    "\\textbf{ONLINE NOTES:} Please include examples of correlated, anticorrelated, and uncorrelated vectors, e.g., Fig 3.8 from VMLS.\n",
    "\n",
    "\\textbf{Principal Component Analysis}\n",
    "\n",
    "To make our notation a little bit cleaner, we'll assume that our observations $x_i \\in \\mathbb{R}^p$ and observation matrix $X \\in \\mathbb{R}^{N \\times p}$ have already been centered so that $\\bar{x} = 0$ and $\\bar{e} = 0$.\n",
    "\n",
    "The goal of PCA is to find a new orthogonal basis for $\\mathbb{R}^p$ defined by the orthogonal $p \\times p$ matrix $P = [u_1, \\ldots, u_p]$, for $u_1, \\ldots, u_p$ an orthonormal basis of $\\mathbb{R}^p$:\n",
    "\n",
    "\\[x = Py = y_1u_1 + y_2u_2 + \\cdots + y_pu_p\\]\n",
    "\n",
    "with the property that the new coordinates $y_1, \\ldots, y_p$ are uncorrelated (i.e., have covariance 0) and arranged in decreasing order of variance. The matrix $P$ is orthogonal, and hence $y = P^T x = P^{-1} x$ provides a decomposition of the measurement $x$ along the directions $u_1, \\ldots, u_p$, where most of the variance in $x$ can be found in the direction $u_1$, 2nd most in direction $u_2$, etc.\n",
    "\n",
    "What does the covariance matrix of the new variables $y$ look like? Note that if $y_i = P^T x_i$, then $Y = P^T X$ is the observation matrix in our new basis, so let\n",
    "\n",
    "\\[S_y = YY^T = P^T X X^T P = P^T S_x P.\\]\n",
    "\n",
    "If the change of basis $x = Py$ is such that the $y_i$ are uncorrelated, the $S_y$, the covariance matrix of the observations $y$, should be diagonal. Our goal is therefore to pick an orthonormal basis $u_1, \\ldots, u_p$ so that for $P = [u_1, \\ldots, u_p]$,\n",
    "\n",
    "\\[S_y = P^T S_x P\\]\n",
    "\n",
    "is diagonal. (But we already know how to do this!) $S_x$ is a symmetric matrix, and thus admits a spectral decomposition $S_x = Q \\Lambda Q^T$, where $Q = [u_1, \\ldots, u_p]$ is an orthogonal matrix composed of the orthonormal eigenvectors of $S_x$, and $\\Lambda = \\text{diag}(\\lambda_1, \\ldots, \\lambda_p)$ a diagonal matrix of its eigenvalues. Hence, setting $P = Q$, we have\n",
    "\n",
    "\\[S_y = Q^T S_x Q = Q^T Q \\Lambda Q^T Q = \\Lambda\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is diagonal, as we wanted!\n",
    "\n",
    "The orthonormal eigenvectors $u_1, \\ldots, u_p$ of the covariance matrix $S_x$ are called the \\textbf{principal components} of the data in the observation matrix $X$: the \\textbf{first principal component} is the eigenvector $u_1$ corresponding to the largest eigenvalue $\\lambda_1$, etc.\n",
    "\n",
    "The first principal component $u_1$ determines the new variable $y_1$ by projecting the original observation $x$ along $u_1$. In particular, since $u_1$ is the first column of $P$, $u_1^T$ is the first row of $P^T$, and hence\n",
    "\n",
    "\\[y_1 = u_1^T x,\\]\n",
    "\n",
    "which is a projection of $x$ along the unit vector $u_1$. In a similar fashion, $u_2$ determines $y_2$, and so on. The direction $u_1$ is aligned with the direction of maximal variance in the data, so we expect \"most of\" each observation $x$ to be aligned with $u_1$.\n",
    "\n",
    "We'll see how to use this observation to compress multivariate data via dimensionality reduction next, but first, let's take a look at a simple example:\n",
    "\n",
    "\\textbf{Example:} The initial data for our landsat imaging sample was composed of 4 million observations $x_i \\in \\mathbb{R}^3$. The associated covariance matrix (after centering the data) is:\n",
    "\n",
    "\\[S_x = \\begin{pmatrix}\n",
    "2394.98 & 2411.94 & 2138.80 \\\\\n",
    "2411.94 & 3066.42 & 2373.40 \\\\\n",
    "2138.80 & 2373.40 & 2650.71\n",
    "\\end{pmatrix}\\]\n",
    "\n",
    "The eigenvalue/vectors of $S_x$ are:\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_1 &= 7614.33, u_1 = \\begin{pmatrix} .5491 \\\\ .6295 \\\\ .5510 \\end{pmatrix} & \\lambda_2 = 427.63, u_2 = \\begin{pmatrix} -.8299 \\\\ .3026 \\\\ .8179 \\end{pmatrix} & \\lambda_3 = 98.10, u_3 = \\begin{pmatrix} .0834 \\\\ -.7157 \\\\ .1441 \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Up to 3 decimal places, the first principal component is:\n",
    "\n",
    "\\[y_1 = .549x_1 + .630x_2 + .551x_3\\]\n",
    "\n",
    "This equation was used to create image (d) by taking a weighted combination of the three spectral band measurements (each suitably centered to gray scale so they too can be combined).\n",
    "\n",
    "The covariance of the transformed observations $y = (y_1, y_2, y_3)$ is the diagonal matrix\n",
    "\n",
    "\\[\\Lambda = \\text{diag}(7614.33, 427.63, 98.10)\\]\n",
    "\n",
    "How can we use the fact that $\\lambda_1 = 7614.33$ is much larger than $\\lambda_2 = 427.63$ and $\\lambda_3 = 98.10$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\textbf{Compression via Dimensionality Reduction}\n",
    "\n",
    "PCA is very useful for applications in which most of the variation of the data lies in a low-dimensional subspace, i.e., can be explained by only a few of the new variables $y_1, \\ldots, y_p$.\n",
    "\n",
    "The total variance in the original data can be measured by summing together the variances of the original observation variables, i.e., by computing the sum of the diagonal entries of $S_x$:\n",
    "\n",
    "\\[T\\text{Var}(X) = s_{11} + s_{22} + \\cdots + s_{pp}.\\]\n",
    "\n",
    "Our first observation is that variance is preserved by the change of variables $y = P^T x$. We have argued this on occasion, but the intuition is that since the change of basis matrix $P$ is orthogonal, it only rotates/flips vectors, and does not stretch or squish them. This means that\n",
    "\n",
    "\\[T\\text{Var}(y) = \\gamma_1 + \\cdots + \\gamma_p = s_{11} + \\cdots + s_{pp} = T\\text{Var}(X),\\]\n",
    "\n",
    "where $\\gamma_i$ is the variance in the $y_i$ coordinate. Thus the ratio $\\gamma_i/T\\text{Var}(y)$ measures the fraction of the total variance \"explained\" by $y_i$.\n",
    "\n",
    "This suggests that if we are interested in compressing the original data, a strategy could be to discard the directions $u_i$ for which $\\gamma_i/T\\text{Var}(y)$ is very small, as these directions do not capture much of the variation in the data.\n",
    "\n",
    "\\textbf{Example:} The total variance of the Landsat data is\n",
    "\n",
    "\\[T\\text{Var}(X) = T\\text{Var}(y) = \\lambda_1 + \\lambda_2 + \\lambda_3 = 7614.33 + 427.63 + 98.10 = 8139.96.\\]\n",
    "\n",
    "The percentages of total variance explained by the principal components are:\n",
    "\n",
    "\\[\\frac{7614.33}{8139.96} = .935, \\quad \\frac{427.63}{8139.96} = .053, \\quad \\frac{98.10}{8139.96} = .012.\\]\n",
    "\n",
    "Thus, 93.5\\% of the information collected by Landsat for this specific image is captured in photograph (d). If that is sufficiently accurate for our purposes, we could compress the three images (a)-(c) into only image (d), reducing our memory requirements by 1/3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\textbf{Computing Principal Components and the SVD}\n",
    "\n",
    "The singular value decomposition is the main tool for performing PCA in practice. Suppose $X$ is our centred observation matrix, and then define\n",
    "\n",
    "\\[A = \\frac{X^T}{\\sqrt{N}} \\in \\mathbb{R}^{p \\times N}.\\]\n",
    "\n",
    "We assume that $\\text{rank}(A) = p$, i.e., our $p$-dimensional measurements $x_1, \\ldots, x_N \\in \\mathbb{R}^p$ span $\\mathbb{R}^p$.\n",
    "\n",
    "Then $A^T A = S_x$ is the covariance matrix of our data. Now, let $A = U \\Sigma V^T$ be a singular value decomposition for $A$: since $\\text{rank}(A) = p$, $\\Sigma \\in \\mathbb{R}^{p \\times p}$, $V \\in \\mathbb{R}^{N \\times p}$, and $U \\in \\mathbb{R}^{p \\times p}$. Then\n",
    "\n",
    "\\[S_x = A^T A = V \\Sigma U^T U \\Sigma V^T = V \\Sigma^2 V^T \\quad (= Q \\Lambda Q^T)\\]\n",
    "\n",
    "i.e., $V \\in \\mathbb{R}^{p \\times p}$ orthogonally diagonalizes the symmetric matrix $S_x$, and defines a spectral factorization of $S_x$. This allows us to immediately conclude that:\n",
    "\n",
    "1) The right singular vectors $v_1, \\ldots, v_p$, which are the orthonormal columns of $V$, are the eigenvectors of $S_x$, and hence are the principal components of our data,\n",
    "2) The squares of the singular values of $A$, $\\sigma_i^2$, are the $p$ eigenvalues of $S_x$.\n",
    "\n",
    "In practice, computing a SVD of $A$ is both faster and more accurate than computing an eigendecomposition of $S_x$, and is particularly true when the observation vector dimension $p$ is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the Missing Entries?\n",
    "\n",
    "Suppose that I run a web streaming service for movies for two of my friends, Amy, Bob, and Carol. It's a very special web movie service for the sci-fi movie lovers. The Movies: Inception, Star Wars Episode 1, Moana, and Inside Out. After 1 month, we ask our friends Amy, Bob, and Carol to rate the movies they've watched. Here are the results. We collect their ratings into a table below (we only allowed movies with 1-5):\n",
    "\n",
    "\\begin{tabular}{|c|c|c|c|c|c|}\n",
    "\\hline\n",
    " & The Matrix & Inception & Star Wars Ep.1 & Moana & Inside Out \\\\\n",
    "\\hline\n",
    "Amy & 9 & ? & ? & ? & 5 \\\\\n",
    "Bob & ? & 3 & ? & ? & 2 \\\\\n",
    "Carol & ? & ? & 2 & 1 & 1 \\\\\n",
    "\\hline\n",
    "\\end{tabular}\n",
    "\n",
    "Carol was asked to provide recommendations to Amy, Bob, and Carol as to which movie they should watch next. Said another way, we are asked to fill in the unknown ? ratings in the table above.\n",
    "\n",
    "This seems a bit unfair! Each of the unknown entries could be any value in 1-5 after all! But what if I told you an additional hint: Amy, Bob, and Carol have the same relative preferences for each movie. For example, Amy likes Inside Out 5 more than Bob likes Inside Out, and this ratio is the same across all movies. Furthermore, we are making the assumption that all columns of the table have a.e. multiples of each other.\n",
    "\n",
    "Thus we can conclude that Bob likes the Matrix $\\frac{9}{5} \\cdot (Amy's rating) = 4$. Similarly, Carol's rating of Inception is $\\frac{3}{2} \\cdot (Bob's rating) = 1.5$, Carol's rating of Inside Out is $\\frac{5}{2} \\cdot (Bob's rating) = 1$, and so on. Here's the completed matrix:\n",
    "\n",
    "\\[\n",
    "M = \\begin{bmatrix}\n",
    "9 & 7.5 & 10 & 5 & 5 \\\\\n",
    "4 & 3 & 4 & 2 & 2 \\\\\n",
    "1.8 & 1.5 & 2 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "The point of this example is that when you know something about the structure of a partially known matrix, then sometimes it is possible to intelligently fill in missing entries. In this perhaps example, the assumption that every column is a scalar multiple of every other column (one column ($\\propto$) every other column) is pretty extreme!\n",
    "\n",
    "One natural and useful definition is that assuming a matrix $M$ has low-rank, that rank counts as \"how\" it approximates dependency but it typically means that for a matrix $M \\in \\mathbb{R}^{m \\times n}$, that rank $M \\ll \\min\\{m,n\\}$.\n",
    "\n",
    "This lecture will explore how we can use this idea of structure to solve the matrix completion problem by finding the best low-rank approximation to a partially known matrix. The SVD will of course be our main tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/10_Ch_11_PCA_Apps/121-Apps.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
