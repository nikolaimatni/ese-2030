{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 6.4 The QR Algorithm for Eigenvalues\n",
    "subject:  Eigenvalues\n",
    "subtitle: \n",
    "short_title: 6.4 The QR Algorithm for Eigenvalues\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: Eigenvalues, Eigenvectors\n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath-/05_Ch_6_Eigenvalues_and_Eigenvectors/074-qr_algorithm.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 11 - Eigvenvalues and Eigenvectors part 1 (dynamical systems, determinants, basic definitions and computations).pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be found in ALA 8.1.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- the definition of similar matrices,\n",
    "- the definition of the Schur decomposition of a matrix,\n",
    "- how to use the QR algorithm to find the eigenvalues of a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/03_Orthogonality/053-orthogonal_matrices.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Motivations for the QR Algorithm\n",
    "\n",
    "Before we introduce the QR algorithm, we introduce a few motivating definitions.\n",
    "\n",
    "## Similar Matrices\n",
    "\n",
    "First, we introduce the notion of *similar matrices*:\n",
    "\n",
    ":::{prf:definition} Similar matrices\n",
    ":label: similar-matrices-defn\n",
    "\n",
    "We say matrices $n\\times n$ matrices $A$ and $B$ are similar if $B = P^{-1}AP$ for some invertible matrix $P$.\n",
    ":::\n",
    "\n",
    "A key property of similar matrices is that they have the same eigenvalues:\n",
    "\n",
    ":::{prf:theorem} Eigenvalues of similar matrices\n",
    ":label: similar-matrices-thm\n",
    "\n",
    "Suppose $A$ and $B$ are similar. Then, $\\lambda$ is an eigenvalue of $A$ if and only if $\\lambda$ is an eigenvalue of $B$.\n",
    ":::\n",
    "\n",
    "To see why this is true, suppose that $\\lambda$ is an eigenvalue of $B$, i.e., $B\\vv v = \\lambda \\vv v$ for some nonzero $\\vv v$. Then, \n",
    "\n",
    "\\begin{align*}\n",
    "    B \\vv v = \\lambda \\vv v &\\implies (P^{-1}AP) \\vv v = \\lambda \\vv v\\\\\n",
    "    &\\implies A(P \\vv v) = P (\\lambda \\vv v) = \\lambda (P\\vv v)\n",
    "\\end{align*}\n",
    "\n",
    "meaning that $P\\vv v$ is an eigenvector of $A$, with the same eigenvalue of $\\lambda$. \n",
    "\n",
    "This motivates the following method: if we want to find the eigenvalues of $B$, we could try to write it in the form $A = P^{-1}BP$, where $B$ is a matrix for which it is easy to find the eigenvalues! This is exactly the motivation for the QR algorithm, which is an iterative algorithm for finding the *Schur decomposition* of $A$.\n",
    "\n",
    "We will revisit similar matrices in more detail a few sections down the line.\n",
    "\n",
    "## The Schur Decomposition\n",
    "\n",
    "Next, we define the Schur decomposition. The initial definition we will give of the Schur decomposition is for complex matrices. Later, we'll see how to turn this into a more useful theorem for real matrices.\n",
    "\n",
    ":::{prf:theorem} The complex Schur decomposition\n",
    ":label: schur-decomposition-thm\n",
    "\n",
    "Every complex square matrix $A$ can be written in the form:\n",
    "\n",
    "\\begin{align*}\n",
    "    A = Q^* U Q.\n",
    "\\end{align*}\n",
    "\n",
    "where $Q$ is a complex *unitary* matrix and $U$ is a complex upper triangular matrix. This is known as a *complex Schur decomposition* of $A$, and in general is not unique.\n",
    "\n",
    "Here, for a complex matrix $Q$, the *conjugate transpose* $Q^*$ is the matrix obtained by first transposing $Q$, then taking the complex conjugate of each of the resulting entries. This can be seen as a generalization of transposition for complex matrices, and in the special case that $Q$ is real, we have $Q^* = Q^\\top$.\n",
    "\n",
    "If $Q^{-1} = Q^*$, then we say that $Q$ is *unitary*. This can be seen as a generalization of orthogonal matrices for complex matrices.\n",
    ":::\n",
    "\n",
    "It is not immediately obvious why every matrix has a Schur decomposition, but a good proof can be found [here](https://en.wikipedia.org/wiki/Schur_decomposition#Proof). The essence of the proof is to show that, given a complex square matrix $A$, one can always construct a sequence of conjugation operations by unitary matrices (i.e., left multiply by $Q^*$ and right multiply by $Q$) to transform $A$ into an upper triangular matrix.\n",
    "\n",
    "Next, we show that given a Schur decomposition $A = Q^\\top U Q$, it's easy to find the eigenvalues of $A$.\n",
    "\n",
    "## The Eigenvalues of a Triangular Matrix\n",
    "\n",
    "It turns out that it's easy to find the eigenvalues of any triangular square matrix; just look at the diagonal entries!\n",
    "\n",
    ":::{prf:theorem} The eigenvalues of a triangular matrix\n",
    ":label: triangular-eigen-thm\n",
    "\n",
    "Let $U$ be a $n\\times n$ triangular matrix. Then, $U$ has eigenvalues $u_{ii}$, for $i = 1, ..., n$. These are the diagonal entries of $U$.\n",
    ":::\n",
    "\n",
    "To see why this is the case, we'll show, for am $n\\times n$ triangular matrix $U$, that $\\det (U - \\lambda I) = 0$ if and only if $\\lambda$ is on the diagonal of $U$:\n",
    "\n",
    "* If $U$ is on the diagonal of $U$, then $U - \\lambda I$ has a zero on its diagonal. Convince yourself that this implies that the row echelon form of $U$ can't have $n$ pivots, meaning that $U$ is singular (noninvertible).\n",
    "\n",
    "* If $\\lambda$ isn't on the diagonal of $U$, then $U - \\lambda I$ has all nonzero diagonal elements. It thus follows that $U - \\lambda I$ has $n$ pivots, and therefore is nonsingular (invertible).\n",
    "\n",
    "We conclude that the eigenvalues of $U$ are exactly its diagonal elements.\n",
    "\n",
    "# The QR Algorithm\n",
    "\n",
    "Equipped with these facts, the motivation behind the QR algorithm is as follows. \n",
    "\n",
    "* First, we find an *approximation* to a Schur decomposition $A = Q^\\top U Q$.\n",
    "\n",
    "* The eigenvalues of $A$ are thus the diagonal entries of $U$.\n",
    "\n",
    "## Approximating a Schur Decomposition\n",
    "\n",
    "The question thus becomes, how do we find an approximate Schur decomposition for $A$? The core idea of the QR algorithm is to use an iterative procedure, which, under certain conditions, converges to a Schur decomposition of $A$.\n",
    "\n",
    "* First, we set $A_1 = A$.\n",
    "\n",
    "* For $t = 1, 2, \\dots$, let $Q_t R_t = A_t$, with $Q_t$ orthogonal and $R_t$ upper triangular, and define $A_{t + 1} = R_t Q_t = Q_t^\\top A_t Q_t$. In other words, we compute a QR-factorization of $A_t$, and reverse the factors to get $A_{t + 1}$.\n",
    "\n",
    "We will not prove this here, but this procedure converges (in the sense that $A_t$ converges to an upper triangular matrix as $t\\to \\infty$) under the assumption that the absolute eigenvalues of $A$ are all distinct. We note that there are more advanced procedures which can get around this assumption, but this basic form of the algorithm highlights the most important ideas.\n",
    "\n",
    "To gain some intuition as to why we would hope this procedure to converge to a Schur decomposition of $A$, suppose that $B$ is a fixed point of this procedure, i.e., applying the QR factorization and switching the factors does not change $B$. That is, if $B$ has the QR factorization $B = QR$, then $QR = RQ$, i.e., $Q$ and $R$ commute. In general, this is \"uncommon\"; one such possibility is that $Q = I$, in which case we will have $B = R$. Next, note that for each $t = 1, 2, \\dots$ we have that $A_{t}$ is similar to $A_{t + 1}$ because $A_{t + 1} = Q^\\top A_{t}Q$ for an orthogonal matrix $Q$; convince yourself that this implies that each $A_t$ is similar to $A$. -->\n",
    "\n",
    "\n",
    "# Motivations\n",
    "\n",
    "Before we introduce the QR algorithm, we introduce a few motivating definitions.\n",
    "\n",
    "## Similar Matrices\n",
    "\n",
    "First, we introduce the notion of *similar matrices*:\n",
    "\n",
    ":::{prf:definition} Similar matrices\n",
    ":label: similar-matrices-defn\n",
    "\n",
    "We say matrices $n\\times n$ matrices $A$ and $B$ are similar if $B = P^{-1}AP$ for some invertible matrix $P$.\n",
    ":::\n",
    "\n",
    "A key property of similar matrices is that they have the same eigenvalues:\n",
    "\n",
    ":::{prf:theorem} Eigenvalues of similar matrices\n",
    ":label: similar-matrices-thm\n",
    "\n",
    "Suppose $A$ and $B$ are similar. Then, $\\lambda$ is an eigenvalue of $A$ if and only if $\\lambda$ is an eigenvalue of $B$.\n",
    ":::\n",
    "\n",
    "To see why this is true, suppose that $\\lambda$ is an eigenvalue of $B$, i.e., $B\\vv v = \\lambda \\vv v$ for some nonzero $\\vv v$. Then, \n",
    "\n",
    "\\begin{align*}\n",
    "    B \\vv v = \\lambda \\vv v &\\implies (P^{-1}AP) \\vv v = \\lambda \\vv v\\\\\n",
    "    &\\implies A(P \\vv v) = P (\\lambda \\vv v) = \\lambda (P\\vv v)\n",
    "\\end{align*}\n",
    "\n",
    "meaning that $P\\vv v$ is an eigenvector of $A$, with the same eigenvalue of $\\lambda$. \n",
    "\n",
    "This motivates the following method: if we want to find the eigenvalues of $B$, we could try to write it in the form $A = P^{-1}BP$, where $B$ is a matrix for which it is easy to find the eigenvalues! This is exactly the motivation for the QR algorithm, which is an iterative algorithm for finding the *Schur decomposition* of $A$.\n",
    "\n",
    "We will revisit similar matrices in more detail a few sections down the line.\n",
    "\n",
    "## The (Real) Schur Decomposition\n",
    "\n",
    "Next, we define the Schur decomposition. The proof of this claim relies on material covered later in the course, so we won't prove it here.\n",
    "\n",
    ":::{prf:theorem} The real Schur decomposition\n",
    ":label: real-schur-decomposition-thm\n",
    "\n",
    "Every real square matrix $A$ can be written in the form:\n",
    "\n",
    "\\begin{align*}\n",
    "    A = Q^\\top U Q.\n",
    "\\end{align*}\n",
    "\n",
    "where $Q$ is a real orthogonal matrix and $U$ is a real quasi-upper triangular matrix. This is known as a *real Schur decomposition* of $A$, and in general is not unique.\n",
    "\n",
    "A quasi-upper triangular matrix is a special type of block upper triangular matrix, in which the diagonal blocks are all $1\\times 1$ or $2\\times 2$.\n",
    ":::\n",
    "\n",
    "Note that, given a real Schur decomposition $A = Q^\\top U Q$, it's easy to find the eigenvalues of $A$; we just find the eigenvalues of each diagonal block! To see why, recall [Determinant Fact 4](#determinant-properties-defn). Since $U$ is quasi-upper triangular, its diagonal blocks are all $1\\times 1$ or $2\\times 2$ matrices, which have easy to solve characteristic equations, so it's easy to find their eigenvalues.\n",
    "\n",
    "# The QR Algorithm\n",
    "\n",
    "Equipped with these facts, the motivation behind the QR algorithm is as follows. \n",
    "\n",
    "* First, we find an *approximation* to a Schur decomposition $A = Q^\\top U Q$ (where $Q$ is orthogonal, $U$ is quasi-upper triangular).\n",
    "\n",
    "* The eigenvalues of $A$ are thus the diagonal entries of $U$.\n",
    "\n",
    "## Approximating a Real Schur Decomposition\n",
    "\n",
    "The question thus becomes, how do we find an approximate Schur decomposition for $A$? The core idea of the QR algorithm is to use an iterative procedure, which, under certain conditions, converges to a real Schur decomposition of $A$.\n",
    "\n",
    "* First, we set $A_1 = A$.\n",
    "\n",
    "* For $t = 1, 2, \\dots$, let $Q_t R_t = A_t$, with $Q_t$ orthogonal and $R_t$ upper triangular, and define $A_{t + 1} = R_t Q_t = Q_t^\\top A_t Q_t$. In other words, we compute a QR-factorization of $A_t$, and reverse the factors to get $A_{t + 1}$.\n",
    "\n",
    "The proof is again out of the scope of this course, but this procedure converges (in the sense that $A_t$ converges to a quasi-upper triangular matrix as $t\\to \\infty$) under the assumption that the absolute eigenvalues of $A$ are all distinct. We note that there are more advanced procedures which can get around this assumption, but this basic form of the algorithm highlights the most important ideas.\n",
    "\n",
    "# Pseudocode for the QR Algorithm\n",
    "\n",
    "We are now ready to give the pseudocode for the QR Algorithm.\n",
    "\n",
    ":::{prf:algorithm} The QR Algorithm for Eigenvalues\n",
    ":label: qr-eigen-alg\n",
    "\n",
    "**Inputs** $n\\times n$ matrix $A$ with distinct absolute value eigenvalues $\\lambda_1, \\dots, \\lambda_n$; number of iterations $T$\n",
    "\n",
    "**Output** approximate values of to $\\lambda_1, \\dots, \\lambda_n$\n",
    "\n",
    "$A_1 \\gets A$\\\n",
    "**for** $t = 1$ to $T$:\\\n",
    "$\\quad$ $Q_t, R_t \\gets $ QR factorization of $A_t$\\\n",
    "$\\quad$ $A_{t + 1} \\gets R_t Q_t$ \\\n",
    "**return** the eigenvalues of $A_{T + 1}$\n",
    ":::\n",
    "\n",
    "# Python Implementations of the QR Algorithm\n",
    "\n",
    "We are now ready to use the QR Algorithm to find eigenvalues in Python. \n",
    "\n",
    "## From Scratch\n",
    "\n",
    "First, we'll give an implementation of the Schur decomposition from scratch. We won't reimplement a QR factorization algorithm, so if you're interested in seeing the implementation for that, it can be found [on this page (scroll down)](#qr-alg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21.36 -6.11  9.82  4.82 -2.86]\n",
      " [ 0.   -2.14  8.72  3.1  -6.6 ]\n",
      " [-0.   -5.12 -1.82  1.99 -0.84]\n",
      " [ 0.   -0.    0.    4.34 -0.32]\n",
      " [ 0.   -0.    0.    0.    1.26]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def schur_form(A, iters=100): # A is a square matrix\n",
    "    B = A\n",
    "    for i in range(iters):\n",
    "        Q, R = np.linalg.qr(B)\n",
    "        B = R @ Q\n",
    "    return B\n",
    "\n",
    "print(np.round(schur_form(np.array([\n",
    "    [2.0, 3.0, 1.0, 0.5, 4.0],\n",
    "    [4.0, 5.0, 7.0, 0.1, 1.0],\n",
    "    [5.0, 3.0, 6.0, 19.2, 9.0],\n",
    "    [1.0, 4.0, 1.0, 4.0, 7.0],\n",
    "    [3.0, 1.0, 6.0, 2.0, 6.0]\n",
    "])), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `numpy.linalg.eigvals`\n",
    "\n",
    "Next, we'll use the `numpy.linalg.eigvals` function, which returns the eigenvalues of a square matrix. Under the hood, `numpy.linalg.eigvals` is based off of a more advanced version of the basic QR algorithm we outlined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21.36+0.j   -1.98+6.68j -1.98-6.68j  1.26+0.j    4.34+0.j  ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.round(np.linalg.eigvals(np.array([\n",
    "    [2.0, 3.0, 1.0, 0.5, 4.0],\n",
    "    [4.0, 5.0, 7.0, 0.1, 1.0],\n",
    "    [5.0, 3.0, 6.0, 19.2, 9.0],\n",
    "    [1.0, 4.0, 1.0, 4.0, 7.0],\n",
    "    [3.0, 1.0, 6.0, 2.0, 6.0]\n",
    "])).T, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify for yourself that the real eigenvalues returned by `numpy.linalg.eigvals` are the diagonal $1\\times 1$ blocks of the Schur form of the input matrix, which we found above. You can also verify that the complex eigenvalues returned by `numpy.linalg.eigvals` are the eigenvalues of the $2\\times 2$ block diagonal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
