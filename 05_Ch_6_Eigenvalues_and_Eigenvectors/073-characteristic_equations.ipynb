{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 6.3 Characteristic Equations\n",
    "subject:  Eigenvalues\n",
    "subtitle: \n",
    "short_title: 6.3 Characteristic Equations\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: Eigenvalues, Eigenvectors\n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath-/05_Ch_6_Eigenvalues_and_Eigenvectors/073-characteristic_equations.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 11 - Eigvenvalues and Eigenvectors part 1 (dynamical systems, determinants, basic definitions and computations).pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be found in ALA 8.1.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- how to define the characteristic equation of a square matrix $A$,\n",
    "- how to solve the characteristic equation to find the eigenvalues of $A$,\n",
    "- that $A$ and $A^\\top$ have the same eigenvalues for square $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Characteristic Equation \n",
    "\n",
    "In the previous section, we mentioned that a square matrix $B$ is singular if and only if $\\det (B) = 0$. Combined with [this fact](#eigenvalue-thm), which states that a scalar $\\lambda$ is an eigenvalue of square matrix $A$ if and only if $A - \\lambda I$ was singular, this motivates the following characterization of eigenvalues:\n",
    "\n",
    ":::{prf:definition} Characteristic Equation\n",
    ":label: characteristic-equation-defn\n",
    "\n",
    "For a square matrix $A \\in \\mathbb{R}^{n \\times n}$, its *characteristic equation* is defined as the equation:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\det(A - \\lambda I)= 0\n",
    "\\end{align*}\n",
    "\n",
    "which has a scalar variable $\\lambda$. The solutions $\\lambda_i$ that satisfy the equation are eigenvalues of $A$ because of the following equivalence which we have mentioned previously\n",
    "\n",
    "\\begin{align*}\n",
    "    \\det(A - \\lambda_i I)= 0 \\Leftrightarrow A - \\lambda_i I \\text{ is singular } \\Leftrightarrow \\lambda_i \\text{ is an eigenvalue}\n",
    "\\end{align*}\n",
    "\n",
    "The characteristic equation is a $n^{\\text{th}}$-degree polynomial equation in terms of $\\lambda$. This can be seen by fully expanding the equation using the entries of $A$ (e.g., via the [Laplace expansion](#laplace-expansion-defn)).\n",
    ":::\n",
    "\n",
    "A key property which follows is that $A$ and $A^\\top$ have the same characteristic polynomial (see determinant [Fact 5](#determinant-properties-defn)) and hence the same eigenvalues. We emphasize that they do not, however, have the same eigenvectors!\n",
    "\n",
    "## Finding the Eigenvalues by Solving the Characteristic Equation\n",
    "\n",
    "We now have all the pieces needed to find the eigenvalues for $2\\times2$ matrices (which is all we will ever ask you to compute, unless there is a special structure). This leads to the following method for finding all eigenvalue/vector pairs for a square matrix $A$:\n",
    "\n",
    "* First, we find all solutions $\\lambda$ to the characteristic equation $\\det(A - \\lambda I)$ to get the eigenvalues.\n",
    "\n",
    "* Second, for each value of $\\lambda$, we find all solutions $\\vv v$ to the homogenous system $(A - \\lambda I)\\vv v = \\vv 0$ to get the eigenvectors for each eigenvalue.\n",
    "\n",
    "Let's look at a small example with a $2\\times 2$ matrix:\n",
    "\n",
    ":::{prf:example} Finding the eigenvalues and eigenvectors of a $2\\times 2$ matrix\n",
    ":label: eigen-ex2\n",
    "\n",
    "Let's find the eigenvalues and eigenvectors of the matrix $\\bm 3&1\\\\1&3 \\em$.\n",
    "\n",
    "First, let's solve the characteristic equation $\\det\\left(\\bm 3&1\\\\1&3 \\em - \\lambda I\\right) = 0$. We have:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\det\\left( \\bm 3&1\\\\1&3 \\em - \\lambda I \\right) = 0 &\\iff \\det\\left( \\bm 3 - \\lambda&1\\\\1&3 - \\lambda \\em \\right) = 0 \\\\\n",
    "    &\\iff (3 - \\lambda)(3 - \\lambda) - (1)(1) = 0\\\\\n",
    "    &\\iff \\lambda^2 - 6\\lambda + 8 = 0\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The second line follows from the determinant formula for $2\\times 2$ matrices. This is a quadratic function of $\\lambda$ and has roots $\\lambda_1 = 2, \\lambda_2 = 4$, our eigenvalues.\n",
    "\n",
    "Second, let's find the eigenvectors corresponding to each eigenvalue. Just like we did [two sections ago](#eigen-ex1), we solve for the nullspaces of $\\bm 3&1\\\\1&3 \\em - 2I$ and $\\bm 3&1\\\\1&3 \\em - 4I$ to get the eigenvectors corresponding to $\\lambda_1$ and $\\lambda_2$, respectively. \n",
    "\n",
    "You should get that the eigenvectors corresponding to $\\lambda_1 = 2$ are:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{span} \\left\\{  \\bm -1\\\\ 1\\em \\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "and the eigenvectors corresponding to $\\lambda_2 = 4$ are:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{span} \\left\\{  \\bm 1\\\\ 1 \\em \\right\\}.\n",
    "\\end{align*}\n",
    "\n",
    "We typically only distinguish linearly independent eignevectors. In this case, we would say that $\\vv{v_1} = \\bm 1\\\\1 \\em$ is *the* eigenvector corresponding to $\\lambda_2 = 4$, although it is understood that any vector $\\vv{\\tilde{v_1}} = a \\vv{v_1}$, for $a \\neq 0$, is also a valid eigenvector.\n",
    "\n",
    ":::\n",
    "\n",
    "In general, the characteristic equation cannot easily be factored because there is no formula to exactly compute the roots of any polynomial of degree 5 or higher. One way to get around this is to use root-finding algorithms such as [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method), but these may converge slowly or have numerical issues. In the next section, we'll introduce an algorithm based on the [QR decomposition](#qr-alg) which alleviates these issues.\n",
    "\n",
    "## Optional: Finding the Characteristic Equation\n",
    "\n",
    "Here, we'll show how to find the characteristic equation, and then solve it for the eigenvalues, in Python. We stress that this method is not used in practice due to being slower than the QR algorithm (the most commonly used eigenvalue algorithm) and having numerical issues. Our aim here is to just demonstrate some applications of ideas we've covered previously in the course in Python.\n",
    "\n",
    "In the previous section, we showed how to compute the characteristic equation for small ($2\\times 2$ and some $3\\times 3$) matrices. A naive way to extend this to larger matrices is through the [Laplace expansion](#laplace-expansion-defn), a method for computing determinants; to get the characteristic equation, we could apply the Laplace expansion on $A - \\lambda I$ and keep track of the coefficients. However, this takes too long for larger matrices.\n",
    "\n",
    "However, instead of using the Laplace expansion, we can make a clever observation. Recall that the characteristic equation $\\det(A - \\lambda I)$, where $A \\in \\mathbb{R}^{n\\times n}$ is a $n$-degree polynomial in $\\lambda$, i.e., $\\det(A - \\lambda I) = c_0 + c_1\\lambda + c_2\\lambda^2 + \\dots + c_n\\lambda^n$ for some unknown coefficients $c_0, c_1, \\dots, c_n$.\n",
    "\n",
    "Suppose we pick $n + 1$ distinct values $l_1, l_2, \\dots l_{n + 1} \\in \\mathbb{R}$, and evaluate $\\det(A - \\lambda l_i)$ for each $i = 1, \\dots, n + 1$. Based on our observation above, we know:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\det(A - l_1 I) &= c_0 + c_1 l_1 + c_2 l_1^2 + \\dots + c_n l_1^n\\\\\n",
    "    \\det(A - l_2 I) &= c_0 + c_1 l_2 + c_2 l_2^2 + \\dots + c_n l_2^n\\\\\n",
    "    &\\dots\\\\\n",
    "    \\det(A - l_{n + 1} I) &= c_0 + c_1 l_{n + 1} + c_2 l_{n + 1}^2 + \\dots + c_n l_{n + 1}^n\n",
    "\\end{align*}\n",
    "\n",
    "If we know the values of $l_1, l_2, \\dots l_{n + 1}$ and the matching values $\\det(A - l_1 I), \\det(A - l_2 I), \\dots, \\det(A - l_{n + 1} I)$, then this is actually a linear system with $n + 1$ equations and $n + 1$ unknowns in $c_0, c_1, \\dots, c_{n + 1}$! In this case, the coefficient matrix $\\bm 1 & l_1 & l_1^2 & \\dots & l_1^n \\\\ 1 & l_2 & l_2^2 & \\dots & l_2^n \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\1 & l_{n + 1} & l_{n + 1}^2 & \\dots & l_{n + 1}^n \\em$ is a special square matrix known as a [Vandermonde matrix](https://en.wikipedia.org/wiki/Vandermonde_matrix) and is nonsingular if and only if the $l_i$'s are all distinct. \n",
    "\n",
    "This method of finding the characteristic equation by solving a system of linear equations for the coefficients is an application of the more general method of [Lagrange interpolation](https://en.wikipedia.org/wiki/Lagrange_polynomial).\n",
    "\n",
    "#### Python Break!\n",
    "\n",
    "Below, we have python code constructing the characteristic equation via Lagrange interpolation, and then applying a root finding algorithm to get the eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients of characteristic equation (c_0, c_1, c_2, ..., c_n):\n",
      "[ 0.0418432  -0.01477132 -0.16053083 -0.22766626  0.41869727  0.14725493\n",
      "  1.69564907  1.29781919 -2.0174545  -4.63859515  1.        ] \n",
      "\n",
      "Eigenvalues (from solving characteristic equation):\n",
      "[ 4.97742109+0.j         -0.63759595+0.40955455j -0.63759595-0.40955455j\n",
      "  0.03608572+0.61160805j  0.03608572-0.61160805j -0.38836424+0.29676011j\n",
      " -0.38836424-0.29676011j  0.74401263+0.j          0.44845519+0.13529401j\n",
      "  0.44845519-0.13529401j] \n",
      "\n",
      "Eigenvalues (from np.linalg.eigvals):\n",
      "[ 4.97742109+0.j         -0.63759595+0.40955455j -0.63759595-0.40955455j\n",
      " -0.38836424+0.29676011j -0.38836424-0.29676011j  0.03608572+0.61160805j\n",
      "  0.03608572-0.61160805j  0.44845519+0.13529401j  0.44845519-0.13529401j\n",
      "  0.74401263+0.j        ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "np.random.seed(541)\n",
    "\n",
    "n = 10\n",
    "\n",
    "# Generate random 10x10 matrix\n",
    "A = np.random.rand(n, n)\n",
    "\n",
    "# Generate 11 values for which to evaluate the characteristic equation\n",
    "values = [i / n for i in range(n + 1)]\n",
    "dets = [np.linalg.det(A - value * np.identity(n)) for value in values]\n",
    "\n",
    "# Construct vandermonde matrix and solve linear system\n",
    "coeff_matrix = np.array([[values[i]**j for j in range(n + 1)] for i in range(n + 1)])\n",
    "characteristic_eqtn_coeffs = np.linalg.solve(coeff_matrix, dets)\n",
    "\n",
    "print('Coefficients of characteristic equation (c_0, c_1, c_2, ..., c_n):')\n",
    "print(characteristic_eqtn_coeffs, '\\n')\n",
    "\n",
    "# Flip the order; np.roots expects the coefficient corresponding to the highest degree to come first\n",
    "characteristic_eqtn_coeffs = np.flip(characteristic_eqtn_coeffs)\n",
    "\n",
    "print('Eigenvalues (from solving characteristic equation):')\n",
    "print(np.roots(characteristic_eqtn_coeffs), '\\n')\n",
    "\n",
    "print('Eigenvalues (from np.linalg.eigvals):')\n",
    "print(np.linalg.eigvals(A), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the eigenvalues obtained from solving the characteristic equation are the same as using the built in `numpy.linalg.eigvals` function, just in a different order. \n",
    "\n",
    "To end, we have a few more notes as to what's going on under the hood of `np.roots`. The function `np.roots`, given a list of coefficients for a polynomial, computes its roots. You can see on the [official documentation](https://numpy.org/doc/stable/reference/generated/numpy.roots.html) that `np.roots` actually works by finding the eigenvalues of a matrix defined using the coefficients of the polynomial!\n",
    "\n",
    "To convince yourself that there isn't any circular logic in here (i.e., do we seem to need an eigenvalue algorithm to find roots, and we need a root-finding algorithm to find eigenvalues?) we note that there exist other root finding algorithms, which are not based on eigenvalue problems. For example, you may have seen [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method) in a previous calculus course. A main reason why eigenvalue-based root-finding algorithms would be prefered to iterative proceedures such as Newton's method is because eigenvalues algorithms find all roots at once; whereas iterative proceedures only find a single root, and must be applied multiple times (this can lead to numerical issues and slow convergence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/03_Orthogonality/053-orthogonal_matrices.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
