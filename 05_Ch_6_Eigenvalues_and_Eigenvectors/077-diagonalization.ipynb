{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 6.7 Similarity, Eigenbases, and Diagonalization\n",
    "subject:  Eigenvalues\n",
    "subtitle: \n",
    "short_title: 6.7 Similarity, Eigenbases, and Diagonalization\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: Eigenvalues, Eigenvectors\n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath-/05_Ch_6_Eigenvalues_and_Eigenvectors/077-diagonalization.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 12 - Eigvenvalues and Eigenvectors part 2 (complex eigenvalues and eigenvectors, similarity transformation, diagonalization and eigenbases).pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be found in ALA 8.3.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- how to define eigenbases,\n",
    "- how to define similar matrices,\n",
    "- how to define the geometric and algebraic multiplicity of an eigenvalue,\n",
    "- when and how a square matrix $A$ can be diagonalized as $A = PDP^{-1}$, where $D$ is diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenbases\n",
    "\n",
    "Most of the vector space bases that are useful in applications are assembled from the eigenvectors of a particular matrix. In this section, we focus on matrices with a \"complete\" set of eigenvectors and show how these form a basis for $\\mathbb{R}^n$ (or in the complex case, $\\mathbb{C}^n$); in these cases, the set of their eigenvectors are known as eigenbases:\n",
    "\n",
    ":::{prf:definition} Eigenbasis\n",
    ":label: eigenbasis-defn\n",
    "\n",
    "Let $\\vv{v_1}, ..., \\vv{v_k}$ be the eigenvectors of a matrix $A \\in \\mathbb{R}^n$. If $ \\text{span} \\{ \\vv{v_1}, ..., \\vv{v_k} \\} = \\mathbb{R}^n$, i.e., the eigenvectors of $A$ span the entire space $\\mathbb{R}^n$, then its eigenvectors form an *eigenbasis* for $\\mathbb{R}^n$.\n",
    "\n",
    "If the eigenvectors of $A$ form a basis for $\\mathbb{R}^n$, then $A$ is *complete*.\n",
    ":::\n",
    "\n",
    "Such *eigenbases* allow us to rewrite the linear transformation determined by a matrix in a simple diagonal form; matrices what allow us to do this are called *diagonalizable*, a definition which we will formalize shortly. We focus on matrices with real eigenvalues and eigenvectors to start, and will return to matrices with complex eigenvalues/eigenvectors in a few pages.\n",
    "\n",
    "Our starting point is the following theorem, which we will state as a fact. It is a generalization of the pattern we saw in [an example before](#eigen-ex2) that the eigenvectors corresponding to distinct eigenvalues are linearly independent:\n",
    "\n",
    ":::{prf:theorem} The eigenvectors of a matrix with distinct eigenvalues\n",
    ":label: distinct-eigenvalue-thm\n",
    "\n",
    "If the matrix $A \\in \\mathbb{R}^{n\\times n}$ has $n$ distinct real eigenvalues $\\lambda_1, ..., \\lambda_n$, then the corresponding real eigenvectors $\\vv{v_1}, ..., \\vv{v_n}$ form a basis for $\\mathbb{R}^n$.\n",
    ":::\n",
    "\n",
    ":::{prf:example} The eigenbasis from a $2\\times 2$ matrix with distinct eigenvalues\n",
    ":label: eigen-ex6\n",
    "\n",
    "In a [previous example](#eigen-ex2), we saw that $A = \\bm 3&1\\\\1&3 \\em$ has eigenvalue/vector pairs\n",
    "\n",
    "\\begin{align*}\n",
    "    \\lambda_1 = 4, \\vv{v_1} = \\bm 1\\\\ 1\\em, \\quad \\lambda_2 = 2, \\vv{v_2} = \\bm -1\\\\1\\em\n",
    "\\end{align*}\n",
    "\n",
    "Here, $\\vv{v_1}$ and $\\vv{v_2}$ are linearly independent, and hence form a basis for $\\mathbb{R}^2$, since $\\dim \\mathbb{R}^2 = 2$.\n",
    ":::\n",
    "\n",
    "However, we also saw [an example](#eigen-ex3) where a $3\\times 3$ matrix only had two distinct eigenvalues, but still had three linearly independent eigenvectors:\n",
    "\n",
    ":::{prf:example} The eigenbasis from a $3\\times 3$ matrix a repeated eigenvalue\n",
    ":label: eigen-ex7\n",
    "\n",
    "Recall the $3\\times 3$ matrix $A = \\bm 2&-1&-1\\\\0&3&1\\\\0&1&3 \\em$. We showed it had the following eigenvalue/vector pairs:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\lambda_1 = 2&, \\quad \\vv{v_1} = \\bm 1\\\\0\\\\1\\em,\\quad \\vv{\\hat {v_1}} = \\bm 0\\\\1\\\\-1\\em\\\\\n",
    "    \\lambda_2 = 4&, \\quad \\vv{v_2} = \\bm -1\\\\1\\\\1\\em\n",
    "\\end{align*}\n",
    "\n",
    "The collection $\\vv{v_1}, \\vv{\\hat {v_1}}, \\vv{v_2} \\in \\mathbb{R}^3$ are linearly independent, and hence form a basis for $\\mathbb{R}^3$ since $\\dim \\mathbb{R}^3 = 3$.\n",
    ":::\n",
    "\n",
    "# Algebraic and Geometric Multiplicities\n",
    "\n",
    "Notice that in this last example $\\dim V_{\\lambda_1} = 2$ (why?) for the double eigenvalue $\\lambda_1 = 2$ (i.e., the [eigenspace](#eigenspace-defn) corresponding to $\\lambda_1$ had dimension of 2), and similarly, $\\dim V_{\\lambda_2} = 1$ for the simple eigenvalue $\\lambda_2 = 4$, so that there is a \"real\" eigenvector for each time an eigenvalue appears as a factor of the characteristic polynomial.\n",
    "\n",
    "These notions can be captured in the idea of algebric and geometric multiplicity:\n",
    "\n",
    ":::{prf:definition} Algebraic and Geometric Multiplicity\n",
    ":label: multiplicity-defn\n",
    "\n",
    "The number of times an eigenvalue $\\lambda_i$ appears as a solution to the characteristic polynomial is called its *algebraic multiplicity*. For example, if a matrix has characteristic polynomial $(x - 2)^2(x - 3) = 0$, then the eigenvalue $\\lambda_1 = 2$ has algebraic multiplicity 2, while $\\lambda_2 = 3$ has algebraic multiplicity 1.\n",
    "\n",
    "For an eigenvalue $\\lambda$ for a matrix $A$, its geometric multiplicity is the dimension of its [eigenspace](#eigenspace-defn) $V_\\lambda$.\n",
    "\n",
    "One key fact is that the geometric multiplicity of $\\lambda$ is always at most the algebraic multiplicity.\n",
    ":::\n",
    "\n",
    "Our observation is that if the algebraic and geometric multiplicity match for each eigenvalue, then we can form a basis for $\\mathbb{R}^n$.\n",
    "\n",
    ":::{prf:definition} Eigenbasis theorem\n",
    ":label: eigenbasis-existence-thm\n",
    "\n",
    "The eigenvectors of a matrix $A \\in \\mathbb{R}^{n\\times n}$ form a basis for $\\mathbb{R}^n$ if and only if, for each distinct eigenvalue $\\lambda_i$, the algebraic multiplicty of $\\lambda_i$ matches its geometric multiplicity $\\dim V_{\\lambda_i}$.\n",
    ":::\n",
    "\n",
    "For the next little bit, we will assume that our matrix $A$ satisfies the above theorem. What does this buy us? To answer this question, we need to introduce the idea of *similarity transformations*.\n",
    "\n",
    "# Similar Matrices\n",
    "\n",
    "Given a vector $\\vv x \\in\\mathbb{R}^n$ with coordinates $x_i$ with respect to the standard basis, i.e., $\\vv x = x_1 \\vv{e_1} + x_2 \\vv{e_2} + ... + x_n \\vv{e_n}$, we can find the coordinates $y_1,..., y_n$ of $\\vv x$ with respect to a new basis $\\vv{b_1}, ..., \\vv{b_n}$ by solving the following linear system:\n",
    "\n",
    "\\begin{align*}\n",
    "    y_1 \\vv{b_1} + y_2 \\vv{b_2} + ... + y_n \\vv{b_n} = \\vv x \\iff B \\vv y = \\vv x\n",
    "\\end{align*},\n",
    "\n",
    "where $V = \\bm \\vv{b_1} & \\vv{b_2} & ... & \\vv{b_n} \\em$. Since the $\\vv{b_i}$ form a basis of $\\mathbb{R}^n$, they are linearly independent, which means that $B$ is nonsingular.\n",
    "\n",
    "Now, suppose I have a matrix $A \\in \\mathbb{R}^{n\\times n}$, which I use to define the linear transformation $f : \\mathbb{R}^n \\to \\mathbb{R}^n$, given by a by $f(\\vv x) = A \\vv x$. Here the $f$'s inputs $\\vv x \\in \\mathbb{R}^n$ and outputs $f(\\vv x) \\in \\mathbb{R}^n$ are both expressed with the standard basis $\\vv{e_1}, ..., \\vv{e_n}$, and its matrix representative is $A$.\n",
    "\n",
    "What if we would like to implement this linear transformation with respect to the basis $B$, that is, define a function $g : \\mathbb{R}^n\\to \\mathbb{R}^n$ with inputs $\\vv y\\in \\mathbb{R}^n$ in $B$-coordinates, and outputs $g(\\vv y) \\in \\mathbb{R}^n$ in $B$-coordinates? To accomplish this, we need to convert both input $\\vv{x}$ and output $f(\\vv x)$ to $B$-coordinates.\n",
    "\n",
    "* Relating inputs $\\vv x$ to $B$-coordinate inputs $\\vv y$ is easy: $\\vv x = B\\vv y$. \n",
    "\n",
    "* Relating outputs $f(\\vv x)$ to $B$-coordinate outputs $g(\\vv y)$ is easy too: $f(\\vv x) = Bg(\\vv y)$.\n",
    "\n",
    "Putting these together, we see that\n",
    "\n",
    "\\begin{align*}\n",
    "    f(\\vv x) = A\\vv x \\iff Bg(\\vv y) = AB \\vv y\n",
    "\\end{align*}\n",
    "\n",
    "which lets us solve for $g(\\vv y) = B^{-1} A B \\vv y$.\n",
    "\n",
    "We conclude that if $A$ is the matrix representation of a linear transformation in the standard basis, then $B^{-1} A B$ is the matrix representation in the basis $B$.\n",
    "\n",
    "![alt text](../figures/04-linear_transformation_basis.png)\n",
    "\n",
    ":::{prf:example} Rewriting an linear transformation in another basis\n",
    ":label: eigen-ex8\n",
    "\n",
    "Consider $A = \\bm 1&2\\\\0&1\\em$ and $f(\\vv x) = A\\vv x$. This transformation maps $\\bm x_1\\\\x_2\\em \\mapsto \\bm x_1 + 2x_2 \\\\x_2\\em$. Consider the basis $\\vv{b_1} = \\bm 1\\\\2\\em, \\vv{b_2} = \\bm 0\\\\1\\em$ illustrated in blue below:\n",
    "\n",
    "![alt text](../figures/04-blue_basis.png)\n",
    "\n",
    "The basis matrix $B$ is $B = \\bm 1&0\\\\2&1\\em$, and $B^{-1} = \\bm 1&0\\\\-2&1 \\em$. The matrix representation for $g(\\vv y)$ is then:\n",
    "\n",
    "\\begin{align*}\n",
    "    B^{-1}AB &= \\bm 1&0\\\\-2&1\\em \\bm 1&2\\\\0&1\\em \\bm 1&0\\\\2&1\\em\\\\\n",
    "    &= \\bm 5&2\\\\-8&-3\\em\n",
    "\\end{align*}\n",
    "\n",
    "and the map $g(\\vv y) = B^{-1}AB\\vv y$ takes $\\bm y_1\\\\y_2\\em \\mapsto \\bm 5y_1 + 2y_2\\\\-8y_1 - 3y_2\\em$.\n",
    ":::\n",
    "\n",
    "# Diagonalization\n",
    "\n",
    "In the above example, our change of basis didn't really help us understand what the linear transformation $f(\\vv x)$ is doing any better than our starting point. However, we'll see how that if we use the basis defiend by the eigenvectors of a matrix, some magic happens! We'll start with an example, and then extract out a general conclusion.\n",
    "\n",
    ":::{prf:example} Rewriting an linear transformation in an eigenbasis\n",
    ":label: eigen-ex9\n",
    "\n",
    "Consider the linear transformation $h(x_1, x_2) = \\bm x_1 - x_2 \\\\ 2x_1 + 4x_2\\em$. It has matrix representation $A = \\bm 1&-1\\\\2&4\\em$4 with respect to the standard basis of $\\mathbb{R}^2$. \n",
    "\n",
    "The eigenvalues of $A$ are computed by solving $\\det(A - \\lambda I) = 0$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\det \\bm 1 - \\lambda & -1\\\\2 & 4 - \\lambda\\em  (1 - \\lambda)(4 - \\lambda) + 2 = \\lambda^2 - 5\\lambda + 6 = (\\lambda - 2)(\\lambda - 3) = 0\n",
    "\\end{align*}\n",
    "\n",
    "so that $\\lambda_1 = 2$ and $\\lambda_2 = 3$. Solving the appropriate eigenvector equations $(A - \\lambda_i I)\\vv{v_i} = \\vv 0$, we obtain the following eigenvalue/eigenvector pairs:\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_1 = 2, \\vv{v_1} = \\bm 1\\\\-1\\em \\quad\\text{and}\\quad \\lambda_2 = 3, \\vv{v_2} = \\bm 1\\\\-2\\em\n",
    "\\end{align*}\n",
    "\n",
    "Let's see what happens if we express $A$ in coordinate system defined by the [eigenbasis](#eigenbasis-defn) $V = \\bm \\vv{v_1} & \\vv{v_2} \\em = \\bm 1&1\\\\-1&-2\\em$.\n",
    "\n",
    "First, we compute $V^{-1} = \\frac{1}{1(-2) - 1(-1)}\\bm -2&-1 \\\\1&1\\em = \\bm 2&1\\\\-1&-1\\em$, \n",
    "\n",
    "and then find $V^{-1}AV = \\bm 2&1\\\\-1&-1\\em \\bm 1&-1 \\\\2&4\\em \\bm 1&1\\\\-1&-2\\em = \\bm 2&0\\\\0&3\\em$.\n",
    "\n",
    "This matrix is diagonal! THis means it applies a simple stretching action in the coordinates defined by the eigenvectors. The eigenvalues for this new matrix are also $\\lambda_1 = 2$ and $\\lambda_2 = 3$, but in this case, eigenvectors are much simpler: $\\vv{\\hat{v_1}} = \\bm 1\\\\0\\em$ and $\\vv{\\hat{v_2}} = \\bm 0\\\\1\\em$.\n",
    ":::\n",
    "\n",
    "The above example showed us an example of a very important property of an eigenbasis: they *diagonalize* the original matrix representative! Working with diagonal matrices is very convenient, and thus diagonalization is very useful when we can do it.\n",
    "\n",
    "Although we only saw a $2\\times 2$ example, the idea is applicable to general $n\\times n$ matrices, in the idea of *diagonalizable* matrices.\n",
    "\n",
    ":::{prf:definition} Diagonalizable\n",
    ":label: diagonalizable-defn\n",
    "\n",
    "We say that a square matrix $A$ is *diagonalizable* if there exists a nonsingular matrix $V$ and a diagonal matrix $D = \\text{diag}(\\lambda_1, ..., \\lambda_n)$ such that\n",
    "\n",
    "\\begin{align*}\\label{expr:diagonalizable}\n",
    "V^{-1} A V = D \\quad\\text{or equivalently}\\quad A = VDV^{-1} \\tag{D}\n",
    "\\end{align*}\n",
    ":::\n",
    "\n",
    "Let's try to understand condition [(D)](expr:diagonalizable) a little bit more by writing it as \n",
    "\n",
    "\\begin{align*}\n",
    "AV = VD\n",
    "\\end{align*}\n",
    "\n",
    "Now, for $V = \\bm \\vv{v_1} & \\vv{v_2} & \\dots & \\vv{v_n} \\em$, this becomes: \n",
    "\n",
    "\\begin{align*}\n",
    "\\bm A\\vv{v_1} & A\\vv{v_2} & \\dots & A\\vv{v_n}\\em = \\bm \\lambda_1 \\vv{v_1} & \\lambda_2 \\vv{v_2} & \\dots & \\lambda_n \\vv{v_n} \\em\n",
    "\\end{align*}\n",
    "\n",
    "Focusing on the $k^{th}$ column of this $n\\times n$ matrix equation, we see something familiar:\n",
    "\n",
    "\\begin{align*}\n",
    "A\\vv{v_k} = \\lambda_k \\vv{v_k},\n",
    "\\end{align*}\n",
    "\n",
    "that is, the columns of $V$ must be eigenvectors, and the diagonal elements $\\lambda_i$ must be eigenvectors! Therefore, we immediately get the following characterization of when a matrix is diagonalizable:\n",
    "\n",
    ":::{prf:theorem} A necessary and sufficient condition for diagonalizability\n",
    ":label: diagonalizable-thm\n",
    "\n",
    "A matrix $A \\in \\mathbb{R}^{n\\times n}$ is diagonalizable if and only if it has $n$ linearly independent eigenvectors. In other words, $A$ is diagonalizable if and only if $A$ is [complete](#eigenbasis-defn).\n",
    "\n",
    "Equivalently, $A$ is digonalizable if and only if, for each eigenvalue $\\lambda$, its [geometric multiplicity matches its algebraic multiplicity](#multiplicity-defn).\n",
    "\n",
    "In this case, we can diagonalize $A$ as:\n",
    "\n",
    "\\begin{align*}\n",
    "    A = \\bm \\vv{v_1} & \\vv{v_2} & \\dots & \\vv{v_n} \\em \\bm \\lambda_1 & 0 & \\dots & 0 \\\\ 0 & \\lambda_2 & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & \\lambda_n \\em \\bm \\vv{v_1} & \\vv{v_2} & \\dots & \\vv{v_n} \\em^{-1}\n",
    "\\end{align*}\n",
    "\n",
    "where $\\lambda_1$ is the eigenvalue corresponding to $\\vv{v_1}$, and so on.\n",
    ":::\n",
    "\n",
    "Next, let's look at some examples of diagonalizable and nondiagonalizable matrices:\n",
    "\n",
    ":::{exercise} Checking diagonalizability of $2\\times 2$ matrices\n",
    ":label:eigen-ex9\n",
    "\n",
    "For each of the matrices below, determine whether or not they are [diagonalizable](#diagonalizable-defn).\n",
    "\n",
    "1. $A = \\bm 1&0 \\\\ 1&1 \\em$\n",
    "\n",
    "2. $B = \\bm 3&-2\\\\2&-1 \\em$\n",
    "\n",
    "3. $C = \\bm 0&0\\\\0&0 \\em$\n",
    "\n",
    "```{solution} eigen-ex9\n",
    ":class: dropdown\n",
    "\n",
    "First, let's check if $A$ is diagonalizable. Solving for the eigenvalues of $A$,\n",
    "\n",
    "\\begin{align*}\n",
    "\\det(A - \\lambda I) = 0 \\iff (1 - \\lambda)(-\\lambda) - 1 = 0 \\\\\n",
    "\\iff \\lambda^2 - \\lambda - 1 = 0 \\iff \\lambda = \\frac{1\\pm \\sqrt 5}{2}\n",
    "\\end{align*}\n",
    "\n",
    "We see that since $A$ has $2$ distinct eigenvalues (which is the dimension of $A$) it immediately follows from [this fact](#distinct-eigenvalue-thm) that the eigenvectors of $A$ span $\\mathbb{R}^2$, hence $A$ is diagonalizable.\n",
    "\n",
    "Next, let's check if $B$ is diagonalizable. Solving for the eigenvalues of $B$,\n",
    "\n",
    "\\begin{align*}\n",
    "\\det(B - \\lambda I) = 0 \\iff (3 - \\lambda)(-1 - \\lambda) - (-2)(2) = 0 \\\\\n",
    "\\iff \\lambda^2 - 2\\lambda + 1 = 0 iff \\lambda = 1\n",
    "\\end{align*}\n",
    "\n",
    "Since $A$ has only $1$ distinct eigenvalue, we must check the dimension of the corresponding eigenspace. Solving the eigenvector equation for $\\lambda_1 = 1$,\n",
    "\n",
    "\\begin{align*}\n",
    "    (B - I) \\vv x = \\vv 0 \\iff \\bm 2&-2\\\\2&-2 \\em \\vv x = \\vv 0 \\iff \\vv x = a\\bm 1\\\\1 \\em, a \\in \\mathbb{R}\n",
    "\\end{align*}\n",
    "\n",
    "We see that the corresponding eigenspace only has a dimension of $1$, meaning that the eigenvectors of $B$ do not space $\\mathbb{R}^2$, hence $B$ is not diagonalizable.\n",
    "\n",
    "Next, let's check if $C$ is diagonalizable. Clearly the eigenvalues of $C$ are just $0$, and the corresponding eigenspace is the entire space $\\mathbb{R}^2$. Hence $C$ is diagonalizable, and one such diagonalization is given by:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\bm 0&0\\\\0&0 \\em = \\bm 1&0\\\\0&1\\em\\bm 0&0\\\\0&0\\em \\bm 1&0\\\\0&1\\em^{-1}\n",
    "\\end{align*}\n",
    "\n",
    "As we see, there is no direct connection between invertibility and diagonalizaibility, in the sense that one does not imply the other. You can have invertible matrices which are not diagonalizable (like $B$) and diagonalizable matrices which are not invertible (like $C$).\n",
    "```\n",
    ":::\n",
    "\n",
    "#### Python Break!\n",
    "\n",
    "Here, we'll show how to use `numpy.linalg` (or `scipy.linalg`) to diagonalize a matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:\n",
      "[[ 1.         -0.57735027  0.        ]\n",
      " [ 0.          0.57735027 -0.70710678]\n",
      " [ 0.          0.57735027  0.70710678]] \n",
      "\n",
      "D:\n",
      "[[2. 0. 0.]\n",
      " [0. 4. 0.]\n",
      " [0. 0. 2.]] \n",
      "\n",
      "PDP^{-1}:\n",
      "[[ 2. -1. -1.]\n",
      " [ 0.  3.  1.]\n",
      " [ 0.  1.  3.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# given a square matrix A, returns a tuple of matrices (P, D) such that A = PDP^{-1}\n",
    "def diagonalize(A):\n",
    "    evals, evecs = np.linalg.eig(A)\n",
    "    return evecs, np.diag(evals)\n",
    "\n",
    "A = np.array([\n",
    "    [2, -1, -1],\n",
    "    [0, 3, 1],\n",
    "    [0, 1, 3]\n",
    "])\n",
    "\n",
    "P, D = diagonalize(A)\n",
    "\n",
    "print('P:')\n",
    "print(P, '\\n')\n",
    "print('D:')\n",
    "print(D, '\\n')\n",
    "print('PDP^{-1}:')\n",
    "print(P @ D @ np.linalg.inv(P))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, finding a diagonalization in Python is really easy! The `numpy.linalg.eig` function returns the eigenvectors in a matrix and the eigenvalues (conveniently, it has the eigenvalues in the same order as the corresponding eigenvectors)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
