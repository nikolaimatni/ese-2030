{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 9.1 Eigenvalues of Symmetric Matrices\n",
    "subject:  Symmetric Matrices\n",
    "subtitle: \n",
    "short_title: 9.1 Eigenvalues of Symmetric Matrices\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: \n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/08_Ch_9_Symmetric_Matrices/101-eigen_symm.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 16 - Eigenvalues of Symmetric Matrices, Spectral Theorem, Quadratic Forms and Positive Definite Matrices, Optimization Principles for Eigenvalues of Symmetric Matrices.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be ALAA 8.5.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- \n",
    "- the Spectral theorem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symmetric Matrix\n",
    "\n",
    "A square matrix $A$ is said to be symmetric if $A = A^T$. For example, all 2$\\times$2 symmetric and 3$\\times$3 symmetric matrices are of the form:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a & b \\\\\n",
    "b & c\n",
    "\\end{bmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "\\begin{bmatrix}\n",
    "a & b & c \\\\\n",
    "b & d & e \\\\\n",
    "c & e & f\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Symmetric matrices arise in many practical contexts: an important one we will spend time on next lecture are _covariance matrices_. For now, we simply take them as a family of interesting matrices.\n",
    "\n",
    "Symmetric matrices enjoy many interesting properties, including the following one which will be the focus of this lecture:\n",
    "\n",
    ":::{prf:theorem}\n",
    "Let $A = A^T \\in \\mathbb{R}^{n\\times n}$ be a symmetric $n\\times n$ matrix. Then:\n",
    "1. All eigenvalues of $A$ are real.\n",
    "2. Eigenvectors corresponding to distinct eigenvalues of $A$ are orthogonal.\n",
    "3. There is an orthonormal basis of $\\mathbb{R}^n$ consisting of $n$ eigenvectors of $A$.\n",
    "In particular, all real symmetric matrices are complete and real diagonalizable.\n",
    ":::\n",
    "\n",
    "We'll spend the rest of this lecture exploring the consequences of this remarkable theorem, before diving into applications over the next few lectures.\n",
    "\n",
    "First, we work through a few simple examples to see this theorem in action.\n",
    "\n",
    ":::{prf:example}\n",
    "$A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix}$. We've seen this matrix in previous examples. It has eigenvalues $\\lambda_1 = 4$ and $\\lambda_2 = 2$ with corresponding eigenvectors $\\vv v_1 = (1,1)$ and $\\vv v_2 = (-1,1)$. We easily verify that $\\vv v_1^T \\vv v_2 = 0$, and hence are orthogonal. We construct an orthonormal basis by dividing each eigenvector by its Euclidean norm:\n",
    "\n",
    "$$\n",
    "\\vv u_1 = \\frac{\\vv v_1}{\\|\\vv v_1\\|} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "\\vv u_2 = \\frac{\\vv v_2}{\\|\\vv v_2\\|} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    ":::\n",
    "\n",
    ":::{prf:example} \n",
    "Consider the symmetric matrix $A = \\begin{bmatrix} 5 & -4 & 2 \\\\ -4 & 5 & 2 \\\\ 2 & 2 & -1 \\end{bmatrix}$. Computing the eigenvalues/eigenvectors of $A$ (e.g., using `np.linalg.eig`) we see that\n",
    "\n",
    "$$\n",
    "\\lambda_1 = 9, \\vv v_1 = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix}, \\quad\n",
    "\\lambda_2 = 3, \\vv v_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad \\text{and} \\quad\n",
    "\\lambda_3 = -3, \\vv v_3 = \\begin{bmatrix} 1 \\\\ 1 \\\\ -2 \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check that these vectors are pairwise orthogonal: $\\vv v_i^T \\vv v_j = 0$ for $i \\neq j$, and hence form an orthogonal basis for $\\mathbb{R}^3$. An orthonormal basis is obtained by the corresponding unit norm eigenvectors:\n",
    "$$\n",
    "\\vv u_1 = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} -1 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad\n",
    "\\vv u_2 = \\frac{1}{\\sqrt{3}} \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad \\text{and} \\quad\n",
    "\\vv u_3 = \\frac{1}{\\sqrt{6}} \\begin{bmatrix} 1 \\\\ 1 \\\\ -2 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "## The Spectral Theorem\n",
    "\n",
    "The theorem above tells us that every real, symmetric matrix admits an eigenvector basis, and hence is diagonalizable. Furthermore, we can always choose eigenvectors that form an orthonormal basis—hence, the diagonalizing matrix takes a particularly simple form.\n",
    "\n",
    "Remember that a matrix $Q \\in \\mathbb{R}^{n \\times n}$ is **orthogonal** if and only if its columns form an orthonormal basis of $\\mathbb{R}^n$. Alternatively, we can characterize orthogonal matrices by the condition that $Q^T Q = Q Q^T = I$, i.e., $Q^{-1} = Q^T$.\n",
    "\n",
    "If we use this orthonormal eigenbasis when diagonalizing a symmetric matrix $A$, we obtain its _spectral factorization_:\n",
    "\n",
    ":::{prf:theorem}\n",
    ":label: spectral_thm\n",
    "Let $A$ be a real symmetric matrix. Then there exists an orthogonal matrix $Q$ such that\n",
    "\\begin{equation}\n",
    "\\label{ST_eqn}\n",
    "A = Q \\Lambda Q^{-1} = Q \\Lambda Q^T \\qquad (\\text{ST})\n",
    "\\end{equation}\n",
    "where $\\Lambda$ is a real diagonal matrix. The eigenvalues of $A$ appear on the diagonal of $\\Lambda$, while the columns of $Q$ are the corresponding orthonormal eigenvectors.\n",
    ":::\n",
    "\n",
    ":::{note} Historical Remark\n",
    "The term \"spectrum\" refers to the eigenvalues of a matrix, or more generally, a linear operator. This terminology originates in physics: the spectral energy lines of atoms, molecules, and nuclei are characterized as the eigenvalues of the governing quantum mechanical Schrödinger operator.\n",
    ":::\n",
    "\n",
    ":::{prf:example}\n",
    "For $A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix}$ seen above, we build $Q = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 & -1 \\\\ 1 & 1 \\end{bmatrix}$, and write\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix} = A = Q \\Lambda Q^T = \n",
    "\\begin{bmatrix} \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\ \n",
    "\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{bmatrix}\n",
    "\\begin{bmatrix} 4 & 0 \\\\ 0 & 2 \\end{bmatrix}\n",
    "\\begin{bmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\ \n",
    "-\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{bmatrix}.\n",
    "$$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric Interpretation \n",
    "\n",
    "You can always choose $Q$ to have $\\det Q = 1$; such a $Q$ represents a rotation. Thus the diagonalization of a symmetric matrix can be interpreted as a rotation of the coordinate system so that the orthogonal eigenvectors align with the coordinate axes. Therefore, the linear transformation $L(\\vv x) = A\\vv x$ for which $A$ has all positive eigenvalues can be interpreted as a combination of stretches in $n$ mutually orthogonal directions. One way to visualize this is to consider what $L(\\vv x)$ does to the unit Euclidean sphere $S = \\{ \\vv x \\in \\mathbb{R}^n \\mid \\|\\vv x\\| = 1\\}$: stretching it in orthogonal directions will transform it into an ellipsoid : $E = L(S) = \\{ A\\vv x \\mid \\|\\vv x\\| = 1\\}$ whose principal axes are the directions of stretch, i.e., the eigenvectors of $A$.\n",
    "\n",
    ":::{figure}../figures/09-ellipse.jpg\n",
    ":label:ellipse\n",
    ":alt:Ellipse\n",
    ":width: 400px\n",
    ":align: center\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/07_Ch_8_Iteration/092-Markov_Chains.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\section*{Quadratic Forms \\& Positive Definite Matrices (ALA 3.4, LAA 7.2)}\n",
    "\n",
    "One common place where symmetric matrices arise in application is in defining quadratic forms, which pop up in engineering design (in design criteria and optimization), signal processing (as output noise power), physics (as potential \\& kinetic energy), differential geometry (as normal curvature of surfaces), economics (as utility functions), and statistics (in confidence ellipsoids).\n",
    "\n",
    "A quadratic form is a function mapping $\\mathbb{R}^n$ to $\\mathbb{R}$ of the form\n",
    "\n",
    "\\[\n",
    "q(x) = x^T k x \\qquad (QF)\n",
    "\\]\n",
    "\n",
    "where $k = k^T \\in \\mathbb{R}^{n \\times n}$ is an $n \\times n$ symmetric matrix. Such quadratic forms arise frequently in applications of linear algebra. For example, setting $k = I_n$ and $x = Ax - b$, we recover the least-squares objective\n",
    "\n",
    "\\[\n",
    "q(Ax-b) = (Ax-b)^T(Ax-b) = \\|Ax-b\\|^2.\n",
    "\\]\n",
    "\n",
    "\\textbf{Example:} For $x \\in \\mathbb{R}^3$, let $q(x) = 5x_1^2 + 3x_2^2 + 2x_3^2 - x_1x_2 + 6x_2x_3$. Find a matrix $k = k^T \\in \\mathbb{R}^{3\\times3}$ such that $q(x) = x^T k x$.\n",
    "\n",
    "The approach is to recognize that the coefficients of $x_1^2$, $x_2^2$, and $x_3^2$ go on the diagonal of $k$. To make $k$ symmetric, the coefficients for $x_ix_j$, $i\\neq j$, should be evenly split between the $(i,j)$ and $(j,i)$ entries of $k$.\n",
    "\n",
    "Using this strategy, we obtain:\n",
    "\n",
    "\\[\n",
    "q(x) = x^T k x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}^T \n",
    "\\begin{bmatrix} \n",
    "5 & -\\frac{1}{2} & 0 \\\\\n",
    "-\\frac{1}{2} & 3 & 3 \\\\\n",
    "0 & 3 & 2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}.\n",
    "\\]\n",
    "\n",
    "\\section*{The Geometry of Quadratic Forms}\n",
    "\n",
    "We'll focus on understanding the geometry of quadratic forms on $\\mathbb{R}^2$. Let $k = k^T \\in \\mathbb{R}^{2\\times2}$ be an invertible $2\\times2$ symmetric matrix, and let's consider quadratic form:\n",
    "\n",
    "\\[\n",
    "q(x) = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}^T \n",
    "\\begin{bmatrix} k_{11} & k_{12} \\\\ k_{12} & k_{22} \\end{bmatrix}\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = k_{11}x_1^2 + 2k_{12}x_1x_2 + k_{22}x_2^2 \\qquad (2D).\n",
    "\\]\n",
    "\n",
    "What kinds of functions do these define? We study this question by looking at the level sets of $q(x)$. The $\\alpha$-level set of $q(x)$ is the set of all $x \\in \\mathbb{R}^2$ such that $q(x) = \\alpha$:\n",
    "\n",
    "\\[\n",
    "C_\\alpha = \\{x \\in \\mathbb{R}^2 : q(x) = \\alpha\\}. \\qquad (a)\n",
    "\\]\n",
    "\n",
    "It is possible to show that such level sets correspond to either an ellipse, a hyperbola, two intersecting lines, a single point, or no points at all. If $k$ is a diagonal matrix, the graph of (a) is in standard position, as seen below:\n",
    "\n",
    "[Insert Figure 2 here]\n",
    "\n",
    "\\textbf{Figure 2}: An ellipse and a hyperbola in standard position.\n",
    "\n",
    "If $k$ is not diagonal, the graph of (a) is rotated out of standard position, as shown below:\n",
    "\n",
    "[Insert figure for rotated ellipse/hyperbola here]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
