{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 9.1 Eigenvalues of Symmetric Matrices\n",
    "subject:  Symmetric Matrices\n",
    "subtitle: factorizing symmetric matrices\n",
    "short_title: 9.1 Eigenvalues of Symmetric Matrices\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: \n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/08_Ch_9_Symmetric_Matrices/101-eigen_symm.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 16 - Eigenvalues of Symmetric Matrices, Spectral Theorem, Quadratic Forms and Positive Definite Matrices, Optimization Principles for Eigenvalues of Symmetric Matrices.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be found in ALA 8.5.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- the properties of eigenvalues and eigenvectors of symmetric matrices\n",
    "- the Spectral theorem\n",
    "- the geometric interpretation of a linear transformation when the matrix is symmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symmetric Matrix\n",
    "\n",
    "A square matrix $A$ is said to be symmetric if $A = A^T$. For example, all 2$\\times$2 symmetric and 3$\\times$3 symmetric matrices are of the form:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a & b \\\\\n",
    "b & c\n",
    "\\end{bmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "\\begin{bmatrix}\n",
    "a & b & c \\\\\n",
    "b & d & e \\\\\n",
    "c & e & f\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Symmetric matrices arise in many practical contexts: an important one we will spend time on next lecture are _covariance matrices_. For now, we simply take them as a family of interesting matrices.\n",
    "\n",
    "Symmetric matrices enjoy many interesting properties, including the following one which will be the focus of this lecture:\n",
    "\n",
    ":::{prf:theorem}\n",
    "Let $A = A^T \\in \\mathbb{R}^{n\\times n}$ be a symmetric $n\\times n$ matrix. Then:\n",
    "1. All eigenvalues of $A$ are real.\n",
    "2. Eigenvectors corresponding to distinct eigenvalues of $A$ are orthogonal.\n",
    "3. There is an orthonormal basis of $\\mathbb{R}^n$ consisting of $n$ eigenvectors of $A$.\n",
    "In particular, all real symmetric matrices are complete and real diagonalizable.\n",
    ":::\n",
    "\n",
    "We'll spend the rest of this lecture exploring the consequences of this remarkable theorem, before diving into applications over the next few lectures.\n",
    "\n",
    "First, we work through a few simple examples to see this theorem in action.\n",
    "\n",
    ":::{prf:example}\n",
    "$A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix}$. We've seen this matrix in previous examples. It has eigenvalues $\\lambda_1 = 4$ and $\\lambda_2 = 2$ with corresponding eigenvectors $\\vv v_1 = (1,1)$ and $\\vv v_2 = (-1,1)$. We easily verify that $\\vv v_1^T \\vv v_2 = 0$, and hence are orthogonal. We construct an orthonormal basis by dividing each eigenvector by its Euclidean norm:\n",
    "\n",
    "$$\n",
    "\\vv u_1 = \\frac{\\vv v_1}{\\|\\vv v_1\\|} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "\\vv u_2 = \\frac{\\vv v_2}{\\|\\vv v_2\\|} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    ":::\n",
    "\n",
    ":::{prf:example} \n",
    "Consider the symmetric matrix $A = \\begin{bmatrix} 5 & -4 & 2 \\\\ -4 & 5 & 2 \\\\ 2 & 2 & -1 \\end{bmatrix}$. Computing the eigenvalues/eigenvectors of $A$ (e.g., using `np.linalg.eig`) we see that\n",
    "\n",
    "$$\n",
    "\\lambda_1 = 9, \\vv v_1 = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix}, \\quad\n",
    "\\lambda_2 = 3, \\vv v_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad \\text{and} \\quad\n",
    "\\lambda_3 = -3, \\vv v_3 = \\begin{bmatrix} 1 \\\\ 1 \\\\ -2 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "You can check that these vectors are pairwise orthogonal: $\\vv v_i^T \\vv v_j = 0$ for $i \\neq j$, and hence form an orthogonal basis for $\\mathbb{R}^3$. An orthonormal basis is obtained by the corresponding unit norm eigenvectors:\n",
    "$$\n",
    "\\vv u_1 = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} -1 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad\n",
    "\\vv u_2 = \\frac{1}{\\sqrt{3}} \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad \\text{and} \\quad\n",
    "\\vv u_3 = \\frac{1}{\\sqrt{6}} \\begin{bmatrix} 1 \\\\ 1 \\\\ -2 \\end{bmatrix}.\n",
    "$$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The Spectral Theorem\n",
    "\n",
    "The theorem above tells us that every real, symmetric matrix admits an eigenvector basis, and hence is diagonalizable. Furthermore, we can always choose eigenvectors that form an orthonormal basis—hence, the diagonalizing matrix takes a particularly simple form.\n",
    "\n",
    "Remember that a matrix $Q \\in \\mathbb{R}^{n \\times n}$ is **orthogonal** if and only if its columns form an orthonormal basis of $\\mathbb{R}^n$. Alternatively, we can characterize orthogonal matrices by the condition that $Q^T Q = Q Q^T = I$, i.e., $Q^{-1} = Q^T$.\n",
    "\n",
    "If we use this orthonormal eigenbasis when diagonalizing a symmetric matrix $A$, we obtain its _spectral factorization_:\n",
    "\n",
    ":::{prf:theorem}\n",
    ":label: spectral_thm\n",
    "Let $A$ be a real symmetric matrix. Then there exists an orthogonal matrix $Q$ such that\n",
    "\\begin{equation}\n",
    "\\label{ST_eqn}\n",
    "A = Q \\Lambda Q^{-1} = Q \\Lambda Q^T \\qquad (\\text{ST})\n",
    "\\end{equation}\n",
    "where $\\Lambda$ is a real diagonal matrix. The eigenvalues of $A$ appear on the diagonal of $\\Lambda$, while the columns of $Q$ are the corresponding orthonormal eigenvectors.\n",
    ":::\n",
    "\n",
    ":::{note} Historical Remark\n",
    "The term \"spectrum\" refers to the eigenvalues of a matrix, or more generally, a linear operator. This terminology originates in physics: the spectral energy lines of atoms, molecules, and nuclei are characterized as the eigenvalues of the governing quantum mechanical Schrödinger operator.\n",
    ":::\n",
    "\n",
    ":::{prf:example}\n",
    "For $A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix}$ seen above, we build $Q = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 & -1 \\\\ 1 & 1 \\end{bmatrix}$, and write\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix} = A = Q \\Lambda Q^T = \n",
    "\\begin{bmatrix} \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\ \n",
    "\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{bmatrix}\n",
    "\\begin{bmatrix} 4 & 0 \\\\ 0 & 2 \\end{bmatrix}\n",
    "\\begin{bmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\ \n",
    "-\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{bmatrix}.\n",
    "$$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric Interpretation \n",
    "\n",
    "You can always choose $Q$ to have $\\det Q = 1$; such a $Q$ represents a rotation. Thus the diagonalization of a symmetric matrix can be interpreted as a rotation of the coordinate system so that the orthogonal eigenvectors align with the coordinate axes. Therefore, the linear transformation $L(\\vv x) = A\\vv x$ for which $A$ has all positive eigenvalues can be interpreted as a combination of stretches in $n$ mutually orthogonal directions. One way to visualize this is to consider what $L(\\vv x)$ does to the unit Euclidean sphere $S = \\{ \\vv x \\in \\mathbb{R}^n \\mid \\|\\vv x\\| = 1\\}$: stretching it in orthogonal directions will transform it into an ellipsoid : $E = L(S) = \\{ A\\vv x \\mid \\|\\vv x\\| = 1\\}$ whose principal axes are the directions of stretch, i.e., the eigenvectors of $A$.\n",
    "\n",
    ":::{figure}../figures/09-ellipse.jpg\n",
    ":label:ellipse\n",
    ":alt:Ellipse\n",
    ":width: 400px\n",
    ":align: center\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/08_Ch_9_Symmetric_Matrices/101-eigen_symm.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
