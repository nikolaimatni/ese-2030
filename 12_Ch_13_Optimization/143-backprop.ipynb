{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 12.3 Backpropogation\n",
    "subject:  Optimization\n",
    "subtitle: \n",
    "short_title: 12.3 Backpropogation\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: \n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/12_Ch_13_Optimization/143-backprop.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 22 - An Introduction to Backpropagation.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be found in LLA Chapter 9.2. Reviewing CalcBLUE2 Chapter 5 on the chain rule is recommended.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation: Motivation\n",
    "\n",
    "Last class, we studied the unconstrained optimization\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{uncon_opt}\n",
    "\\text{minimize } f(\\vv x)\n",
    "\\end{equation}\n",
    "\n",
    "over $\\vv x \\in \\mathbb{R}^n$, where we look for the $\\vv x \\in \\mathbb{R}^n$ that makes the value of the cost function $f: \\mathbb{R}^n \\to \\mathbb{R}$ as small as possible. We saw that one way to find either a local or global minimum $\\vv x^*$ is gradient descent. Starting at an initial guess $\\vv x^{(0)}$, we iteratively update our guess via\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{GD}\n",
    "\\vv x^{(k+1)} = \\vv x^{(k)} - s \\nabla f(\\vv x^{(k)}), \\quad k = 0, 1, 2, \\ldots \\text{(GD)}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\nabla f(\\vv x^{(k)}) \\in \\mathbb{R}^n$ is the gradient of $f$ evaluated at the current guess, and $s > 0$ is a step size chosen large enough to make progress towards $\\vv x^*$, but not so big as to overshoot.\n",
    "\n",
    "Today, we'll focus our attention on optimization problems [](#uncon_opt) for which the cost function takes the following special form\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{cost}\n",
    "f(\\vv x) = \\sum_{i=1}^N f_i(\\vv x),\n",
    "\\end{equation}\n",
    "\n",
    "i.e., cost functions $f$ that decompose into a sum of $N$ \"sub-costs\" $f_i$. Problems with cost functions of the form [](#cost) are particularly common in machine learning.\n",
    "\n",
    "For example, a typical problem setup in machine learning is as follows (we saw an example of this when we studied least squares for data-fitting). We are given a set of _training data_ $\\{(\\vv z_i, \\vv y_i)\\}, i=1, \\ldots, N$, comprised of \"inputs\" $\\vv z_i \\in \\mathbb{R}^p$ and \"outputs\" $\\vv y_i \\in \\mathbb{R}^p$. Our goal is to find a set of weights $\\vv x \\in \\mathbb{R}^n$ which parametrize a model such that $m(\\vv z_i; \\vv x) \\approx \\vv y_i$ on our training data. A common way of doing this is to minimize a _loss function_ of the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{loss}\n",
    "\\text{loss}((\\vv z_i, \\vv y_i); \\vv x) = \\frac{1}{N} \\sum_{i=1}^N \\ell(m(\\vv z_i; \\vv x) - \\vv y_i),\n",
    "\\end{equation}\n",
    "\n",
    "where each term $\\ell(m(\\vv z_i; \\vv x) - \\vv y_i)$ is a term penalizing the difference between our model prediction $m(\\vv z_i; \\vv x)$ on input $\\vv z_i$ and the observed output $\\vv y_i$. In this setting, the loss function [](#loss) takes the form [](#cost), with $f_i = \\frac{1}{N} \\ell(m(\\vv z_i; \\vv x) - \\vv y_i)$ the error between our prediction $\\hat{\\vv y}_i = m(\\vv z_i; \\vv x)$ and the true output $\\vv y_i$.\n",
    "\n",
    "A common choice for the \"sub-loss\" function is $\\ell(\\vv e) = \\|\\vv e\\|^2$, leading to a least-squares regression problem, but note that most other choices of loss function are compatible with the following discussion.\n",
    "\n",
    "Now suppose that we want to implement gradient descent [(GD)](#GD) on the loss function [](#loss). Our first step is to compute the gradient $\\nabla_{\\vv x} \\text{loss}((\\vv z_i, \\vv y_i); \\vv x)$. Because of the sum structure of [](#loss), we have that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\vv x} \\text{loss}((\\vv z_i,\\vv y_i);\\vv x) = \\frac{1}{N} \\sum_{i=1}^N \\nabla_{\\vv x} \\ell(m(\\vv z_i; \\vv x) - \\vv y_i),\n",
    "\\end{equation}\n",
    "\n",
    "i.e., the gradient of the loss function is the sum of the gradients of the \"sub-losses\" on each of the $i=1,\\ldots,N$ data points.\n",
    "\n",
    "Our task now is therefore to compute the gradient $\\nabla_{\\vv x} \\ell(m(\\vv z_i; \\vv x)- \\vv y_i)$. This requires the _multivariate chain rule_, as $f_i(\\vv x) = \\ell(m(\\vv z_i;\\vv x)-\\vv y_i)$ is a _composition_ of the functions $\\ell(\\vv e), \\vv e = \\vv w - \\vv y_i$, and $\\vv w = m(\\vv z_i;\\vv x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Multivariate Chain Rule (CalcBLUE 2 Ch.5)\n",
    "\n",
    "We begin with a reminder of the chain rule for scalar functions. Let $f:\\mathbb{R}\\to\\mathbb{R}$ and $g:\\mathbb{R}\\to\\mathbb{R}$ be differentiable functions. Then for $h(x) = g(f(x))$, we have that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{c1}\n",
    "h'(x) = g'(f(x)) f'(x). \n",
    "\\end{equation}\n",
    "\n",
    "If we define $g = g(f)$ and $f = f(x)$, then we can rewrite [](#c1) as $\\frac{dh}{dx} = \\frac{dg}{df}\\cdot\\frac{df}{dx}$. This is a useful way of writing things as we can \"cancel\" $df$ on the RHS to check that our formula is correct.\n",
    "\n",
    ":::{warning}\n",
    "$\\frac{dh}{dx} = \\frac{dg}{df} \\cdot \\frac{df}{dx}$ is shorthand for $\\frac{dh}{dx}(x) = \\frac{dg}{df}(f(x))\\frac{df}{dx}(x)$. The evaluation points matter!\n",
    ":::\n",
    "\n",
    "Generalizing slightly, suppose now that $f:\\mathbb{R}^n\\to\\mathbb{R}$ maps a vector $\\vv x\\in\\mathbb{R}^n$ to $f(\\vv x)\\in\\mathbb{R}$. Then for $h(x\\vv ) = g(f(\\vv x))$, we have:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{c2}\n",
    "\\nabla_{\\vv x} h(\\vv x) = g'(f(\\vv x)) \\nabla f(\\vv x),\n",
    "\\end{equation}\n",
    "\n",
    "which we see is a natural generalization of equation [](#c1). It will be convenient for us later to define $\\frac{df}{d \\vv x} = \\nabla_{\\vv x} f(\\vv x)^T$ and $\\frac{dh}{d \\vv x} = \\nabla_{\\vv x} h(\\vv x)^T$. Again defining $g = g(f)$ and $f = f(\\vv x)$, we can rewrite [](#c2) as $\\frac{dh}{d \\vv x} = \\frac{dg}{df} \\cdot \\frac{df}{d \\vv x}$, which looks exactly the same as before!\n",
    "\n",
    ":::{warning}\n",
    "$\\frac{dh}{d\\vv x} = \\frac{dg}{df} \\cdot \\frac{df}{d \\vv x}$ is shorthand for $\\frac{dh}{d \\vv x}(\\vv x) = \\frac{dg}{df}(f(\\vv x))\\frac{df}{d \\vv x}(\\vv x)$. The evaluation points matter!\n",
    ":::\n",
    "\n",
    "Now, let's apply these ideas to computing the gradient of $h(\\vv x) = \\ell(m(\\vv z_i; \\vv x)-  y_i)$, where we'll assume for now that $m(\\vv z_i;\\vv x), y_i \\in \\mathbb{R}$. Applying [](#c2), we get\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\vv x} h(\\vv x) = \\ell'(m(\\vv z_i;\\vv x)-y_i) \\cdot \\nabla_{\\vv x} (m(\\vv z_i;\\vv x) - y_i) = \\ell'(m(\\vv z_i;\\vv x)-y_i) \\cdot \\nabla_{\\vv x} m(\\vv z_i; \\vv x)\n",
    "\\end{equation}\n",
    "\n",
    "where we use that $\\nabla_{\\vv x} y_i = 0$ (since it's a constant). Without knowing more about the functions $\\ell$ and $m$, this is all we can say."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ":::{prf:example}\n",
    ":label: eg1\n",
    "Suppose $\\ell(e) = \\frac{1}{2}e^2$ and $m(\\vv z_i; \\vv x) = \\vv x^T \\vv z_i$. Then\n",
    "$$\n",
    "\\ell(m(\\vv z_i;\\vv x)-y_i) = \\frac{1}{2}(\\vv x^T\\vv z_i - y_i)^2 \\text{ and } \\nabla_{\\vv x} \\ell(m(\\vv z_i; \\vv x)-y_i) = \\underbrace{(\\vv x^T\\vv z_i - y_i)}_{\\ell'(m - y_i)} \\cdot \\underbrace{\\vv z_i}_{\\nabla_{\\vv x} m}\n",
    "$$\n",
    "\n",
    "Next lecture we will have a brief introduction to deep learning. In deep learning, the function $m(\\vv z_i; \\vv x)$ is often parameterized as a _chain of function compositions_:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{DN}\n",
    "\\vv m(\\vv z_i; \\vv x) &= \\vv m_L(\\vv m_{L-1}(\\cdots(\\vv m_2(\\vv m_1(\\vv z_i))\\cdots)) \\\\\n",
    "&= \\vv m_L \\circ \\vv m_{L-1} \\circ \\cdots \\circ \\vv m_2 \\circ \\vv m_1(\\vv z_i).\n",
    "\\end{equation}\n",
    "\n",
    "A more suggestive way of writing this parameterization (that also highlights the dependence on $\\vv x$) is\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{DNN}\n",
    "\\vv O_0 &= \\vv z_i &  \\\\\n",
    "\\vv O_1 &= \\vv m_1(\\vv O_0; \\vv x_1) & \\vv O_1 \\in \\mathbb{R}^{p_1}, \\vv O_0 \\in \\mathbb{R}^{p_0}  \\\\\n",
    "\\vv O_2 &= \\vv m_2(\\vv O_1; \\vv x_2) & \\vv O_2 \\in \\mathbb{R}^{p_2}, \\vv O_1 \\in \\mathbb{R}^{p_1} \\\\\n",
    "&\\vdots & \\vdots \\\\\n",
    "\\vv O_L &= \\vv m_L(\\vv O_{L-1}; \\vv x_L) & \\vv O_L \\in \\mathbb{R}^{p_L}, \\vv O_{L-1} \\in \\mathbb{R}^{p_{L-1}}\n",
    "\\end{equation}\n",
    "\n",
    "Here the model parameters \\vv $x = (\\vv x_1, \\ldots, \\vv x_L)$ we split across the _layers $1, \\ldots, L$_. The intermediate outputs $\\vv O_i$ can be of different dimensions, as can the layer parameters $\\vv x_i$. Writing [](#DN) as [](#DNN) highlights why these functions are called deep neural networks as the number of layers $L$ increases. Our goal is then to compute $\\nabla_{\\vv x} \\ell(m(\\vv z_i; \\vv x)-\\vv y_i)$ for $m$ of the form [](#DN), and where $m, y_i \\in \\mathbb{R}^{p_L}$ are now also possibly vector-valued. To do this, we need the fully general multivariable chain rule.\n",
    "\n",
    "For $h(\\vv x) = g(f(\\vv x))$ with vector-valued $\\vv f: \\mathbb{R}^n \\to \\mathbb{R}^p$ and $\\vv g: \\mathbb{R}^p \\to \\mathbb{R}^m$, we need to define the _Jacobian matrices_ for $\\vv f$ and $\\vv g$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{jac}\n",
    "\\frac{d\\vv f}{d\\vv x} = \\bm \\frac{d f_1}{d \\vv x} \\\\ \\vdots \\\\ \\frac{d f_p}{d \\vv x} \\em\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "\\vdots & & \\vdots \\\\\n",
    "\\frac{\\partial f_p}{\\partial x_1} & \\cdots & \\frac{\\partial f_p}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "\\quad \\text{and} \\quad\n",
    "\\frac{d\\vv g}{d \\vv f} = \n",
    "\\bm \\frac{d f_1}{d \\vv f} \\\\ \\vdots \\\\ \\frac{d g_m}{d \\vv f} \\em\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial g_1}{\\partial f_1} & \\cdots & \\frac{\\partial g_1}{\\partial f_p} \\\\\n",
    "\\vdots & & \\vdots \\\\\n",
    "\\frac{\\partial g_m}{\\partial f_1} & \\cdots & \\frac{\\partial g_m}{\\partial f_p}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "as the $p\\times n$ and $m\\times p$ matrices of partial derivatives, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We'll use our same intuition of \"cancelling\" to derive the expression:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{dh}{dx} = \\frac{dg}{dt} \\cdot \\frac{df}{dx} \\tag{C3}\n",
    "\\end{equation}\n",
    "\n",
    "Note that (C3) is defined by a matrix-matrix multiplication of an $m\\times p$ and $p\\times n$ matrix, meaning $\\frac{dh}{dx} \\in \\mathbb{R}^{m\\times n}$. The claim is that $(C3)$ is a perfectly valid expression for $\\frac{\\partial h_i}{\\partial x_j}$ with respect to $x_j$. From (J) and (C3), we have\n",
    "\n",
    "\\begin{equation}\n",
    "\\left(\\frac{dh}{dx}\\right)_{i,j} = \\frac{dg_i}{dt} \\cdot \\left[\\frac{\\partial f}{\\partial x_j}\\right] = \\frac{\\partial g_i}{\\partial t_1} \\cdot \\frac{\\partial f_1}{\\partial x_j} + \\cdots + \\frac{\\partial g_i}{\\partial t_p} \\cdot \\frac{\\partial f_p}{\\partial x_j},\n",
    "\\end{equation}\n",
    "\n",
    "which is precisely the expression we were looking for. The \"cancellation rule\" tells us each term in the sum is computing the partial of $\\frac{\\partial h_i}{\\partial x_j}$ in the \"t\" coordinate.\n",
    "\n",
    "We can apply this formula recursively to our function class (NN) to obtain the formula:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{dm}{dx} = \\frac{dM_L}{dM_{L-1}} \\cdot \\frac{\\partial M_{L-1}}{\\partial M_{L-2}} \\cdots \\frac{dM_2}{dM_1} \\cdot \\frac{\\partial M_1}{\\partial x} \\tag{MC}\n",
    "\\end{equation}\n",
    "\n",
    "which is a fully general matrix chain rule. We'll use (MC) next to explore the key idea behind backpropagation, which has been a key technical enabler of contemporary deep learning.\n",
    "\n",
    "Backpropagation\n",
    "\n",
    "We are going to work out how to efficiently compute the gradient of\n",
    "\n",
    "\\[\\ell(m(z_i;x)-y_i)\\]\n",
    "\n",
    "when $m$ takes the form in (NNN). We'll furthermore assume, as is often the case in deep learning, that each layer function $M_\\ell$ takes the following form:\n",
    "\n",
    "\\[M_\\ell(O_{\\ell-1}; x_\\ell) = \\sigma\\left(X_\\ell \\begin{bmatrix} O_{\\ell-1} \\\\ 1 \\end{bmatrix}\\right)\\]\n",
    "\n",
    "where $X_\\ell$ is a $n_\\ell \\times (n_{\\ell-1}+1)$ matrix with entries given by $x_\\ell \\in \\mathbb{R}^{n_\\ell(n_{\\ell-1}+1)}$, and $\\sigma$ is a pointwise nonlinearity $\\sigma(x) = (G(x_1),\\ldots,G(x_p))$ called an activation function (we'll soon have more to say)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Applying our matrix chain rule to $\\ell(m(x_i)-y_i)$ (we won't write $z_i$ to save space) we get the expression\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d\\ell}{dx} = \\frac{\\partial \\ell}{\\partial m} \\frac{dm}{dx} = \\frac{\\partial \\ell}{\\partial m_L} \\frac{\\partial m_L}{\\partial m_{L-1}} \\cdots \\frac{\\partial m_2}{\\partial m_1} \\frac{\\partial m_1}{\\partial x}.\n",
    "\\end{equation}\n",
    "\n",
    "Here, $\\frac{\\partial \\ell}{\\partial m}$ is a $p_L$ dimensional row vector, and $\\frac{\\partial m_i}{\\partial m_{i-1}}$ is a $p_i \\times p_{i-1}$ matrix.\n",
    "\n",
    "In modern architectures, the layer dimensions (also called layer widths) $p_i$ can be very large (on the order of 100s of thousands or even millions), meaning the $\\frac{\\partial m_i}{\\partial m_{i-1}}$ matrices are \\textit{very} large. Too large to store in memory explicitly.\n",
    "\n",
    "Fortunately, since $\\frac{\\partial \\ell}{\\partial m}$ is a row vector, we can build $\\frac{d\\ell}{dx}$ by sequentially computing inner products. For example, if $\\frac{\\partial \\ell}{\\partial m_L} = [a_1 \\cdots a_{p_L}]$,\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\ell}{\\partial m_L} \\frac{\\partial m_L}{\\partial m_{L-1}} &= \\underbrace{\\frac{\\partial \\ell}{\\partial m_L}}_{1 \\times p_L} \\underbrace{\\begin{bmatrix} a_1 \\cdots a_{p_L} \\end{bmatrix}}_{p_L \\times p_{L-1}} \\\\\n",
    "&= [\\frac{\\partial \\ell}{\\partial m_L} a_1 \\cdots \\frac{\\partial \\ell}{\\partial m_L} a_{p_{L-1}}],\n",
    "\\end{align*}\n",
    "\n",
    "meaning we only ever need to store $\\frac{\\partial \\ell}{\\partial m_L}$ and $a_i$ in memory at any given time, which is only $2p_L$ numbers, as opposed to $p_L \\times p_{L-1}$. Then once we've computed $\\frac{\\partial \\ell}{\\partial m_L} \\frac{\\partial m_L}{\\partial m_{L-1}}$, which is now a $p_{L-1}$ dim. row vector, we can continue our way down the chain.\n",
    "\n",
    "What's left to do is compute the partial derivatives. Let's break down $\\frac{\\partial \\ell}{\\partial x}$ into partial derivatives with respect to a layer's parameters $x_i$. For layer $L$, we have:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\ell}{\\partial x_L} = \\frac{\\partial \\ell}{\\partial m_L} \\frac{\\partial m_L}{\\partial x_L} + \\frac{\\partial \\ell}{\\partial m_L} \\frac{\\partial m_L}{\\partial m_{L-1}} \\frac{\\partial m_{L-1}}{\\partial x_L} = \\frac{\\partial \\ell}{\\partial m_L} \\frac{\\partial m_L}{\\partial x_L}\n",
    "\\end{equation}\n",
    "\n",
    "Since $x_L$ appears in the last layer, it shows up right away in the first term above, which is the derivative of $m_L(m_{L-1};x_L)$ with respect to $x_L$ (the 2nd argument). The second term\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\ell}{\\partial m_L} \\frac{\\partial m_L}{\\partial m_{L-1}} \\frac{\\partial m_{L-1}}{\\partial x_L} = 0\n",
    "\\end{equation}\n",
    "\n",
    "which measures how $m_L$ changes with respect to changes in $m_{L-1}$ caused by changes in $x_L$ is zero because $m_{L-1}$ does not depend on $x_L$ at all! This is a key observation in the backpropagation algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's proceed to compute the derivative with respect to the parameter $x_{l-2}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial l}{\\partial x_{l-2}} &= \\frac{\\partial l}{\\partial w_l} \\cdot \\frac{\\partial w_l}{\\partial w_{l-1}} \\cdot \\left( \\frac{\\partial w_{l-1}}{\\partial x_{l-2}} + \\frac{\\partial w_{l-1}}{\\partial w_{l-2}} \\cdot \\frac{\\partial w_{l-2}}{\\partial x_{l-2}} \\right) \\\\\n",
    "&= \\frac{\\partial l}{\\partial w_l} \\cdot \\frac{\\partial w_l}{\\partial w_{l-1}} \\cdot \\frac{\\partial w_{l-1}}{\\partial x_{l-2}}\n",
    "\\end{align*}\n",
    "\n",
    "We see again that if we can \"stop\" one or two layers short depends explicitly on $x_{l-2}$. Formally, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial l}{\\partial x_l} &= \\frac{\\partial l}{\\partial w_l} \\cdot \\frac{\\partial w_l}{\\partial x_l} \\\\\n",
    "\\frac{\\partial l}{\\partial x_{l-1}} &= \\frac{\\partial l}{\\partial w_l} \\cdot \\frac{\\partial w_l}{\\partial w_{l-1}} \\cdot \\frac{\\partial w_{l-1}}{\\partial x_{l-1}} \\quad \\left( \\frac{\\partial l}{\\partial w_{l-1}} = \\frac{\\partial l}{\\partial w_l} \\cdot \\frac{\\partial w_l}{\\partial w_{l-1}} \\right) \\\\\n",
    "\\frac{\\partial l}{\\partial x_{l-2}} &= \\frac{\\partial l}{\\partial w_l} \\cdot \\frac{\\partial w_l}{\\partial w_{l-1}} \\cdot \\frac{\\partial w_{l-1}}{\\partial w_{l-2}} \\cdot \\frac{\\partial w_{l-2}}{\\partial x_{l-2}} \\quad \\left( \\frac{\\partial l}{\\partial w_{l-2}} = \\frac{\\partial l}{\\partial w_{l-1}} \\cdot \\frac{\\partial w_{l-1}}{\\partial w_{l-2}} \\right) \\\\\n",
    "\\frac{\\partial l}{\\partial x_j} &= \\frac{\\partial l}{\\partial w_l} \\cdot \\frac{\\partial w_l}{\\partial w_{l-1}} \\cdot \\frac{\\partial w_{l-1}}{\\partial w_{l-2}} \\cdot \\frac{\\partial w_{l-2}}{\\partial x_{l-2}} \\cdots \\frac{\\partial w_{j+1}}{\\partial w_j} \\cdot \\frac{\\partial w_j}{\\partial x_j} \\quad \\left( \\frac{\\partial l}{\\partial w_{j+1}} = \\frac{\\partial l}{\\partial w_j} \\cdot \\frac{\\partial w_j}{\\partial w_{j+1}} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "Notice that there is a lot of reuse of expressions, which means we don't have to recompute things over and over. In particular\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial l}{\\partial w_{l-1}} &= \\frac{\\partial l}{\\partial w_l} \\cdot \\frac{\\partial w_l}{\\partial w_{l-1}}, \\quad\n",
    "\\frac{\\partial l}{\\partial w_j} = \\frac{\\partial l}{\\partial w_{j+1}} \\cdot \\frac{\\partial w_{j+1}}{\\partial w_j},\n",
    "\\end{align*}\n",
    "\n",
    "and in general\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial l}{\\partial w_{j-1}} = \\frac{\\partial l}{\\partial w_j} \\cdot \\frac{\\partial w_j}{\\partial w_{j-1}}\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\frac{\\partial l}{\\partial w_j}$ will have been computed at the layer above. This is another key piece of backpropagation!\n",
    "\n",
    "The only thing left to compute is $\\frac{\\partial w_j}{\\partial x_j}$ - this is now just an exercise in calculus, so we'll not work it out in class, but the chain rules will provide tools to forge with further automation for those interested.\n",
    "\n",
    "Optional:\n",
    "We apply our chain rule $(w_j = X_j [G_j:1])$ to get\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial w_j}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} G(X_j [G_{j-1}:1]) = \\frac{\\partial G}{\\partial w} \\cdot \\frac{\\partial w}{\\partial x_j}\n",
    "\\end{equation*}\n",
    "\n",
    "Now for $G(w) = \\begin{bmatrix} G(w_1) \\\\ \\vdots \\\\ G(w_{p+1}) \\end{bmatrix}$, $\\frac{\\partial G}{\\partial w} = \\begin{bmatrix} G'(w_1) \\\\ \\vdots \\\\ G'(w_{p+1}) \\end{bmatrix}$. Next, we need to find\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial w}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} (X_j [G_{j-1}:1]). \\text{ This can be computed using matrix/linear algebra (tedious).}\n",
    "\\end{equation*}\n",
    "\n",
    "We won't work it out, but note that it can be found efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/10_Ch_11_PCA_Apps/121-Apps.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
