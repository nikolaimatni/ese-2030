{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 13.2 Gradient Descent\n",
    "subject:  Optimization\n",
    "subtitle: \n",
    "short_title: 13.2 Gradient Descent\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: \n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/12_Ch_13_Optimization/142-Gradient_Descent.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 21 - An introduction to unconstrained optimization, gradient descent, and Newtonâ€™s method.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be found in Chapter 12 in ROB101 textbook by Jesse Grizzle \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Our intuition so far is that we should try to \"walk downhill\" and that the negative gradient $-\\nabla f(\\vv x)$ tells us the steepest direction of descent at point $\\vv x$. Can we turn this into an algorithm for minimizing (at least locally) a cost function $f(\\vv x)$?\n",
    "\n",
    "This intuition is precisely the motivation behind the _gradient descent algorithm_, which starting with an initial guess $\\vv x^{(0)}$, iteratively updates the current best guess $\\vv x^{(k)}$ of $\\vv x^*$ according to:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{GD}\n",
    "\\vv x^{(k+1)} = \\vv x^{(k)} - s\\nabla f(\\vv x^{(k)}), \\quad \\text{for } k=0,1,2,\\ldots \\quad (\\text{GD})\n",
    "\\end{equation}\n",
    "\n",
    "where $s>0$ is called the _step size_. The update rule [(GD)](#GD) moves you in the direction of a local minimum if $s>0$, but be careful, because if $s$ is too large, you can overshoot (we'll see more about this later).\n",
    "\n",
    "Because we know that $\\nabla f(\\vv x^*)=0$ if $\\vv x^*$ is a local minima, we can use the norm of the gradient as a stopping criterion, i.e., if $\\|\\nabla f(\\vv x^{(k)})\\| \\leq \\epsilon$ for some small $\\epsilon>0$, we stop updating our iterate because $\\vv x^{(k)}$ is \"close enough\" to $\\vv x^*$ (typical choice of $\\epsilon$ are $10^{-4}$ or $10^{-6}$, depending on how precise of a solution is required).\n",
    "\n",
    "Before looking at some examples of [(GD)](#GD) in action, let's try to get some intuition as to why it might work. Suppose we are currently at $\\vv x^{(k)}$: let's form a _linear approximation_ of $f(\\vv x)$ near $\\vv x^{(k)}$ using its _first-order Taylor series approximation_:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{TS}\n",
    "f(\\vv x) \\approx \\underbrace{f(\\vv x^{(k)})}_{\\text{base point}} + \\underbrace{\\nabla f(\\vv x^{(k)})^T}_{\\text{how } f \\text{ changes at } x^{(k)}}\\underbrace{(\\vv x-\\vv x^{(k)})}_{\\text{direction of change}} \\quad (\\text{TS})\n",
    "\\end{equation}\n",
    "\n",
    ":::{figure}../figures/14-GD.jpg\n",
    ":label:GD_fig\n",
    ":alt:GD\n",
    ":width: 500px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    "As you can see from the [figure](#GD_fig), [(TS)](#TS) is a very good approximation of $f(\\vv x)$ when $\\vv x$ is not too far from $\\vv x^{(k)}$, but gets worse as we move further away.\n",
    "\n",
    "Let's use [(TS)](#TS) to define our next point $\\vv x^{(k+1)}$ so that $f(\\vv x^{(k+1)}) < f(\\vv x^{(k)})$. If we define $\\Delta \\vv x^{(k)} = \\vv x^{(k+1)} - \\vv x^{(k)}$, then evaluating [(TS)](#TS) at point $\\vv x^{(k+1)}$ becomes\n",
    "\n",
    "$$\n",
    "f(\\vv x^{(k+1)}) - f(\\vv x^{(k)}) \\approx \\nabla f(\\vv x^{(k)})^T \\Delta \\vv x^{(k)} \\left(= \\langle \\nabla f(\\vv x^{(k)}), \\Delta \\vv x^{(k)} \\rangle\\right)\n",
    "$$\n",
    "\n",
    "so that if we want $f(\\vv x^{(k+1)}) < f(\\vv x^{(k)})$, then we should find a nearby $\\vv x^{(k+1)}$ such that\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{good_neighb}\n",
    "\\nabla f(\\vv x^{(k)})^T \\Delta \\vv x^{(k)} < 0.\n",
    "\\end{equation}\n",
    "\n",
    "Now, assuming that $\\nabla f(\\vv x^{(k)}) \\neq \\vv 0$ (so we're not at a local extremum), a clear choice for $\\Delta \\vv x^{(k)}$ is $-s\\nabla f(\\vv x^{(k)})$ for $s>0$ a step size chosen small enough so that [(TS)](#TS) is a good approximation. In that case, we have\n",
    "\n",
    "$$\n",
    "\\nabla f(\\vv x^{(k)})^T \\Delta \\vv x^{(k)} = -s\\|\\nabla f(\\vv x^{(k)})\\|^2 < 0.\n",
    "$$\n",
    "\n",
    "In general though, any choice $\\Delta \\vv x^{(k)}$ such that [](#good_neighb) holds is a valid descent direction. Geometrically, this is illustrated in the picture below:\n",
    "\n",
    "\n",
    ":::{figure}../figures/14-GD_dir.jpg\n",
    ":label:GD_dir\n",
    ":alt:GD direction\n",
    ":width: 400px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    ":::{prf:example}\n",
    ":label: eg1\n",
    "Let's use gradient descent to minimize $f(\\vv x) = \\frac{1}{2}\\|\\vv x\\|^2$. This is a silly example, but one we can easily compute iterates by hand for. Here $\\nabla f(\\vv x) = \\vv x$, so our descent direction is $-\\nabla f(\\vv x) = -\\vv x$. Let's use $\\vv x\\in\\mathbb{R}^2$ and an initial guess of $\\vv x^{(0)} = (1,1)$. We'll use a step size of $s = \\frac{1}{2}$. Then\n",
    "\n",
    "$$\n",
    "\\vv x^{(1)} = \\bm 1 \\\\ 1\\em, \\vv x^{(1)} = \\vv x^{(0)} - \\frac{1}{2}\\nabla f(\\vv x^{(0)}) = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \\frac{1}{2}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\frac{1}{2}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\frac{1}{2}\\vv x^{(0)} \\\\\n",
    "\\vv x^{(2)} = x^{(1)} - \\frac{1}{2}\\nabla f(\\vv x^{(1)}) = \\frac{1}{2}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \\left(\\frac{1}{2}\\right)^2 \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\frac{1}{4}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\left(\\frac{1}{2}\\right)^2 \\vv x^{(0)} \\\\\n",
    "\\ldots \\vv x^{(k)} = \\left(\\frac{1}{2}\\right)^k \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}.\n",
    "$$\n",
    "So we see that $$\\vv x^{(k)} \\to \\vv x^* = \\vv 0$$ exponentially quickly \n",
    "at rate $\\left(\\frac{1}{2}\\right)^k$.\n",
    ":::\n",
    "\n",
    "**TO DO**: Example OUTLINE NOTES PLEASE ADD A $||A\\vv x-\\vv b||^2$ EXAMPLE THAT'S MORE INTERESTING OR TO DO MOSTLY COMPUTATIONAL SHOW A GOOD STEPSIZE CHOICE AND ONE WHERE IT DIVERGES."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zig-zags and What to do About Them\n",
    "\n",
    "Let's consider a very simple optimization over $\\vv x\\in\\mathbb{R}^2$ with cost function\n",
    "\n",
    "$$\n",
    "f(\\vv x_1, \\vv x_2) = \\frac{1}{2}(x^2 + by^2)\n",
    "$$\n",
    "\n",
    "where we'll let $0<b\\leq 1$ vary. The optimal solution is obviously $(x^*, y^*) = (0,0)$ but we'll use this to illustrate how gradient descent can get you into trouble sometimes.\n",
    "\n",
    "Suppose we run gradient descent on $f$, and we further allow ourselves to pick the best possible step size $s_k$ at each iteration, i.e., we choose step size\n",
    "\n",
    "$$\n",
    "s_k = \\argmin_{s > 0} f(\\vv x^{(k)} - s\\nabla f(\\vv x^{(k)}))\n",
    "$$\n",
    "\n",
    "and then update $\\vv x^{(k+1)} = \\vv x^{(k)} - s_k \\nabla f(\\vv x^{(k)})$.\n",
    "\n",
    "This is called _exact line search_ for selecting the step size, and is widely used in practice. If we use this choice of step size $s_k$, then it is possible to write an explicit formula for our iterates $(x^{(k)}, y^{(k)})$ as we progress down the bowl. Starting at $(x^{(0)},y^{(0)})=(b,1)$, we get\n",
    "\\begin{equation}\n",
    "\\label{line-search}\n",
    "x^{(k)} = b \\left(\\frac{b-1}{b+1}\\right)^k, \\quad y^{(k)} = \\left(\\frac{1-b}{1+b}\\right)^k, \\quad f(x^{(k)},y^{(k)}) = \\left(\\frac{1-b}{1+b}\\right)^{2k} f(x^{(0)},y^{(0)}).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "If $b=1$, which corresponds to a function with level sets that are perfect circles,\n",
    "we succeed immediately in one step $(x^{(1)} = y^{(1)} = 0)$. This is because the gradient always points directly to the optimal point $(0,0)$:\n",
    "\n",
    ":::{figure}../figures/14-point.jpg\n",
    ":label:point\n",
    ":alt:point\n",
    ":width: 400px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    "The real purpose of this example is seen when $b$ is small. The crucial ratio in\n",
    "equation [](#line-search) is\n",
    "$$\n",
    "r = \\frac{b-1}{b+1},\n",
    "$$\n",
    "\n",
    "If $r$ is small, $(x^{(k)},y^{(k)})$ converges to $(0,0)$ very quickly. However, if $r$ is close to 1, then this convergence is very slow. For example, if $b = \\frac{1}{10}$, then $r = \\frac{9}{11}$; for $b = \\frac{1}{100}$, $r = \\frac{99}{101}$.\n",
    "\n",
    "This means we need to take many more gradient steps to get close to $\\vv x^* = (0,0)$. The [picture below](#zigzag) highlights what's going wrong: for small $b$, the level sets become elongated ellipses, so that following gradients leads to us zig-zagging our way to the origin instead of taking a straight path. It is this zig-zagging that causes slow convergence.\n",
    "\n",
    ":::{figure}../figures/14-zigzag.jpg\n",
    ":label:zigzag\n",
    ":alt:zigzag\n",
    ":width: 500px\n",
    ":align: center\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So what's going wrong here? If we write $F(x)$ as a quadratic form, it is:\n",
    "\n",
    "\\[\n",
    "F(x) = \\begin{bmatrix} x^T & 1 \\end{bmatrix} \\begin{bmatrix} 0 & 0 \\\\ 0 & b \\end{bmatrix} \\begin{bmatrix} x \\\\ 1 \\end{bmatrix} = x^T b x\n",
    "\\]\n",
    "\n",
    "Notice that the condition number of the matrix $K$ is precisely $\\frac{1}{b}$, which is large for\n",
    "small $b$. This means that one direction (in this case the x-axis) is penalized much more\n",
    "than the other (the y-axis). This leads to stretched out level sets, which leads to\n",
    "zig-zags and slow convergence. How can we fix this?\n",
    "\n",
    "Newton's Method (continued)\n",
    "\n",
    "To derive the gradient descent method (GD), we used a first order approximation of\n",
    "$F(x)$ near $x^{(k)}$ to figure out what direction we should move in. What if in this example\n",
    "we saw steeper and shallower directions where the function changes quickly (look at the contours of the\n",
    "function in the figure; they are all very thin ellipses)? Isn't one step enough?\n",
    "\n",
    "We should also account for how quickly the gradient changes: we need to compute the\n",
    "\"gradient of the gradient\", a.k.a. the Hessian of $F$ at $x$. The Hessian of a function $f: \\mathbb{R}^n \\to \\mathbb{R}$\n",
    "is an $n \\times n$ symmetric matrix with entries given by 2nd order partial derivatives of $f$:\n",
    "\n",
    "\\[\n",
    "[\\nabla^2 f(x)]_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}(x)\n",
    "\\]\n",
    "\n",
    "The Hessian tells us how quickly the gradient changes in the same way $f'(x)$ tells us\n",
    "how quickly $f(x)$ changes for a scalar function $f(x)$. The Hessian lets us make\n",
    "a second order Taylor series approximation to our function $F(x)$ near our current\n",
    "guess $x^{(k)}$:\n",
    "\n",
    "\\[\n",
    "F(x) \\approx F(x^{(k)}) + \\nabla f(x^{(k)})^T(x-x^{(k)}) + (x-x^{(k)})^T \\nabla^2 f(x^{(k)})(x-x^{(k)}), \\quad (6)\n",
    "\\]\n",
    "\n",
    "which provides a local quadratic approximation to $F(x)$ near $F(x^{(k)})$:\n",
    "\n",
    "\\begin{tikzpicture}\n",
    "\\draw[->] (-2,0) -- (2,0) node[right] {$x$};\n",
    "\\draw[->] (0,-0.5) -- (0,2) node[above] {$F(x)$};\n",
    "\\draw[thick, blue] plot[domain=-2:2,samples=100] (\\x,{0.5*\\x*\\x});\n",
    "\\draw[thick, red] plot[domain=-1:1,samples=100] (\\x,{0.5+0.5*\\x*\\x});\n",
    "\\node[blue] at (1.5,1.5) {$F(x)$};\n",
    "\\node[red] at (-1.5,1.5) {quad approx};\n",
    "\\fill[blue] (0,0.5) circle (2pt);\n",
    "\\node[right] at (0,0.5) {$F(x^{(k)})$};\n",
    "\\end{tikzpicture}\n",
    "\n",
    "As before, if we let $\\Delta x^{(k)} = x^{(k+1)} - x^{(k)}$, we can rewrite (6) as\n",
    "\n",
    "\\[\n",
    "F(x^{(k+1)}) - F(x^{(k)}) \\approx (\\Delta x^{(k)})^T \\nabla F(x^{(k)}) + \\frac{1}{2} \\nabla F(x^{(k)})^T \\Delta x^{(k)}. \\quad (7a)\n",
    "\\]\n",
    "\n",
    "Since we want to make $F(x^{(k+1)}) - F(x^{(k)})$ as small as possible, it makes sense\n",
    "to pick $\\Delta x^{(k)}$ to minimize the RHS of (7a), which is another minimization problem!\n",
    "\n",
    "We'll focus on the setting where $\\nabla^2 F(x^{(k)})$ is positive definite - this corresponds to\n",
    "settings where our function is convex. In this case, the RHS of (7a) is a positive definite\n",
    "quadratic function, which is minimized at:\n",
    "\n",
    "\\[\n",
    "\\Delta x^{(k)} = - \\nabla^2 F(x^{(k)})^{-1} \\nabla F(x^{(k)}).\n",
    "\\]\n",
    "\n",
    "Using this descent direction instead of $-\\nabla F(x^{(k)})$ yields Newton's Method:\n",
    "\n",
    "\\[\n",
    "x^{(k+1)} = x^{(k)} - \\nabla^2 F(x^{(k)})^{-1} \\nabla F(x^{(k)}).\n",
    "\\]\n",
    "\n",
    "The idea behind Newton's Method is to \"unstretch\" the stretched out directions,\n",
    "so that to our algorithm, the level sets of a function are locally circles. If\n",
    "we look at our test example above, note that:\n",
    "\n",
    "\\[\n",
    "\\nabla^2 F(x) = \\begin{bmatrix} 1 & \\\\ & b \\end{bmatrix} \\Rightarrow \\nabla^2 F(x)^{-1} = \\begin{bmatrix} 1 & \\\\ & \\frac{1}{b} \\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "so that for $x^{(0)} = (b,1)$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "x^{(1)} &= x^{(0)} - \\nabla^2 F(x)^{-1} \\nabla F(x) \\\\\n",
    "&= \\begin{bmatrix} b \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 1 & \\\\ & \\frac{1}{b} \\end{bmatrix} \\begin{bmatrix} b \\\\ b \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "i.e., we converge in one step no matter what the choice of $b$ is in $F(x) = \\frac{1}{2}(x^2+by^2)$!\n",
    "\n",
    "The cost of this fast convergence though is that at each update, we need to solve\n",
    "a linear system of equations of the form\n",
    "\n",
    "\\[\n",
    "\\nabla^2 F(x^{(k)}) \\Delta x^{(k)} = \\nabla F(x^{(k)})\n",
    "\\]\n",
    "\n",
    "which may be expensive if $x$ is a high-dimensional vector. It is for this reason that\n",
    "gradient descent based methods are the predominant methods used in machine learning,\n",
    "where oftentimes the dimensionality of $x$ can be on the order of millions or\n",
    "billions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/12_Ch_13_Optimization/142-Gradient_Descent.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
