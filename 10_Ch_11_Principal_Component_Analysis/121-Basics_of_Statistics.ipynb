{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 11.1 Basics of Statistics\n",
    "subject:  Principal Component Analysis (PCA)\n",
    "subtitle: \n",
    "short_title: 11.1 Basics of Statistics\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: \n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/10_Ch_11_PCA_Apps/121-Basics_of_Statistics.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 19 - Principal Component Analysis with Applications to Imaging and Data Compression.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be found in LAA 7.5 and ALA 8.8.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation: Satellite Imagery\n",
    "\n",
    "\n",
    "We start with a motivating application from satellite imagery analysis. The [Landsat](https://landsat.gsfc.nasa.gov/) satellites are a pair of imaging satellites that record images of terrain and coastlines. These satellites cover almost every square mile of the Earth's surface every 16 days.\n",
    "\n",
    "Satellite sensors acquire seven simultaneous images of any given region, with each sensor recording energy from separate wavelength bands: three in the visible light spectrum and four in the infrared and thermal bands.\n",
    "\n",
    "Each image is digitized and stored as a rectangular array of numbers, with each number representing the signal intensity at the corresponding _pixel_. Each of the seven images is one channel of a _multichannel or multispectral image_.\n",
    "\n",
    "The seven Landsat images of a given region typically contain a lot of redundant information, as some features will appear across most channels. However, other features, because of their color or temperature, may only appear in one or two channels. A goal of multispectral image processing is to view the data in a way that extracts information better than studying each image separately.\n",
    "\n",
    "One approach, called _Principal Component Analysis (PCA)_, seeks to find a special linear combination of the data that takes a weighted combination of all seven images into just one or two images. Importantly, we want these one or two composite images. or _principal components_ to capture as much of the scene variance (features) as possibl; in particular, features should be more visible in the composite images than any of the original individual ones.\n",
    "\n",
    "This idea, which we'll explore in detail today, is illustrated with some Landsat imagery taken over Railroad Valley Nevada.\n",
    "\n",
    ":::{figure}../figures/12-railroad.jpg\n",
    ":label:railroad\n",
    ":alt:Railroad Satellite Imagery\n",
    ":width: 600px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    "Images from three Landsat spectral bands are shown in [(a)-(c)](#railroad); the total information in these images is \"rearranged\" into the three principal components in [(d)-(f)](#railroad). The first component, (d), \"explains\" 93.5\\% of the scene features (or variance) found in the original data. In this way, we could compress all of the original data to the single image (d) with only a 6.5\\% loss of scene variance.\n",
    "\n",
    "PCA can in general be applied to any data that consists of lists of measurements made on a collection of objects or individuals, including data mining, machine learning, image processing, speech recognition, facial recognition, and health informatics. As we'll see next, the way in which these \"special combinations\" of measurements are computed are via the singular vectors of an _observation matrix_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Observation Matrix\n",
    "\n",
    "Let $\\mathbf{x}_j \\in \\mathbb{R}^p$ denote an observation vector obtained from measurement $j$, and suppose that $j=1,\\ldots,N$ measurements are obtained. The _observation matrix $X \\in \\mathbb{R}^{p \\times N}$_ is a $p \\times N$ matrix with $j^{th}$ column equal to the $j^{th}$ measurement vector $\\mathbf{x}_j$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{obs_mat}\n",
    "X = \\bm \\mathbf{x}_1 & \\mathbf{x}_2 & \\cdots & \\mathbf{x}_N\\em \\in \\mathbb{R}^{p \\times N}\n",
    "\\end{equation}\n",
    "\n",
    "::::{prf:example}\n",
    ":label: eg_1\n",
    "Suppose that $\\mathbf{x}_j \\in \\mathbb{R}^2$ is a two dimensional data given by the weight and height of the $j^{th}$ student at Penn: $\\mathbf{x}_j = (w_j, h_j) \\in \\mathbb{R}^2$. Then if measurements are obtained from $N$ students, the observation matrix $X \\in \\mathbb{R}^{2 \\times N}$ has the form:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "w_1 & w_2 & \\cdots & w_N \\\\\n",
    "\\underbrace{h_1}_{\\vv x_1} & \\underbrace{h_2}_{\\vv x_2} & \\cdots & \\underbrace{h_N}_{\\vv x_N}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The set of observation vectors can be visualized as a two-dimensional _scatter plot_:\n",
    "\n",
    ":::{figure}../figures/12-scatter.jpg\n",
    ":label:scatter\n",
    ":alt:scatter plot\n",
    ":width: 300px\n",
    ":align: center\n",
    ":::\n",
    "::::\n",
    "\n",
    "::::{prf:example}\n",
    ":label: eg_2 \n",
    "The three images [(a)-(c)](#railroad) above can be thought of as _one image_ composed of _three spectral components_, as each image gives information about the same region. We can capture this mathematically by associating a vector in $\\mathbb{R}^3$ to each pixel (one small area of the image) that lists the intensity for that pixel in the three spectral bands. Typically the image is 2000 $\\times$ 2000 pixels, so there are 4 million pixels in the image. The observation matrix for this data is a matrix with 3 rows and 4 million columns. The data can thus be visualized as a scatter plot of 4 million points in $\\mathbb{R}^3$ (see [Figure below](#scatter_satellite) for a synthetic example).\n",
    ":::{figure}../figures/12-scatter_satellite.jpg\n",
    ":label:scatter_satellite\n",
    ":alt:scatter plot satellite\n",
    ":width: 300px\n",
    ":align: center\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean and Covariance\n",
    "\n",
    "To understand PCA, we need to understand some basic concepts from statistics. We will review the _mean_ and _covariance_ of a set of observations $\\vv x_1, \\ldots, \\vv x_N$. For our purposes, these will simply be things we can compute from the data, but you should be aware that these are well motivated quantities from a statistical perspective.: you will learn more about this in ESE 3010, STAT 4300 or ESE 4020.\n",
    "\n",
    "Let's start with an observation matrix $X \\in \\mathbb{R}^{p\\times N}$, with columns $\\mathbf{x}_1,\\ldots,\\mathbf{x}_N \\in \\mathbb{R}^p$. \n",
    "\n",
    ":::{prf:definition} Sample Mean/Centroid\n",
    ":label: mean_defn\n",
    "The _sample mean $\\mathbf{m}$_ of the observation vectors $\\vv x_1, \\ldots, \\vv x_N$ is given by\n",
    "\n",
    "$$\n",
    "\\mathbf{m} = \\frac{1}{N}\\left(\\mathbf{x}_1 + \\cdots + \\mathbf{x}_N\\right) = \\frac{1}{N}\\sum_{j=1}^N \\mathbf{x}_j.\n",
    "$$\n",
    "\n",
    "Another name for the sample mean is the _centroid_ of the data, which we encountered when we learned about the k-means algorithm.\n",
    ":::\n",
    "\n",
    "Since PCA is interested in directions of (maximal) variation in our data, it makes sense to subtract off the mean $\\mathbf{m}$, as it captures the average behavior of our data set. To that end, define the _centered observations_ to be\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}}_j = \\mathbf{x}_j - \\mathbf{m}, \\quad j=1,\\ldots,N,\n",
    "$$\n",
    "\n",
    "and the _centered or de-meaned observation matrix_\n",
    "\n",
    "$$\n",
    "\\hat{X} = \\bm \\hat{\\mathbf{x}}_1 & \\hat{\\mathbf{x}}_2 & \\cdots & \\hat{\\mathbf{x}}_N\\em.\n",
    "$$\n",
    "\n",
    "For example, [Fig. 3](#centered) below shows a centered version of the weight/height data illustrated in [Fig. 1](#scatter):\n",
    "\n",
    ":::{figure}../figures/12-centered.jpg\n",
    ":label:centered\n",
    ":alt:scatter plot centered\n",
    ":width: 300px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    ":::{prf:definition} Sample Covariance Matrix\n",
    ":label: var_defn\n",
    "Finally, we define the _sample covariance matrix $S \\in \\mathbb{R}^{p\\times p}$_ as\n",
    "\n",
    "$$\n",
    "S = \\frac{1}{N} \\hat{X}\\hat{X}^T.\n",
    "$$\n",
    ":::\n",
    "\n",
    "Since any matrix of the form $AA^T$ is positive semidefinite (can you see why?), so is $S$. Note sometimes $\\frac{1}{N-1}$ is used as normalization; this is motivated for statistical considerations beyond the scope of this course (it leads to $S$ being an unbiased estimator of the \"true\" covariance of the data). We will just use $\\frac{1}{N}$.\n",
    "\n",
    ":::{prf:example}\n",
    ":label: eg_covariance\n",
    "Three measurements are made on each of four individuals in a random sample from a population. The observation vectors are:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\quad\n",
    "\\mathbf{x}_2 = \\begin{bmatrix} 4 \\\\ 2 \\\\ 13 \\end{bmatrix}, \\quad\n",
    "\\mathbf{x}_3 = \\begin{bmatrix} 7 \\\\ 8 \\\\ 1 \\end{bmatrix}, \\quad\n",
    "\\mathbf{x}_4 = \\begin{bmatrix} 8 \\\\ 4 \\\\ 5 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The sample mean is $\\mathbf{m} = \\frac{1}{4}\\left(\\mathbf{x}_1+\\mathbf{x}_2+\\mathbf{x}_3+\\mathbf{x}_4\\right) = \\begin{pmatrix} 5 \\\\ 4 \\\\ 5 \\end{pmatrix}$.\n",
    "\n",
    "The centered observations $\\hat{\\mathbf{x}}_j = \\mathbf{x}_j - \\mathbf{m}$ are then\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}}_1 = \\begin{bmatrix} -4 \\\\ -2 \\\\ -4 \\end{bmatrix}, \\quad\n",
    "\\hat{\\mathbf{x}}_2 = \\begin{bmatrix} -1 \\\\ -2 \\\\ 8 \\end{bmatrix}, \\quad\n",
    "\\hat{\\mathbf{x}}_3 = \\begin{bmatrix} 2 \\\\ 4 \\\\ -4 \\end{bmatrix}, \\quad\n",
    "\\hat{\\mathbf{x}}_4 = \\begin{bmatrix} 3 \\\\ 0 \\\\ 0 \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "and the centered observation matrix is\n",
    "\n",
    "$$\n",
    "\\hat{X} =\\begin{bmatrix}\n",
    "-4 & -1 & 2 & 3 \\\\\n",
    "-2 & -2 & 4 & 0 \\\\\n",
    "-4 & 8 & -4 & 0\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The sample covariance matrix is\n",
    "\n",
    "$$\n",
    "S = \\frac{1}{4} \\hat{X}\\hat{X}^T = \\begin{bmatrix}\n",
    "7.5 & 4.5 & 0 \\\\\n",
    "4.5 & 6 & -6 \\\\\n",
    "0 & -6 & 24\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be wondering what the entries $s_{ij}$ of the covariance matrix $S$ mean. Let's take a bit of a closer look. We'll consider a small example where the observations $\\mathbf{x}_j \\in \\mathbb{R}^2$ are two dimensional, and assume we have $N=3$ observations. Let the first measurement be $a \\in \\mathbb{R}$ and the second $b \\in \\mathbb{R}$, so that $\\mathbf{x}_i = (a_i, b_i) \\in \\mathbb{R}^2$ and the centered observation is $\\hat{\\mathbf{x}}_i = (\\hat{a}_i, \\hat{b}_i) \\in \\mathbb{R}^2$. Our centered observation matrix is then\n",
    "\n",
    "$$\n",
    "\\hat{X} = \\bm \\hat{a}_1 & \\hat{a}_2 & \\hat{a}_3 \\\\ \n",
    "           \\hat{b}_1 & \\hat{b}_2 & \\hat{b}_3 \\em  = \\bm \\hat{\\mathbf{a}}^T \\\\ \\hat{\\mathbf{b}}^T \\em,\n",
    "$$\n",
    "\n",
    "where we defined $\\hat{\\mathbf{a}} = (\\hat{a}_1, \\hat{a}_2, \\hat{a}_3) \\in \\mathbb{R}^3$ and $\\hat{\\mathbf{b}} = (\\hat{b}_1, \\hat{b}_2, \\hat{b}_3)$ as the vectors in $\\mathbb{R}^3$ containing all of the centered first and second measurements, respectively.\n",
    "\n",
    "Then, we can write our sample covariance matrix as:\n",
    "\n",
    "$$\n",
    "S = \\frac{1}{3} \\hat{X}\\hat{X}^T = \\frac{1}{3} \\bm \\hat{\\mathbf{a}}^T \\\\ \\hat{\\mathbf{b}}^T \\em \\bm \\hat{\\mathbf{a}} & \\hat{\\mathbf{b}} \\em  =  \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\|\\hat{\\mathbf{a}}\\|^2}{3} & \\frac{\\hat{\\mathbf{a}}^T\\hat{\\mathbf{b}}}{3} \\\\\n",
    "\\frac{\\hat{\\mathbf{a}}^T\\hat{\\mathbf{b}}}{3} & \\frac{\\|\\hat{\\mathbf{b}}\\|^2}{3}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The diagonal entry $s_{11} = \\frac{\\|\\hat{\\mathbf{a}}\\|^2}{3}$ is called the variance of measurement 1.\n",
    "\n",
    "Expanding it out:\n",
    "\n",
    "$$\n",
    "s_{11} = \\frac{\\|\\hat{\\mathbf{a}}\\|^2}{3} &= \\frac{1}{3}(\\hat{a}_1^2 + \\hat{a}_2^2 + \\hat{a}_3^2) \\\\\n",
    "&= \\frac{1}{3}((a_1-m_1)^2 + (a_2-m_2)^2 + (a_3-m_3)^2)\n",
    "$$\n",
    "\n",
    "we see that $s_{11}$ captures how much the first measurement $a_i$ deviates from its mean value $m_i$, on average, i.e., it measures how much $a_i$ varies relative to its mean. Similarly, $s_{22} = \\frac{\\|\\hat{\\mathbf{b}}\\|^2}{3}$ is the variance of measurement 2.\n",
    "\n",
    "Now let's look at the off-diagonal term $s_{12} = s_{21} = \\frac{\\hat{\\mathbf{a}}^T\\hat{\\mathbf{b}}}{3}$. Recall from our work on inner products that $\\hat{\\mathbf{a}}^T\\hat{\\mathbf{b}} = \\|\\hat{\\mathbf{a}}\\| \\|\\hat{\\mathbf{b}}\\| \\cos \\theta$, where $\\theta$ is the angle between $\\hat{\\mathbf{a}}$ and $\\hat{\\mathbf{b}}$. We can view\n",
    "\n",
    "$$\n",
    "\\cos \\theta = \\frac{\\hat{\\mathbf{a}}^T\\hat{\\mathbf{b}}}{\\|\\hat{\\mathbf{a}}\\| \\|\\hat{\\mathbf{b}}\\|}\n",
    "$$\n",
    "\n",
    "as a measure of how well aligned, or _correlated_: if $\\hat{\\mathbf{a}}$ and $\\hat{\\mathbf{b}}$ are parallel, $\\cos \\theta = 1$ or $-1$, and if $\\hat{\\mathbf{a}}$ and $\\hat{\\mathbf{b}}$ are perpendicular, $\\cos \\theta = 0$. This lets us interpret $s_{12} = \\frac{\\hat{\\mathbf{a}}^T\\hat{\\mathbf{b}}}{3}$, which is proportional to $\\cos \\theta$, as a measure of how similarly $\\hat{\\mathbf{a}}$ and $\\hat{\\mathbf{b}}$ deviate from their means: if $\\hat{\\mathbf{a}}^T\\hat{\\mathbf{b}}$ is positive, this means $\\hat{\\mathbf{a}}$ and $\\hat{\\mathbf{b}}$ tend to move up or down together; if it is negative they tend to move in opposite directions; and if it is small (or zero), $\\hat{\\mathbf{a}}$ and $\\hat{\\mathbf{b}}$ tend to move independently of each other. Since $s_{12}$ captures how the 1st and 2nd measurements vary with each other, it is called their _covariance_.\n",
    "\n",
    "Finally, although we worked out the concepts for $\\vv x_j \\in \\mathbb{R}^p$ and $j=1,2,3,$ These concepts extend naturally to the general setting:\n",
    "\n",
    "- $S_{ii}$ = variance of measurement $i$ across measurements $j=1,\\ldots,N$\n",
    "- $S_{kl}$ = cvariance of measurements $k$ and $l$ across measurements $j=1,\\ldots,N$.\n",
    "\n",
    ":::{prf:example}  Correlated, anticorrelated, and uncorrelated vectors (Fig 3.8 from VLMS)\n",
    ":label: basics-ex1\n",
    "\n",
    "Below, we 3 pairs of time series (labelled $a_k$ and $b_k$), exhibiting positive correlation, negative correlation, and little/no correlation respectively.\n",
    "\n",
    "![alt text](../figures/12-correlated_vectors.png)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/10_Ch_11_PCA_Apps/121-Basics_of_Statistics.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
