{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 12.2 Matrix Norm\n",
    "subject:  Low Rank Approxmiation\n",
    "subtitle: \n",
    "short_title: 12.2 Matrix Norm\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: \n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/11_Ch_12_Low_Rank/132-MatrixNorm.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 20 - Low-Rank Matrix Approximations via the SVD with applications to matrix completion and recommender systems.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page can be found in [Lecture 9](https://web.stanford.edu/class/cs168/l/l9.pdf) from Stanford CS168 course.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- what is a matrix norm\n",
    "- the definition of the Frobenious norm and examples of computing the same\n",
    "- some important properties of the Frobenius norm\n",
    "- how the Frobenious norm is used as an approximation error for the Low Rank approximation of a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Matrix Norm\n",
    "\n",
    "For an $m\\times n$ matrix $M\\in\\mathbb{R}^{m\\times n}$, let $\\hat{M}$ be a low-rank approximation of $M$, and define the approximation error as $E = M-\\hat{M}$. Intuitively, a \"good\" approximation will lead to \"small\" error $E$. But we need to quantify the \"size\" of $E\\in\\mathbb{R}^{m\\times n}$. We know that for vectors $\\vv x\\in\\mathbb{R}^n$, the right way to quantify the size of $\\vv x$ was through its norm $\\|\\vv x\\|$, where $\\|\\cdot\\|$ is a function that needs to satisfy the axioms of a norm.\n",
    "\n",
    "1. $\\|a \\vv x\\| = |a| \\|\\vv x\\|$ for all $\\vv x\\in\\mathbb{R}^n$, $a \\in\\mathbb{R}$\n",
    "2. $\\|\\vv x\\| \\geq 0$ for all $\\vv x\\in\\mathbb{R}^n$, with $\\|\\vv x\\|=0$ if and only if $\\vv x=0$\n",
    "3. $\\|\\vv x+ \\vv y\\| \\leq \\|\\vv x\\| + \\|\\vv y\\|$ for all $\\vv x,\\vv y\\in\\mathbb{R}^n$\n",
    "\n",
    "It turns out we can define functions on the vector space of $m\\times n$ matrices that satisfy these same properties: these are called _matrix norms_. We'll introduce one of them here that is particularly relevant to low-rank matrix approximations, but be aware that just as for vectors there are many kinds of matrix norms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Frobenius Norm\n",
    "\n",
    ":::{prf:definition} Frobenious Norm\n",
    ":label: frob-norm\n",
    "The _Frobenius norm_ of an $m\\times n$ matrix $M\\in\\mathbb{R}^{m\\times n}$ simply computes the Euclidean norm of $M$ as if it were an $mn$ vector:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{frob-norm_eqn}\n",
    "\\|M\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n m_{ij}^2} \\quad \\text{(F)}\n",
    "\\end{equation}\n",
    ":::\n",
    "\n",
    ":::{prf:example}\n",
    ":label: eg_fn\n",
    "\n",
    "$M = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}$ has $\\|M\\|_F^2 = 1^2 + 2^2 + 3^2 + 4^2 = 30$\n",
    "\n",
    "which is the same as $\\|\\text{vec}(M)\\|_2^2 = \\left\\|\\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}\\right\\|_2^2$.\n",
    ":::\n",
    "\n",
    "We need a couple of properties of the Frobenius norm before we can connect the SVD to low-rank matrix approximation.\n",
    "\n",
    "1. **Property 1: For $A \\in \\mathbb{R}^{n \\times n}$ a square matrix, $\\|A\\|_F = \\|A^T\\|_F$**\n",
    "\n",
    "This isn't too hard to check from the definition of [(F)](#frob-norm_eqn): taking the transpose just swaps the role of $(i,j)$ in the sum, but you still end up adding together the square of all entries in $A$, which are the same as the square of all of the entries in $A^T$.\n",
    "\n",
    "2. **Property 2: If $Q \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix and $A \\in \\mathbb{R}^{n \\times n}$ is a square matrix, then $\\|QA\\|_F = \\|AQ\\|_F = \\|A\\|_F$, i.e., the Frobenius norm of a matrix $A$ is unchanged by left or right multiplication by an orthogonal matrix.**\n",
    "\n",
    "To see why this is true, recall that if $A = \\bm \\vv a_1 \\cdots \\vv a_n\\em$ are the columns of $A$, then $QA = \\bm Q \\vv a_1 \\cdots Q\\vv a_n\\em$. Then, since we can write the Frobenius norm squared of a matrix as the sum of the Euclidean norm squared of its columns, we have:\n",
    "\n",
    "$$\n",
    "\\|QA\\|_F^2 &= \\|Q\\vv a_1\\|^2 + \\cdots + \\|Q\\vv a_n\\|^2 \\\\\n",
    "&= \\|\\vv a_1\\|^2 + \\cdots + \\|\\vv a_n\\|^2 = \\|A\\|_F^2\n",
    "$$\n",
    "\n",
    "Here, the second equality holds because multiplying a vector by an orthogonal matrix does not change its Euclidean norm. Finally we use this and Property 1 to conclude:\n",
    "\n",
    "$$\n",
    "\\|AQ\\|_F = \\|Q^T A^T\\|_F \\underbrace{=}_{\\text{since} \\ Q^{\\top} \\ \\text{is also orthogonal}} \\|A^T\\|_F \\underbrace{=}_{\\text{property 1}} \\|A\\|_F\n",
    "$$\n",
    "\n",
    "We will measure the quality of our rank $k$ approximation [(SVD-k)](#SVD-k) $\\hat{M}$ to $M$ in terms of the Frobenius norm of their difference.\n",
    "\n",
    "The following theorem tells us that the SVD-based approximation [(SVD-k)](#SVD-k) is _optimal with respect to the Frobenius norm of the approximation error!_\n",
    "\n",
    ":::{prf:theorem} \n",
    ":label: svd_approx_thm\n",
    "For every $m \\times n$ matrix $M \\in \\mathbb{R}^{m \\times n}$, every rank target $k \\geq 1$, and every rank $k$ $m \\times n$ matrix $B \\in \\mathbb{R}^{m \\times n}$,\n",
    "\n",
    "$$\n",
    "\\|M - \\hat{M}_k\\|_F \\leq \\|M - B\\|_F\n",
    "$$\n",
    "\n",
    "where $\\hat{M}_k$ is the rank $k$ approximation derived from the SVD $M = U \\Sigma V^T$ as in [(SVD-k)](#SVD-k).\n",
    ":::\n",
    "\n",
    "We won't formally prove [this theorem](#svd_approx_thm), but let's get some intuition as to why this is true. \n",
    "\n",
    "### Understanding [](#svd_approx_thm)\n",
    "\n",
    "To keep things simple, we'll assume $M$ is square and full rank, i.e., $M \\in \\mathbb{R}^{n \\times n}$ with rank $M= n$. Nearly the exact same argument works for general $M$, but we have to use the non-compact SVD of M (which keeps zero singular values around).\n",
    "\n",
    "Our goal is to find a rank k matrix $\\hat{M}$ which minimizes $\\|\\hat{M} - M\\|_F^2$. Let $M = U \\Sigma V^T$ be the SVD of M, where $U, \\Sigma, V \\in \\mathbb{R}^{n\\times n}$ since rank$M=n$. By Property 2 of the Frobenius norm, we then have the following sequence of equalities:\n",
    "\n",
    "$$\n",
    "\\|\\hat{M} - M\\|_F^2 &= \\|\\hat{M} - U \\Sigma V^T\\|_F^2 \\\\\n",
    "&= \\|(U^T(\\hat{M} - U \\Sigma V^T))\\|_F^2 \\quad (\\|AB\\|_F = \\|BA\\|_F) \\\\\n",
    "&= \\|U^T\\hat{M} - \\Sigma V^T\\|_F^2 \\quad (U^TU = I) \\\\\n",
    "&= \\|(U^T\\hat{M} - \\Sigma V^T)V\\|_F^2 \\quad (\\|A\\|_F = \\|AQ\\|_F) \\\\\n",
    "&= \\|U^T\\hat{M}V - \\Sigma\\|_F^2 \\quad (V^TV = I)\n",
    "$$\n",
    "\n",
    "Now notice that since $\\Sigma$ is a diagonal matrix, any non-diagonal entry in $U^T\\hat{M}V$ adds to our approx error, so $U^T\\hat{M}V$ should be diagonal. Let $\\hat{M} = UDV^T$ for some diagonal matrix $D$. Then\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{m_hat_svd}\n",
    "\\|\\hat{M} - M\\|_F^2 = \\|U^T(UDV^T)V - \\Sigma\\|_F^2 = \\|D - \\Sigma\\|_F^2 = \\sum_{i=1}^n (d_{ii} - \\sigma_{i})^2.\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, we want to pick the diagonal entries $d_{ii}$ of $D$ to minimize the right-most expression in [](#m_hat_svd). If there was no rank restriction on $\\hat{M}$, we simply would set $d_{ii} = \\sigma_{i}$.\n",
    "However, notice $\\hat{M} = UDV^T$ is an SVD of $\\hat{M}$! Therefore, for $\\hat{M}$ to be rank $k$, only $k$ of the $d_{ii}$ can be nonzero: if we can only knock off $k$ of the $(d_{ii} - \\sigma_{i})^2$ terms in [](#m_hat_svd), we should pick the top $k$, i.e., $d_{ii} = \\sigma_{i}$  for $i = 1, \\ldots, k$ and $d_{ii} = 0 \\text{ for } i = k+1, \\ldots, n$.\n",
    "\n",
    "Then, \n",
    "$$\\hat{M} = \\bm \\vv u_1 \\ldots \\vv u_k \\vv u_{k+1} \\ldots \\vv u_n\\em \\begin{bmatrix}\n",
    "\\sigma_1 & & & \\\\\n",
    "& \\ddots & & \\\\\n",
    "& & \\sigma_k & \\\\\n",
    "& & & 0 \\\\\n",
    "& & & & \\ddots \\\\\n",
    "& & & & & 0\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "\\vv v_1^T \\\\\n",
    "\\vdots \\\\\n",
    "\\vv v_k^T \\\\\n",
    "\\vv v_{k+1}^T \\\\\n",
    "\\vdots \\\\\n",
    "\\vv v_n^T\n",
    "\\end{bmatrix} = \\sum_{i=1}^k \\sigma_i \\vv u_i \\vv v_i^T = U_k\\Sigma_k V_k^T\n",
    "$$\n",
    "\n",
    "is exactly the expression in [(SVD-k)](#SVD-k), and the square approximation error it incurs is\n",
    "\n",
    "$$\n",
    "\\|\\hat{E}\\|_F^2 = \\|\\hat{M} - M\\|_F^2 = \\sum_{i=k+1}^n \\sigma_i^2,\n",
    "$$\n",
    "i.e. the sum of the squares of the \"tail\" singular values of $M$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    ":::{prf:example}\n",
    ":label: eg_lec18\n",
    "Recall the matrix $A = \\begin{bmatrix}\n",
    "4 & 11 & 14 \\\\\n",
    "8 & 7 & -2\n",
    "\\end{bmatrix}$ from {doc}`Lecture 18 <../lecture_notes/Lecture 18 - Singular Values and the Singular Value Decomposition.pdf>`, we computed its SVD as:\n",
    "\n",
    "$$\n",
    "A =  \\begin{bmatrix}\n",
    "\\frac{3}{\\sqrt{10}} & \\frac{1}{\\sqrt{10}} \\\\\n",
    "\\frac{1}{\\sqrt{10}} & -\\frac{3}{\\sqrt{10}}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "6\\sqrt{10} & 0 \\\\\n",
    "0 & 3\\sqrt{10}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "\\frac{1}{3} & \\frac{2}{3} & \\frac{2}{3} \\\\\n",
    "-\\frac{2}{3} & -\\frac{1}{3} & \\frac{2}{3}\n",
    "\\end{bmatrix} = U \\Sigma V^T.\n",
    "$$\n",
    "\n",
    "$A$ is rank 2, and its rank 1 approximation is, according to [(SVD-k)](#SVD-k), given by\n",
    "\n",
    "$$\n",
    "\\hat{A}_1 = \\begin{bmatrix}\n",
    "\\frac{3}{\\sqrt{10}} \\\\\n",
    "\\frac{1}{\\sqrt{10}}\n",
    "\\end{bmatrix} 6\\sqrt{10} \\begin{bmatrix}\n",
    "\\frac{1}{3} & \\frac{2}{3} & \\frac{2}{3}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "6 & 12 & 12 \\\\\n",
    "2 & 4 & 4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If we compute $\\|\\hat{A}_1 - A\\|_F^2$ we get:\n",
    "$$\n",
    "\\left\\|\\begin{bmatrix}\n",
    "2 & -1 & 2 \\\\\n",
    "-6 & -3 & 6\n",
    "\\end{bmatrix}\\right\\|_F^2 &= 2^2 + (-1)^2 + 2^2 + (-6)^2 + (-3)^2 + 6^2 \\\\\n",
    "&= 90\n",
    "$$\n",
    "\n",
    "which is exactly $\\sigma_2^2 = (3\\sqrt{10})^2 = 90$.\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we address an obvious question when applying these ideas in practice: **how should we pick the rank $k$ of our approximation?**\n",
    "\n",
    "In a perfect world, the singular values of the original data matrix will give strong guidance: if the top few singular values are much larger than the rest, then the obvious solution is to take $k = \\#$ of big values. This was the case in the handset example previous lecture: the 1st singular value was significantly larger than others, suggesting a rank 1 approximation would be a good choice (which was the [image (d)](./121-Basics_of_Statistics.ipynb#railroad)).\n",
    "\n",
    "In less clear settings, the rule of thumb is to take $k$ as small as possible while still providing a \"useful\" approximation of the original data. For example, it is common to choose $k$ so that the sum of the top k singular values is at least $c$ times larger than the sum of the other singular values. The ratio $c$ is typically a domain-dependent constant picked based on the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/11_Ch_12_Low_Rank/132-MatrixNorm.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
