{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd7554ca-5ac4-4958-b713-648e3fd54114",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "title: 1.5 LU-Factorization\n",
    "subject:  Linear Algebraic Systems\n",
    "subtitle: A new matrix factorization\n",
    "short_title: 1.5 LU-Factorization\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: Gaussian Elimination, LU factorization\n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c842cd0f-7e4c-440f-bf7c-6e04a0db062d",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/00_Linear_Algebraic_Systems/023a-linsys-LU.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 01 - Systems of linear equations, vectors, matrices, Gauss Elimination and LU-factorization.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efca8556-3e7f-4f1c-b466-04f0e14b12e5",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "This section is based on ALA Ch 1.3, Ch 5.3 of Jessy Grizzles [ROB 101 notes](https://github.com/michiganrobotics/rob101/blob/main/Fall%202021/Textbook/ROB_101_December_2021_Grizzle.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170438e7",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:- \n",
    "- what the $LU$ factorization of a matrix is\n",
    "- how to apply $LU$ factorization to solve systems of linear equations\n",
    "- how to represent row operations using elementary matrices\n",
    "- LU factorization using elementary matrix actions\n",
    "- how LU factorization relates to Gaussian Elimination (forward elimination and back substitution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2378cbe-20bd-404d-baa2-d111b1877e72",
   "metadata": {},
   "source": [
    "### We like triangular matrices\n",
    "A theme we've exploited so far is that if we have an upper triangular system of equations, i.e., if $U\\vv x = \\vv c$ for $U$ and upper triangular matrix, then finding a solution $\\vv x$ is easy via back substitution.  Hopefully you believe that a _lower_ triangular system of equations of the form $L\\vv x = \\vv c$ for $L$ a lower triangular matrix is also easy to solve via forward substitution.  This section starts with the following question:\n",
    "\n",
    "> Is it easy to solve a system of linear equations of the form $LU \\vv x = \\vv b$, where $L$ is a lower triangular matrix and $U$ is an upper triangular matrix?\n",
    "\n",
    "<!-- We've made a lot of progress, but we still do not have a way of dealing with cases where the matrix $A$ is not regular, i.e., when it has pivots that are zero.  We'll defer that situation for a little bit still, and instead look at why we bothered with $LU$-factorizations to begin with!  Suppose we are interested in solving a system of linear equations $A\\vv x = \\vv b$, and we assume that $A$ is regular so that we can apply the algorithm above to compute an $LU$-facotrization of $A$, i.e., we find a lower triangular matrix $L$ and an upper triangular matrix $U$ so that $A=LU$.  Then our linear system becomes $LU\\vv x = \\vv b$.  \n",
    "\n",
    "Neat, but so what?   -->\n",
    "\n",
    "This looks like something we might be able to handle, since we know how to solve linear systems that look like $U\\vv x = \\vv z$, for $U$ upper triangular, by Back Substitution, and how to solve linear systems that look like $L\\vv y = \\vv c$, for $L$ lower triangular, by _Forward Substitution_.[^fsub]  Our strategy for solving $LU\\vv x = \\vv b$ will be to tackle the problem in two steps: first, we solve an intermediate lower triangular system by Forward Substitution, and then use the solution to that intermediate problem to define an upper triangular system, which we solve by Back Substitution!  With that strategy in mind, let's introduce a new intermediate variable $\\vv z = U \\vv x$.  Then we can find the solution to $LU\\vv x = \\vv b$ by solving\n",
    "$$\n",
    "L\\vv z = \\vv b,\n",
    "$$\n",
    "for $\\vv z$ via Forward Substitution, and then solving \n",
    "$$\n",
    "U \\vv x = \\vv z,\n",
    "$$\n",
    "for $\\vv x$ via Backward Substitution.  If we piece everything together, we see that $\\vv x$ is indeed a solution to our original equation $LU\\vv x = \\vv b$ because\n",
    "$$\n",
    "L\\underbrace{U \\vv x}_{=\\vv z}= \\underbrace{L\\vv z}_{=\\vv b} = \\vv b.\n",
    "$$\n",
    "\n",
    "We summarize the above in the following algorithm for solving linear systems of the form $LU\\vv x = \\vv b$, where $L$ is a lower triangular matrix, and $U$ is an upper triangular matrix:\n",
    "\n",
    "[^fsub]: Forward Substitution is exactly what it sounds like.  To solve $L\\vv y = \\vv c$, when $L$ is lower triangular, we start at the top row and set $y_1 = c_1/l_{11}$. We then move down a row, and solve $l_{21}y_1 + l_{22}y_2 = c_2$ by plugging in $y_1$ and solving for $y_2$.  We continue in this manner until we've solved for $\\vv y$. \n",
    "\n",
    "1. Solve $L\\vv z = \\vv b$ via forward substitution.\n",
    "2. Using $\\vv z$, solve $U\\vv x = \\vv z$ via back substitution.\n",
    "\n",
    "The above is all good and well, but there's a catch!  Most linear systems we care about are posted of the form $A\\vv x = \\vv b$, where $A$ is neither lower nor upper triangular.  What we'll show in this section is an amazing fact of linear algebra: any matrix $A$ that is regular can be factored as a product of a lower triangular matrix $L$ and an upper triangular matrix $U$ such that $A=LU$.  This _matrix factorization_, the first (but not the last!) we will see in this class, is called the $LU$ factorization of a matrix (a descriptive, if not particularly creative, name)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e165f6d-8d07-4f2b-8407-edaa3e9c47b1",
   "metadata": {},
   "source": [
    "## Elementary Matrices\n",
    "\n",
    "\n",
    "It turns out that our strategy of adding equations together is in fact an operation that can be realized by matrix multiplication.  Our starting point will be to introduce a the idea of an _elementary matrix_, which encodes the corresponding elementary operation.  So far we've only seen the elementary operation of adding equations together, which we recall, is equivalent to adding rows of the augmented matrix $M = [A \\, | \\, \\vv b]$ together, but we'll see other types of operations shortly.  The definition below will work for all of them.\n",
    "\n",
    "```{prf:definition} Elementary Matrix\n",
    ":label: elementary\n",
    "The _elementary matrix_ associated with an elementary row operation for $m$-rowed matrices is the $m \\times m$ matrix obtained by applying the the row operation to the $m\\times m$ identity matrix $I_m$.\n",
    "```\n",
    "\n",
    "This definition is a bit abstract, so let's see it in action.  Suppose that we have a system of three equations in three unknowns, encoded by the augmented matrix $M = [ A\\, | \\, \\vv b]$, where $A$ is a $3\\times 3$ matrix and $\\vv b$ is a $3 \\times 1$ vector.  What is the elementary matrix associated with subtracting twice the first row from the second row?  If we start with the identify matrix \n",
    "$$\n",
    "I_3 = \\bm 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\em,\n",
    "$$\n",
    "and apply this operation to it, we end up with the elementary matrix \n",
    "$$\n",
    "E_1 = \\bm 1 & 0 & 0 \\\\ -2 & 1 & 0 \\\\ 0 & 0 & 1\\em,\n",
    "$$\n",
    "which we named $E_1$ just to keep track of which elementary matrix we are talking about later on.  Let's check if it does what it's supposed to on a two different $3 \\times 3$ matrices \n",
    "$$\n",
    "\\underbrace{\\bm 1 & 0 & 0 \\\\ -2 & 1 & 0 \\\\ 0 & 0 & 1\\em}_{E_1}\\bm 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\em = \\bm 1 & 2 & 3 \\\\ 2 & 1 & 0 \\\\ 7 & 8 & 9 \\em, \\quad \\underbrace{\\bm 1 & 0 & 0 \\\\ -2 & 1 & 0 \\\\ 0 & 0 & 1\\em}_{E_1}\\bm 1 & 2 & 3 \\\\ 1 & 2 & 3 \\\\ 1 & 2 & 3 \\em =\\bm 1 & 2 & 3 \\\\ -1 & -2 & -3 \\\\ 1 & 2 & 3 \\em .\n",
    "$$\n",
    "Indeed it does, and by playing around with our rules of matrix arithmetic and multiplication, you should be able to convince yourself that that left multiplying _any_ 3-row matrix by $E_1$ will subtract twice its from its second row.\n",
    "\n",
    "We can also use [](#elementary) to reverse engineer what row operation an elementary matrix is encoding.\n",
    "\n",
    "````{exercise} Reverse Engineering Elementary Matrices\n",
    ":label: em-ex\n",
    "What elementary row operations are realized if we left multiply by the following matrices?\n",
    "$$\n",
    "E_2 = \\bm 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ -1 & 0 & 1 \\em, \\quad E_3 = \\bm 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & \\frac{1}{2} & 1 \\em.\n",
    "$$\n",
    ":::{hint} Click me for a hint!\n",
    ":class: dropdown\n",
    "What would I have to do to\n",
    "$$\n",
    "I_3 = \\bm 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\em,\n",
    "$$\n",
    "to get $E_2$ and $E_3$?\n",
    ":::\n",
    "```{solution} em-ex\n",
    ":class: dropdown\n",
    "Left multiplying by $E_2$ realizes subtracting the first row from the last row of a matrix.  This is true because to get the last row of $E_2$, $\\bm -1 & 0 & 1\\em$ from $I_3$, we need to subract the first row $\\bm 1 & 0 & 0 \\em$ of $I_3$ from the last row $\\bm 0 & 0 & 1 \\em$ of $I_3$.\n",
    "\n",
    "Using similar reasoning, we see that left multiplying by $E_3$ realizes adding $1/2$ the second row of a matrix to its last row.\n",
    "```\n",
    "````\n",
    "\n",
    "### Elementary Matrices in Action\n",
    "Let us use elementary matrices to design a matrix that can help us solve linear equations.  Let's revisit the very first system of equations we encountered:\n",
    "\\begin{eqnarray}\n",
    "\\label{simple-linsys}\n",
    "x_1+2x_2+x_3 & = 2,\\\\\n",
    "2x_1+6x_2+x_3 & =7,\\\\\n",
    "x_1+x_2+4x_3 & =3,\n",
    "\\end{eqnarray}\n",
    "or in matrix-vector form\n",
    "$$\\underbrace{\\begin{bmatrix} 1 & 2 & 1 \\\\\n",
    "2 & 6 & 1 \\\\\n",
    "1 & 1 & 4 \\end{bmatrix}}_A\\underbrace{\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}}_{\\vv x} = \\underbrace{\\begin{bmatrix} 2 \\\\ 7 \\\\ 3 \\end{bmatrix}}_{\\vv b}\n",
    "$$\n",
    "which we solved by first applying the row operation encoded by $E_1$ (subtracted twice equation 1 from equation 2), then applying the operation encoded by $E_2$ (subtract the first equation from the last equation ), and finally by applying the operation encoded by $E_3$ (add 1/2 first equation to last equation).  Funny how that conveninently worked out!  Therefore, if we keep careful track of the row operations and their matrix multiplication realizations, we conclude that:\n",
    "$$\n",
    "A = \\begin{bmatrix} 1 & 2 & 1 \\\\\n",
    "2 & 6 & 1 \\\\\n",
    "1 & 1 & 4 \\end{bmatrix}, \\quad E_3 E_2 E_1 A = U = \\bm 1 & 2 & 1 \\\\ 0 & 2 & -1 \\\\ 0 & 0 & \\frac{5}{2} \\em.\n",
    "$$\n",
    "The way to read the expression $E_3 E_2 E_1 A$ is from left-to-right: we start with $A$, then apply $E_1$, then $E_2$, and finally $E_3$.  If we give the composition of elementary operations a name, say $E = E_3 E_2 E_1 A$, then we can check that $E [A \\, | \\, \\vv b] = [U \\, | \\, \\vv c]$, that is to say left multiplying the augmented matrix $M = [A \\, | \\, \\vv b]$ by the matrix $E$ performs Gaussian Elmination for us!  This is just a first taste of a powerful feature of linear algebra: _we can encode very sophisticated and complex operations via matrix multiplication._\n",
    "```{important}\n",
    "The order of the operations here matters!  For example, $E_2 E_1 E_3 A$ is a very different matrix than $E_3 E_2 E_1 A$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714610d3-3167-4f05-a9c3-7804529a7074",
   "metadata": {},
   "source": [
    "### Inverse Elementary Matrix\n",
    "\n",
    "We can undo the action of an elementary matrix using the corresponding _inverse elementary matrix_. For example, to undo the action of $E_1$ (subtract twice equation 1 from equation 2), we add twice equation 1 to equation 2. The corresponding matrix is \n",
    "$$\n",
    "L_1 = \\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "2 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "We note that $L_1E_1 = I$, since, $L_1$ reverses the action done by $E_1$ and the end result $I$ does not do any action. Similarly, we can define appropriate inverses\n",
    "$$\n",
    "L_2 = \\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 0 & 1\n",
    "\\end{bmatrix}, \\ L_3 = \\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & -\\frac{1}{2} & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "We have $U = E_3E_2E_1A$. If we reverse the actions using $L_1L_2L_3$, we have\n",
    "$$\n",
    "L_1L_2L_3U = L_1L_2L_3E_3E_2E_1A = L_1L_2E_2E_1A = A,\n",
    "$$\n",
    "where we used the fact that $L_2E_2 = L_1E_1 = I$. Hence, calling $L = L_1L_2L_3$, we showed that $A = LU$. Since $L_1, L_2, L_3$ are all lower triangular, so is their product $L$. This is how we can obtain the LU facotrization of $A$ using elementary matrix operations.\n",
    "\n",
    "Using the $LU$ factorization explained in this page, we give a formal definition for a _regular_ matrix. \n",
    "\n",
    "```{prf:definition} Regular matrix\n",
    ":label: regular_matrix\n",
    "A matrix $A$ is _regular_ if and only if it can be factored $A = L U$, where $L$ is a lower unitriangular matrix, having all 1’s on the diagonal, and $U$ is upper\n",
    "triangular with nonzero diagonal entries, which are the pivots of $A$. The nonzero oﬀ-\n",
    "diagonal entries $l_{ij}$ for $i > j$ appearing in $L$ prescribe the elementary row operations that bring $A$ into upper triangular form; namely, one subtracts $l_ij$ times row $j$ from row $i$ at the appropriate step of the Gaussian Elimination process.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88791b79-a3a6-4576-993c-1313c249c6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "L: \n",
      " [[1.  0.  0. ]\n",
      " [0.5 1.  0. ]\n",
      " [0.5 0.5 1. ]] \n",
      "U: \n",
      " [[ 2.    6.    1.  ]\n",
      " [ 0.   -2.    3.5 ]\n",
      " [ 0.    0.   -1.25]]\n",
      "\n",
      "Solution x: \n",
      " [-3.  2.  1.]\n"
     ]
    }
   ],
   "source": [
    "## applying LU factoriation to solve linear equations\n",
    "\n",
    "from scipy.linalg import lu\n",
    "import numpy as np\n",
    "\n",
    "A = np.array([[2, 6, 1],\n",
    "              [1, 1, 4],\n",
    "              [1, 2, 1]])\n",
    "b = np.array([7, 3, 2])\n",
    "_, L, U = lu(A) # LU decomposition using scipy package\n",
    "print(\"\\nL: \\n\", L, \"\\nU: \\n\", U)\n",
    "\n",
    "n = A.shape[1]\n",
    "\n",
    "# forward substitution\n",
    "z = np.zeros((n,))\n",
    "z[0] = b[0]/L[0, 0]\n",
    "for i in range(1, n):\n",
    "    z[i] = (b[i] - np.sum(L[i, :i]*z[:i]))/L[i, i]\n",
    "\n",
    "# back substitution\n",
    "x = np.zeros((n,))\n",
    "x[-1] = z[-1]/U[-1, -1]\n",
    "for i in range(n-2, -1, -1):\n",
    "    x[i] = (z[i] - np.sum(U[i, i+1:]*x[i+1:]))/U[i, i]\n",
    "\n",
    "print(\"\\nSolution x: \\n\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0942cd5a-3265-4c29-a76c-3bdb9e17f160",
   "metadata": {},
   "source": [
    "## Optional: $LU$-factorization is Gaussian Elimination!\n",
    "\n",
    "We have $A = LU$.  Suppose we have applied Gaussian Elimination to get our system into $U\\vv x = \\vv c$.  There is a matrix $M$ such that $MA= U$ and $M\\vv b=\\vv c$.  So we have \n",
    "$$\n",
    "MA\\vv x = MLU\\vv x = M\\vv b\n",
    "$$\n",
    "But if $MA = U$ then we must have that $ML= I$.\n",
    "\n",
    "Now, if we go back to solving our system using $LU$-factorization, we have that $LU\\vv x = \\vv b$ which we can turn into $L\\vv z = \\vv b$ and $U\\vv x = \\vv z$.  We saw that we can solve that $L\\vv z = b$ by forward substitution, but notice that we also have that $ML\\vv z = \\vv z = M \\vv b$, i.e., the matrix $M$ that reduces our original system of equations to upper triangular form is actually performing this forward substitution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb71568-1a8e-4a4a-bf51-a7f81e6a4def",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/00_Linear_Algebraic_Systems/023a-linsys-LU.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0a8a6a-dbc5-449e-92ce-5b54b1bed122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
