{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd7554ca-5ac4-4958-b713-648e3fd54114",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "title: 1.7 Matrix Inverses\n",
    "subject:  Linear Algebraic Systems\n",
    "subtitle: How do I divide by a matrix?\n",
    "short_title: 1.7 Matrix Inverses\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: inverse, Gauss Jordan elimination, identity matrix\n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e67bbe-3d79-4a20-8327-f68b183e9790",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/00_Ch_1_Linear_Algebraic_Systems/025-linsys-inverses.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 02 - Pivots + Permutations, Matrix Inverses (Gauss-Jordan), Transposes, Symmetric Matrices, and General Linear Systems.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efca8556-3e7f-4f1c-b466-04f0e14b12e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Reading\n",
    "Material related to this page, as well as additional exercises, can be found in ALA Ch. 1.5, LAA Ch 2.2.  These notes are mostly based on ALA Ch 1.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170438e7",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- what is the inverse of a matrix\n",
    "- computing inverse for 2x2 matrices\n",
    "- important properties of matrix inverse: existence, product\n",
    "- Gauss Jordan elimination to compute inverses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd6aeff-4ad9-42b7-9fb4-a82ada3e07b6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Basic Definition\n",
    "\n",
    "The inverse of a matrix is analogous to the reciprocal $a^{−1} =\\frac{1}{a}$ of a nonzero scalar $a \\neq 0$. We already encountered the inverses of matrices corresponding to elementary row operations. In this section, we will study inverses of general square matrices. We begin with the formal deﬁnition.\n",
    "\n",
    "```{prf:definition} Matrix inverse\n",
    ":label: inverse\n",
    "Let $A$ be a square matrix of size $n \\times n$. An  $n \\times n$ matrix $X$ is called the inverse of $A$ if it satisﬁes\n",
    "$$\n",
    "X A = I = A X,\n",
    "$$\n",
    "where $I = I_n$ is the $n \\times n$ identity matrix. The inverse of $A$ is commonly denoted by $A^{−1}$.\n",
    "```\n",
    "\n",
    "```{warning}\n",
    "Not every square matrix has an inverse, just like not every scalar has an inverse: $0^{-1} = \\frac{1}{0}$ is not defined since $0x = 1$ has no solution.\n",
    "```\n",
    "```{warning}\n",
    "In general, there is no straightforward way to guess the entries of $A^{-1}$ by looking at the entries of $A$.\n",
    "```\n",
    "\n",
    "The inverse of a matrix is typically more useful in theory than it is in practice.  In fact, a commandment of numerical linear algebra is \"thou shalt not invert a matrix\" because it tends to cause numerical issues like the ones we described in the last section.  Because of this, we will not spend too much time on computing matrix inverses: it is actually very rare that you will ever need to do this outsie of a linear algebra class! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebf45f1-aed4-4ba9-be1a-318a2e0dbe4f",
   "metadata": {},
   "source": [
    "## Formula for 2x2 matrices\n",
    "\n",
    "We want to find the inverse of the matrix $A$, that is denoted by $X$\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "a & b \\\\ c & d\n",
    "\\end{bmatrix},\\ X = \\begin{bmatrix}\n",
    "x & y \\\\ z & w\n",
    " \\end{bmatrix} \\Rightarrow AX = I = \\begin{bmatrix}\n",
    "1 & 0 \\\\ 0 & 1\n",
    " \\end{bmatrix}\n",
    "$$\n",
    "The above matrix equation produces a set of four linear equations in the unknowns $(x, y, z, w)$:\n",
    "$$\n",
    "AX = \\begin{bmatrix}\n",
    "a & b \\\\ c & d\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "x & y \\\\ z & w\n",
    " \\end{bmatrix} = \\begin{bmatrix} ax + bz & ay + bw \\\\ cx + dz & cy + dw \\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & 0 \\\\ 0 & 1\n",
    " \\end{bmatrix} = I,\n",
    "$$\n",
    "which holds if an only if $(x,y,z,w)$ satisfy the linear system:\n",
    "\\begin{eqnarray}\n",
    "ax + bz &= 1\\\\\n",
    "ay + bw &= 0\\\\\n",
    "cx + dz &= 0\\\\\n",
    "cy + dw & =1.\n",
    "\\end{eqnarray}\n",
    "\n",
    "Solving by Gaussian Elimination, we find\n",
    "\\begin{equation}\n",
    "\\label{inv_2x2}\n",
    "x = \\frac{d}{ad-bc}, \\ y = \\frac{-b}{ad-bc}, \\ z = \\frac{-c}{ad-bc}, \\ w = \\frac{a}{ad-bc} \\Rightarrow X = \\frac{1}{ad-bc}\\begin{bmatrix}\n",
    "d & -b \\\\ -c & a\n",
    " \\end{bmatrix},\n",
    "\\end{equation}\n",
    "provided that the common denominator $ad-bc \\neq 0$.  You can verify that $XA = I$ also holds, which lets us conclude that $X=A^{-1}$ is the inverse of $A$.\n",
    "\n",
    "```{warning}\n",
    "The inverse of a 2x2 matrix $A$ exists if and only if $ad-bc \\neq0$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c956f2d-5be9-4216-a8e4-3bf13c4e593a",
   "metadata": {},
   "source": [
    "#### Python Break!\n",
    "NumPy has a built in function for computing matrix inverses, `np.linalg.inv`.  Let's compare the output of that function to a matrix inverse computed with our formula [](#inv_2x2).  Notice that because of numerical errors, $AA^{-1} \\approx I$, but that the approximation error is _very_ small (on the order of 1e-16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "853cd053-1294-49c0-8931-92a31499fa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A=\n",
      " [[ 1  2]\n",
      " [-3  5]], \n",
      " Ainv =\n",
      " [[ 0.45454545 -0.18181818]\n",
      " [ 0.27272727  0.09090909]], \n",
      " Ainv_np = \n",
      " [[ 0.45454545 -0.18181818]\n",
      " [ 0.27272727  0.09090909]]\n",
      "AA^-1 = \n",
      "[[ 1.00000000e+00  0.00000000e+00]\n",
      " [-2.22044605e-16  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def my_inv(A):\n",
    "    a = A[0,0]\n",
    "    b = A[0,1]\n",
    "    c = A[1,0]\n",
    "    d = A[1,1]\n",
    "    denominator = a*d - b*c\n",
    "    X = 1/denominator * np.array([[d, -b], [-c, a]])\n",
    "    return X\n",
    "\n",
    "A = np.array([[1,2],[-3,5]])\n",
    "Ainv = my_inv(A)\n",
    "Ainv_np = np.linalg.inv(A)\n",
    "\n",
    "print(f'A=\\n {A}, \\n Ainv =\\n {Ainv}, \\n Ainv_np = \\n {Ainv_np}')\n",
    "print(f'AA^{-1} = \\n{A @ Ainv}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636cb1e7-6a17-4d2d-b4eb-b594ed46a68f",
   "metadata": {},
   "source": [
    "## Some Useful Properties\n",
    "\n",
    "One way to understand the matrix inverse is to take a dynamic view of matrix multiplication.  If we think of the matrix $A$ as defining a function $f(\\vv x)$ that maps $\\vv x$ to a new vector $f(\\vv x) = A\\vv x$, then we can intuitively think of the matrix inverse as \"undoing\" this action, just as we did for elementary operation matrices and their inverses.\n",
    "\n",
    "(inverse-tip)=\n",
    "```{tip}\n",
    "The inverse of a matrix $A$ reverses the changes $A$ does to a vector $\\textbf{x}$ via matrix-vector multiplication. \n",
    "$$\n",
    "\\textbf{x} \\xrightarrow{A} A \\textbf{x} \\xrightarrow{A^{-1}} A^{-1}A\\textbf{x} = \\textbf{x}\n",
    "$$\n",
    "```\n",
    "\n",
    "For example, the elementary operation of adding twice the first row to the third row is given by \n",
    "\n",
    "$$\n",
    "E = \\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 2 & 0 & 1 \\end{bmatrix},\n",
    "$$\n",
    "while the inverse operation is given by\n",
    "$$\n",
    "L = \\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ -2 & 0 & 1 \\end{bmatrix},\n",
    "$$\n",
    "and you can verify that $L = E^{-1}$. You can also verify similarly that for permutation matrices with exactly one interchange that\n",
    "$$\n",
    "P = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} = P^{-1},\n",
    "$$\n",
    "i.e., that $P$ is its own inverse!  Our [observation](#inverse-tip) above gives us some easy intuition for understanding why this is true, but you can also check this directly by using the formula [](#inv_2x2).\n",
    "\n",
    "```{important}\n",
    "A square matrix has an inverse if and only if it is nonsingular\n",
    "```\n",
    "\n",
    "The above statement will be proved later, but for now think about the scalar analogy. The equation $ax=b$ has a unique solution $x = a^{-1}b$ if and only if $a \\neq 0$. Similarly, $A \\textbf{x} = \\textbf{b}$ has a unique solution $\\textbf{x} = A^{-1}\\textbf{b}$ if and only if $A^{-1}$ exists.\n",
    "\n",
    "```{note} Properties\n",
    "1. The inverse of a square matrix, if it exists, is unique.\n",
    "2. If $A$ is invertible, so is $A^{-1}$ and $\\left(A^{-1}\\right)^{-1} = A$.\n",
    "3. If $A$ and $B$ are invertible matrices of the same size, then their product $AB$ is also invertible, and\n",
    "$$\n",
    "(AB)^{-1} = B^{-1}A^{-1} \\\\ \\textbf{order is reversed!}\n",
    "$$\n",
    "Again, we can intuit why this is true by thinking about the transformations $\\vv x \\mapsto B \\vv x$ and $\\vv y \\mapsto A \\vv y$: we have to undo the transformation of $\\vv x$ computed by first transforming $\\vv y = B \\vv x$, and then $\\vv z = A \\vv y = AB \\vv x$ in the right order:\n",
    "$$\n",
    "\\textbf{x} \\xrightarrow{B} B \\textbf{x} \\xrightarrow{A} AB\\textbf{x} \\xrightarrow{A^{-1}} A^{-1}AB\\textbf{x} = B\\textbf{x} \\xrightarrow{B^{-1}} B^{-1}B\\textbf{x} = \\textbf{x}\n",
    "$$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceffd52-56eb-4625-8fbc-e4696a69e3f4",
   "metadata": {},
   "source": [
    "````{exercise}  Matrix inverses\n",
    ":label: inverses-ex1\n",
    "\n",
    "For each of the following inveritble matrices, find their inverse.\n",
    "\n",
    "a. $\\bm 1&2\\\\3&4 \\em$\n",
    "\n",
    "b. $\\bm 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & -3 \\em$\n",
    "\n",
    "c. $I_n$ (the identity matrix in $n$ dimensions)\n",
    "\n",
    "d. $\\bm 1&2&0\\\\0&1&2\\\\0&0&1 \\em$\n",
    "\n",
    ":::{hint} Click me for a hint!\n",
    ":class: dropdown\n",
    "\n",
    "a. Use the formula for $2\\times 2$ matrices!\n",
    "\n",
    "b. The inverse of this matrix will be another diagonal matrix (has zeros on off diagonal entries).\n",
    "\n",
    "c. Can you find any (square) matrix $X$ such $I_nX = I_n$?\n",
    "\n",
    ":::\n",
    "\n",
    "```{solution} inverses-ex1\n",
    ":class: dropdown\n",
    "\n",
    "a. Using the formula for inverses of $2 \\times 2$ matrices,\n",
    "\n",
    "\\begin{align*}\n",
    "    \\bm 1&2\\\\3&4 \\em^{-1} = \\frac{1}{1\\cdot 4 - 2 \\cdot 3}\\bm 4 & -2 \\\\ -3 & 1\\em = \\bm -2 & 1 \\\\ \\frac 3 2 & -\\frac 1 2\\em\n",
    "\\end{align*}\n",
    "\n",
    "b. We can confirm that \n",
    "\n",
    "\\begin{align*}\n",
    "\\bm 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & -3 \\em\\bm 1 & 0 & 0 \\\\ 0 & \\frac 1 2 & 0 \\\\ 0 & 0 & -\\frac 1 3 \\em = \\bm 1&0&0\\\\0&1&0\\\\0&0&1 \\em = \\bm 1 & 0 & 0 \\\\ 0 & \\frac 1 2 & 0 \\\\ 0 & 0 & -\\frac 1 3 \\em\\bm 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & -3 \\em\n",
    "\\end{align*}\n",
    "\n",
    "which means that \n",
    "\n",
    "\\begin{align*}\n",
    "\\bm 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & -3 \\em^{-1} = \\bm 1 & 0 & 0 \\\\ 0 & \\frac 1 2 & 0 \\\\ 0 & 0 & -\\frac 1 3 \\em\n",
    "\\end{align*}\n",
    "\n",
    "c. Remember that $I_n \\times I_n = I_n$, which means $I_n^{-1} = I_n$.\n",
    "\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6493b5-6b12-415a-a159-646579661219",
   "metadata": {},
   "source": [
    "````{exercise}  Inverse of a product of (invertible) matrices\n",
    ":label: inverses-ex2\n",
    "\n",
    "Given matrices \n",
    "\n",
    "\\begin{align*}\n",
    "    A = \\bm 1&2\\\\3&4 \\em, \\quad B = \\bm 1&1\\\\2&3 \\em\n",
    "\\end{align*}\n",
    "\n",
    "Find $A^{-1}$, $B^{-1}$, and $(AB)^{-1}$. Check that $(AB)^{-1} = B^{-1}A^{-1}$.\n",
    "\n",
    "```{solution} inverses-ex2\n",
    ":class: dropdown\n",
    "\n",
    "Multiplying out $AB$, we get that\n",
    "\n",
    "\\begin{align*}\n",
    "    AB = \\bm 5&7\\\\11&15 \\em \n",
    "\\end{align*}\n",
    "\n",
    "Using the formula for inverse of $2\\times 2$ matrices, we get that\n",
    "\n",
    "\\begin{align*}\n",
    "    A^{-1} = \\bm -2&1\\\\ \\frac 3 2 & -\\frac 1 2 \\em,\\quad B^{-1} = \\bm 3 & -1\\\\ -2 & 1 \\em, \\quad (AB)^{-1} = \\bm -\\frac {15} 2 & \\frac 7 2 \\\\ \\frac{ 11} 2 & -\\frac 5 2 \\em\n",
    "\\end{align*}\n",
    "\n",
    "Finally, we check that $B^{-1}A^{-1} = (AB)^{-1}$:\n",
    "\n",
    "\\begin{align*}\n",
    "    B^{-1}A^{-1} = \\bm 3 & -1\\\\ -2 & 1 \\em\\bm -2&1\\\\ \\frac 3 2 & -\\frac 1 2 \\em = \\bm -\\frac {15} 2 & \\frac 7 2 \\\\ \\frac{ 11} 2 & -\\frac 5 2 \\em = (AB)^{-1}\n",
    "\\end{align*}\n",
    "\n",
    "Looks good!\n",
    "\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c6626e-3784-43cc-b2c9-ceab90079515",
   "metadata": {},
   "source": [
    "#### Python Break!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e987c5fb-0090-46fe-8347-f283fe46d853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inverse of product: \n",
      " [[-2.425       0.35        0.575     ]\n",
      " [ 0.825      -0.15       -0.175     ]\n",
      " [ 5.45833333 -0.75       -1.20833333]] \n",
      "Product of inverses:\n",
      " [[-2.425       0.35        0.575     ]\n",
      " [ 0.825      -0.15       -0.175     ]\n",
      " [ 5.45833333 -0.75       -1.20833333]]\n"
     ]
    }
   ],
   "source": [
    "# Inverse of the product\n",
    "A = np.array([[1, 2, 3], \n",
    "              [4, -5, 6], \n",
    "              [7, 8, 9]])\n",
    "\n",
    "B = np.array([[1, 2, 0],\n",
    "              [1, 3, 0],\n",
    "              [1, -3, 1]])\n",
    "\n",
    "A_inv = np.linalg.inv(A)\n",
    "B_inv = np.linalg.inv(B)\n",
    "\n",
    "AB_inv = np.linalg.inv(A@B)\n",
    "print(\"\\nInverse of product: \\n\", AB_inv, \"\\nProduct of inverses:\\n\", B_inv @ A_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486bd141-e390-44ec-ab7f-a40e1d4b9ff6",
   "metadata": {},
   "source": [
    "## Gauss-Jordan Elimination\n",
    "\n",
    "Gauss-Jordan Elimination (GJE) is the principal algorithm for computing inverses of a nonsingular matrix. \n",
    "\n",
    "```{important}\n",
    "For a square matrix $A$, we only need to solve for the _right inverse_ $AX = I$, since it then automatically satisfies the _left inverse_ $XA = I$ condition.\n",
    "```\n",
    "\n",
    "```{note}\n",
    "For some matrices $A$, we can only find a matrix $X$ that satsifies the right inverse condition $AX = I$, but not the left inverse condition $XA = I$.  Such a matrix is called a _right inverse_.  Similarly, a matrix $X$ that only satisfies the left inverse condition $XA=I$ but not the right inverse condition is called a _left inverse_. For a non-square matrix, the same $X$ cannot simultaneously satisfy both $AX = I$ and $XA = I$ (check dimensions). Hence, we emphasize that for $X$ to be an inverse of $A$, both left and right inverse conditions should be satisfied, even if checking one of the conditions is sufficient.\n",
    "```\n",
    "\n",
    "Here is some good news: we already have all of the tools needed to solve $AX = I$ for the unknown matrix $X$.  Our starting point is to recognize that the matrix equation $AX = I$ is really $n$ linear systems of the form $A\\vv x_i = \\vv e_i$ in parallel, where the $\\vv x_i$ and $\\vv e_i$ are the columns of the matrix $X$ and the identify matrix $I$, respevtively.\n",
    "\n",
    "We define the $n \\times 1$ _unit vectors_ $\\textbf{e}_i$:\n",
    "$$\n",
    "\\textbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, \\ \\textbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, \\cdots, \\ \\textbf{e}_n = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "as the vectors with exactly one entry of $1$ in the $i^{th}$ position and zeros elsewhere. The vectors $\\textbf{e}_i$ are the columns of the identity matrix $I_n$:\n",
    "$$\n",
    "I_n = \\begin{bmatrix} \\textbf{e}_1 & \\textbf{e}_2 & \\cdots & \\textbf{e}_n \\end{bmatrix}\n",
    "$$\n",
    "Hence, the right inverse equation can be written as\n",
    "\\begin{eqnarray}\n",
    "AX = I \\iff\n",
    "A \\begin{bmatrix} \\textbf{x}_1 & \\textbf{x}_2 & \\cdots & \\textbf{x}_n \\end{bmatrix} = \\begin{bmatrix} A \\textbf{x}_1 & A \\textbf{x}_2 & \\cdots & A \\textbf{x}_n \\end{bmatrix} = \\begin{bmatrix} \\textbf{e}_1 & \\textbf{e}_2 & \\cdots & \\textbf{e}_n \\end{bmatrix} \\iff\n",
    "A\\textbf{x}_1 = \\textbf{e}_1, A\\textbf{x}_2 = \\textbf{e}_2, \\cdots, A\\textbf{x}_n = \\textbf{e}_n.\n",
    "\\end{eqnarray}\n",
    "\n",
    "The above defines a set of $n$ systems of linear equations.  A key feature here is that all $n$ linear systems _have the same coefficient matrix_ $A$.  We can take advantage of that to build one large augmented matrix $M$ that stacks all $n$ right hand sides on the right of the coefficient matrix $A$:\n",
    "\\begin{equation}\n",
    "\\label{big_augm}\n",
    "M = \\left[ \\begin{array}{c|ccc} A & \\textbf{e}_1 & \\textbf{e}_2 & \\cdots & \\textbf{e}_n  \\end{array}\\right] = \\left[ \\begin{array}{c|c} A & I  \\end{array}\\right].\n",
    "\\end{equation}\n",
    "\n",
    "We can then apply our row operations ([scaling and adding](./023-linsys-gauss.ipynb#rowop1), [swapping](./024-linsys-perms.ipynb#rowop2)) to [](#big_augm) to reduce $A$ to upper triangular form\n",
    "$$\n",
    "M = \\left[ \\begin{array}{c|c} A & I  \\end{array}\\right] \\to N = \\left[ \\begin{array}{c|c} U & C  \\end{array}\\right],\n",
    "$$\n",
    "which is equivalent to reducing the original $n$ linear systems to \n",
    "$$\n",
    "U\\textbf{x}_1 = \\textbf{c}_1, U\\textbf{x}_2 = \\textbf{c}_2, \\cdots, U\\textbf{x}_n = \\textbf{c}_n,\n",
    "$$\n",
    "which we could then solve via back substituion for the columns $\\vv x_i$ of the matrix inverse $X$.\n",
    "\n",
    "For example, consider the following $A$ matrix, corresponding augmented matrix $M$, and corresponding upper triangular form\n",
    "\\begin{equation}\n",
    "\\label{U_I}\n",
    "A = \\begin{bmatrix}0 & 2 & 1 \\\\ 2 & 6 & 1 \\\\ 1 & 1 & 4 \\end{bmatrix},\\ M =  \n",
    "\\left[ \\begin{array}{ccc|ccc} 0 & 2 & 1 & 1 & 0 & 0 \\\\ 2 & 6 & 1 & 0 & 1 & 0 \\\\ 1 & 1 & 4 & 0 & 0 & 1 \\end{array}\\right] \\to N = \\left[ \\begin{array}{ccc|ccc} 2 & 6 & 1 & 0 & 1 & 0 \\\\ 0 & 2 & 1 & 1 & 0 & 0 \\\\ 0 & 0 & \\frac{9}{2} & 1 & -\\frac{1}{2} & 1 \\end{array}\\right].\n",
    "\\end{equation}\n",
    "\n",
    "Although we could stop here, it's worth highlighting that a more common version of GJE continues to apply row operations to fully reduce the augmented matrix to the form $\\left[ \\begin{array}{c|c} I & X  \\end{array}\\right]$ so that $X$ is the inverse of $A$. \n",
    "\n",
    "We first note that both $U$ and $I$ have zeros below the diagonal, so this is a good start!  However, in our current form $\\left[ \\begin{array}{c|c} U & C  \\end{array}\\right] $, the diagonal pivots of $U$ are not $1$.  We need another row operation!\n",
    "\n",
    "```{prf:observation} Elementary Row Operation \\#3\n",
    ":label: rowop3\n",
    "Multiplying a row of the augmented matrix by a nonzero scalar yields an equivalent linear system.\n",
    "```\n",
    "The [scaling operation](#rowop3) on $N$ in [](#U_I) reduces the augmented matrix to\n",
    "\\begin{equation}\n",
    "\\label{V}\n",
    "N = \\left[ \\begin{array}{ccc|ccc} 2 & 6 & 1 & 0 & 1 & 0 \\\\ 0 & 2 & 1 & 1 & 0 & 0 \\\\ 0 & 0 & \\frac{9}{2} & 1 & -\\frac{1}{2} & 1 \\end{array}\\right] \\to\\left[ \\begin{array}{c|c} V & B \\end{array}\\right] = \\left[ \\begin{array}{ccc|ccc} 1 & 3 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\ 0 & 1 & \\frac{1}{2} & \\frac{1}{2} & 0 & 0 \\\\ 0 & 0 & 1 & \\frac{2}{9} & -\\frac{1}{9} & \\frac{2}{9} \\end{array}\\right],\n",
    "\\end{equation}\n",
    "\n",
    "where we divide each row by its corresponding pivot. Now, to make $V$ identity, we perform use the same idea as we did in Gaussian Elimination, but to zero out entries _above the pivot_.  In this case, we start with the $(3,3)$ pivot to zero out the $(2, 3)$ and $(1,3)$ entries, and then use the $(2,2)$ pivot to zero out the $(2,1)$ entry.\n",
    "\\begin{equation}\n",
    "\\label{I_X}\n",
    "\\left[ \\begin{array}{c|c} V & B \\end{array}\\right] = \\left[ \\begin{array}{ccc|ccc} 1 & 3 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\ 0 & 1 & \\frac{1}{2} & \\frac{1}{2} & 0 & 0 \\\\ 0 & 0 & 1 & \\frac{2}{9} & -\\frac{1}{9} & \\frac{2}{9} \\end{array}\\right] \\to \\left[ \\begin{array}{ccc|ccc} 1 & 0 & 0 & -\\frac{23}{18} & \\frac{7}{18} & \\frac{2}{9} \\\\ 0 & 1 & 0 & \\frac{7}{18} & \\frac{1}{18} & -\\frac{1}{9} \\\\ 0 & 0 & 1 & \\frac{2}{9} & -\\frac{1}{9} & \\frac{2}{9} \\end{array}\\right]\n",
    "\\end{equation}\n",
    "Finally, the right-hand matrix in [](#I_X) is the inverse of $A$:\n",
    "$$\n",
    "A^{-1} = \\left[ \\begin{array}{ccc}  -\\frac{23}{18} & \\frac{7}{18} & \\frac{2}{9} \\\\  \\frac{7}{18} & \\frac{1}{18} & -\\frac{1}{9} \\\\  \\frac{2}{9} & -\\frac{1}{9} & \\frac{2}{9} \\end{array}\\right]\n",
    "$$\n",
    "\n",
    ":::{hint} Scaling Elementary Matrix\n",
    "What is the [elementary matrix](./023a-linsys-LU.ipynb#elementary) corresponding to the [scaling operation](#row-op3)?  Starting with the identify matrix as before, we see that scaling the $i$th row by a scalar $a\\neq 0$ only affects the $(i,i)$ entry which becomes $a$, instead of $1$.  For example, the elementary matrix associated that scales the 2nd row of a 3-row matrix by 4 is\n",
    "$$\n",
    "E = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}.\n",
    "$$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3175d4",
   "metadata": {},
   "source": [
    "````{exercise}  Gauss-Jordan Elimination\n",
    ":label: inverses-ex3\n",
    "\n",
    "Given the matrix\n",
    "\n",
    "\\begin{align*}\n",
    "A = \\bm 1&1&1 \\\\ 1&2&4 \\\\ 1&3&9 \\em\n",
    "\\end{align*}\n",
    "\n",
    "Find $A^{-1}$ using Gauss-Jordan elimination.\n",
    "\n",
    "```{solution} inverses-ex3\n",
    ":class: dropdown\n",
    "\n",
    "Autmenting $A$ to the identity, we get\n",
    "\n",
    "\\begin{align*}\n",
    "    \\left[ \\begin{array}{ccc|ccc} \n",
    "    1 & 1 & 1 &     1 & 0 & 0\\\\\n",
    "    1 & 2 & 4 &     0 & 1 & 0\\\\\n",
    "    1 & 3 & 9 &     0 & 0 & 1\n",
    "    \\end{array}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "Using Row 1 to eliminate Column 1, we get\n",
    "\n",
    "\\begin{align*}\n",
    "    \\left[ \\begin{array}{ccc|ccc} \n",
    "    1 & 1 & 1 &     1 & 0 & 0\\\\\n",
    "    0 & 1 & 3 &     -1 & 1 & 0\\\\\n",
    "    0 & 2 & 8 &     -1 & 0 & 1\n",
    "    \\end{array}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "Using Row 2 to eliminate Column 2, we get\n",
    "\n",
    "\\begin{align*}\n",
    "    \\left[ \\begin{array}{ccc|ccc} \n",
    "    1 & 0 & -2 &     2 & -1 & 0\\\\\n",
    "    0 & 1 & 3 &     -1 & 1 & 0\\\\\n",
    "    0 & 0 & 2 &     1 & -2 & 1\n",
    "    \\end{array}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "Using Row 3 to eliminate Column 3, we get\n",
    "\n",
    "\\begin{align*}\n",
    "    \\left[ \\begin{array}{ccc|ccc} \n",
    "    1 & 0 & 0 &     3 & -3 & 1\\\\\n",
    "    0 & 1 & 0 &     -2.5 & 4 & -1.5\\\\\n",
    "    0 & 0 & 2 &     1 & -2 & 1\n",
    "    \\end{array}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "Scaling Row 3 so it's entry is one,\n",
    "\n",
    "\\begin{align*}\n",
    "    \\left[ \\begin{array}{ccc|ccc} \n",
    "    1 & 0 & 0 &     3 & -3 & 1\\\\\n",
    "    0 & 1 & 0 &     -2.5 & 4 & -1.5\\\\\n",
    "    0 & 0 & 1 &     0.5 & -1 & 0.5\n",
    "    \\end{array}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "Hence we have found that\n",
    "\n",
    "\\begin{align*}\n",
    "    A^{-1} = \\bm 3 & -3 & 1\\\\ -2.5 & 4 & -1.5\\\\0.5 & -1 & 0.5 \\em\n",
    "\\end{align*}\n",
    "\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e6b1da-e6cf-4009-9603-58023cee6b91",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/00_Ch_1_Linear_Algebraic_Systems/025-linsys-inverses.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
