{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 10.1 SVD Theory\n",
    "subject:  SVD\n",
    "subtitle: \n",
    "short_title: 10.1 SVD Theory\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: \n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/09_Ch_10_SVD_Apps/111-SVD.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 18 - Singular Values and the Singular Value Decomposition.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be ALA 8.7 and LAA 7.4\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup\n",
    "\n",
    "The diagonalization theorems we've seen for complete and symmetric matrices have played a role in many interesting applications. Unfortunately, not all matrices can be factored as $A = PDP^{-1}$ for a diagonal matrix $D$; for example such a factorization makes no sense if $A$ is not square! Fortunately, a factorization $A = P \\Delta Q^T$ is possible for any matrix $m \\times n$ matrix $A$! A special factorization of this type, called the _singular value decomposition_, is one of the most useful and widely applicable matrix factorizations in linear algebra.\n",
    "\n",
    "The singular value decomposition is based on the following key property of matrix diagonalization which we'll show can be captured in general rectangular matrices:\n",
    "\n",
    ":::{note} Key Observation\n",
    "The absolute values of the eigenvalues of a symmetric matrix $A$ measure the amounts that $A$ stretches or shrinks certain vectors (the eigenvectors). If $A \\vv x = \\lambda \\vv x$ and $\\|\\vv x\\| = 1$, then\n",
    "\n",
    "$$\n",
    "\\|A \\vv x\\| = \\lambda \\|\\vv x\\| = |\\lambda| \\|\\vv x\\| = |\\lambda|.\n",
    "$$\n",
    "\n",
    "If $\\lambda_1$ is the eigenvalue with the greatest magnitude, i.e., if $|\\lambda_1| \\geq |\\lambda_i|$ for $i=1,\\ldots,n$, then a corresponding unit eigenvector $\\vv v_1$ identifies the direction in which stretching is greatest. That is, the length of $A \\vv x$ is maximized when $\\vv x = \\vv v_1$, and $\\|A\\vv v_1\\| = |\\lambda_1|$.\n",
    ":::\n",
    "\n",
    "The above description is reminiscent of the [optimization principle](../08_Ch_9_Symmetric_Matrices/103-opt_princ.ipynb#opt_eig_thm) we saw for characterizing eigenvalues of symmetric matrices, albeit with a focus on maximizing length $\\|A\\vv x\\|$ rather than the quadratic form $\\vv x^T A \\vv x$. What we'll see next is that this description of $\\vv v_1$ and $|\\lambda_1|$ has an analogue for rectangular matrices that will lead to the singular value decomposition.\n",
    "\n",
    "::::{prf:example}\n",
    ":label: eg_1\n",
    "The matrix $A = \\begin{bmatrix} 4 & 11 & 14 \\\\ 8 & 7 & -2 \\end{bmatrix}$ defines a linear map $\\vv x \\mapsto A \\vv x$ from $\\mathbb{R}^3$ to $\\mathbb{R}^2$. If we consider the effects of this map on the unit sphere $\\{\\vv x \\in \\mathbb{R}^3 \\mid \\|\\vv x\\| = 1\\}$, we observe that multiplication by $A$ transforms this sphere in $\\mathbb{R}^3$ into an ellipse in $\\mathbb{R}^2$:\n",
    "\n",
    ":::{figure}../figures/10-sphere_ellipse.jpg\n",
    ":label:sphere_ellipse\n",
    ":alt:Sphere to Ellipse\n",
    ":width: 400px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    "Our task is to find a unit vector $\\vv x$ at which the length $\\|A\\vv x\\|$ is maximized, and compute this maximum length. That is, we want to solve the optimization problem:\n",
    "\n",
    "$$\n",
    "\\text{maximize} \\|A \\vv x\\|\n",
    "$$\n",
    "\n",
    "over choices of $\\vv x$ satisfying $\\|\\vv x\\|=1$. Our first observation is that the quantity $\\|A\\vv x\\|^2$ is maximized by the same $\\|\\vv x\\|$ that maximizes $\\|A\\vv x\\|$, but that $\\|A\\vv x\\|^2$ is easier to work with. Specifically, note that\n",
    "\n",
    "$$\n",
    "\\|A\\vv x\\|^2 = \\langle A \\vv x, A\\vv x \\rangle = (A\\vv x)^T(A\\vv x) = \\vv x^TA^TA\\vv x = \\vv x^T(A^TA)\\vv x.\n",
    "$$\n",
    "\n",
    "So our task is to now find a unit vector $\\|\\vv x\\|=1$ that maximizes the quadratic form $\\vv x^T(A^TA)\\vv x$ defined by the symmetric (positive semidefinite) matrix $A^{T}A$: we know how to do this. By our [theorem](../08_Ch_9_Symmetric_Matrices/103-opt_princ.ipynb#opt_eig_thm) characterizing eigenvalues of symmetric matrices from an optimization perspective, we know the maximum value is the largest eigenvalue $\\lambda_1$ of the matrix $A^TA$, and is attained at the unit eigenvector $\\vv v_1$ of $A^TA$ corresponding to $\\lambda_1$.\n",
    "\n",
    "For the matrix in this example:\n",
    "\n",
    "$$\n",
    "A^TA = \\begin{bmatrix}\n",
    "4 & 8 \\\\\n",
    "11 & 7  \\\\\n",
    "14 & -2 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "4 & 11 & 14 \\\\\n",
    "8 & 7 & -2\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "80 & 100 & 40 \\\\\n",
    "100 & 170 & 140 \\\\\n",
    "40 & 140 & 200\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and the eigenvalue/vector pairs are:\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_1 = 360, \\quad \\vv v_1 &= \\begin{bmatrix} \\frac{1}{3} \\\\ \\frac{2}{3} \\\\ \\frac{2}{3} \\end{bmatrix}, \\quad\n",
    "\\lambda_2 = 90, \\quad \\vv v_2 &= \\begin{bmatrix} -\\frac{2}{3} \\\\ -\\frac{1}{3} \\\\ \\frac{2}{3} \\end{bmatrix}, \\quad\n",
    "\\lambda_3 = 0, \\quad \\vv v_3 &= \\begin{bmatrix} \\frac{2}{3} \\\\ -\\frac{2}{3} \\\\ \\frac{1}{3} \\end{bmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "The maximum value of $\\vv x^T(A^TA)\\vv x = \\|A\\vv x\\|^2$ is thus $\\lambda_1 = 360$, and attained when $\\vv x = \\vv v_1$. The vector $A\\vv v_1$ is a point on the ellipse in [](#sphere_ellipse) farthest from the origin, namely\n",
    "\n",
    "$$\n",
    "A\\vv v_1 = \\begin{bmatrix}\n",
    "4 & 11 & 14 \\\\\n",
    "8 & 7 & -2\n",
    "\\end{bmatrix}\n",
    " \\begin{bmatrix} \\frac{1}{3} \\\\ \\frac{2}{3} \\\\ \\frac{2}{3} \\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "18 \\\\\n",
    "6\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "For $\\|\\vv x\\| = 1$, the maximum value of $\\|A\\vv x\\|$ is $\\|A\\vv v_1\\| = \\sqrt{360} = 6\\sqrt{10}$.\n",
    "\n",
    "This example suggests that the effect of a matrix $A$ on the unit sphere in $\\mathbb{R}^3$ is related to the quadratic form $\\vv x^T A^TA \\vv x$. What we'll see next is that _the entire geometric behavior of the map $\\vv x \\mapsto A \\vv x$_ is captured by this quadratic form.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Singular Values of an $m \\times n$ Matrix\n",
    "\n",
    "Consider an $m \\times n$ real matrix $A \\in \\mathbb{R}^{m \\times n}$. Then $A^TA$ is an $n \\times n$ symmetric matrix, and can be orthogonally diagonalized. Let $V = \\bm \\vv v_1 & \\cdots & \\vv v_n\\em$ be an orthogonal matrix composed of orthonormal eigenvectors of $A^TA$, and let $\\lambda_1, \\ldots, \\lambda_n$ be the associated eigenvalues of $A^TA$. Then for $i = 1, \\ldots, n$,\n",
    "\n",
    "\\begin{align*}\n",
    "\\|A \\vv v_i\\|^2 &= (A \\vv v_i)^T(A \\vv v_i) = \\vv v_i^T(A^TA \\vv v_i) \\\\\n",
    "&= \\vv v_i^T(\\lambda_i \\vv v_i) \\\\\n",
    "&= \\lambda_i \\vv v_i^T \\vv v_i = \\lambda_i \\|\\vv v_i\\|^2 \\\\\n",
    "&= \\lambda_i.\n",
    "\\end{align*}\n",
    "\n",
    "This tells us that all of the eigenvalues $\\lambda_i = \\|A \\vv v_i\\|^2 \\geq 0$, since norms can only take on nonnegative values. $A^TA$ is a positive semidefinite matrix. Let's assume that we've ordered our eigenvalues in decreasing order:\n",
    "\n",
    "$$\n",
    "\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_n \\geq 0.\n",
    "$$\n",
    "\n",
    "The _singular values of $A$_ are the positive square roots of the nonzero eigenvalues $\\lambda_i > 0$ of $A^TA$ denoted $\\sigma_i$. That is, let $\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_r > 0$ and $\\lambda_{r+1} = \\lambda_{r+2} = \\cdots = \\lambda_n = 0$ be a partition of the eigenvalues such that $\\lambda_i > 0$ for $i = 1, \\ldots, r$, and $\\lambda_i = 0$ for $i = r+1, \\ldots, n$. Then $A$ has $r$ singular values, defined as\n",
    "$$\n",
    "\\sigma_i = \\sqrt{\\lambda_i}, \\quad i = 1, \\ldots, r.\n",
    "$$\n",
    "\n",
    ":::{warning}\n",
    "Some texts include the zero eigenvalues $\\lambda_{r+1}, \\ldots, \\lambda_n$ of $A^TA$ as singular values of $A$. This is simply a different convention and is mathematically equivalent. However, we find our definition to be more natural for our purposes.\n",
    ":::\n",
    "\n",
    "::::{prf:example} \n",
    ":label: major_minor_eg\n",
    "Using the same $A = \\begin{bmatrix} 4 & 11 & 14 \\\\ 8 & 7 & -2 \\end{bmatrix}$ as the previous example,\n",
    "we have $\\sigma_1 = \\sqrt{360} = 6\\sqrt{10}$, $\\sigma_2 = \\sqrt{90} = 3\\sqrt{10}$. In this case, $A$ only has two singular values as $\\lambda_3 = 0$. For this example, $r = 2$, and $\\lambda_1 = 360 > \\lambda_2 = 90 > \\lambda_3 = 0$.\n",
    "\n",
    "From the previous example, the first singular value of $A$ is the maximum of $\\|A\\vv x\\|$ over all $\\|\\vv x\\| = 1$, attained at $\\vv v_1$. Our optimization based characterization of eigenvalues of symmetric matrices tells us that the second singular value of $A$ is the maximum of $\\|A\\vv x\\|$ over all unit vectors _orthogonal_ to $\\vv v_1$: this is attained by $\\vv u_2$, the second eigenvector of $A^TA$. For $\\vv u_2$ from the previous example:\n",
    "\n",
    "$$\n",
    "A \\vv u_2 = \\begin{bmatrix} 4 & 11 & 14 \\\\ 8 & 7 & -2 \\end{bmatrix} \\begin{bmatrix} -\\frac{2}{3} \\\\ -\\frac{1}{3} \\\\ \\frac{2}{3} \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ -9 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This point is on the minor axis of the ellipse in [](#sphere_ellipse), just as $A \\vv v_1$ is on the major axis (see [](#major_minor) below). The two singular values of $A$ are the lengths of the major and mini semiaxes of thes ellipse.\n",
    ":::{figure}../figures/10-major_minor.jpg\n",
    ":label:major_minor\n",
    ":alt:Major and Minor Axes\n",
    ":width: 300px\n",
    ":align: center\n",
    ":::\n",
    "That $A\\vv v_1$ and $A\\vv v_2$ are orthogonal is no accident, as the next theorem shows.\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::{prf:theorem}\n",
    ":label: major_minor_thm\n",
    "Suppose that $\\vv u_1, \\ldots, \\vv u_n$ is an orthonormal basis for $\\mathbb{R}^n$ composed of the eigenvectors of $A^TA$, ordered so that the corresponding eigenvalues of $A^TA$ satisfy\n",
    "$$\n",
    "\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_r > \\lambda_{r+1} = \\cdots = \\lambda_n = 0,\n",
    "$$\n",
    "where $r$ denotes the number of nonzero eigenvalues of $A^TA$, i.e. the number of singular values $\\sigma_i = \\sqrt{\\lambda_i} > 0$, $i=1,\\ldots,r$ of $A$. Then, $A \\vv u_1, \\ldots, A\\vv u_r$ is an orthogonal basis for Col$(A)$, and rank$(A) = r$.\n",
    "\n",
    ":::{prf:proof} Proof of [](#major_minor_thm)\n",
    ":label: proof-major_minor_thm\n",
    ":class: dropdown\n",
    "Because $\\vv v_i$ and $\\lambda _j \\vv v_j$ are orthogonal for $i \\neq j$,\n",
    "$$\n",
    "(A\\vv v_i)^T(A \\vv v_j) = \\vv v_i^TA^TA \\vv v_j = \\vv v_i^T\\lambda_j \\vv v_j = 0.\n",
    "$$\n",
    "\n",
    "Thus, $A\\vv v_1, \\ldots, A\\vv v_r$ are mutually orthogonal, and hence linearly independent. They are also clearly contained in $\\text{Col}(A)$. Now, for any $\\vv y \\in \\text{Col}(A)$, there must be an $\\vv x \\in \\mathbb{R}^n$ such that $\\vv y = A\\vv x$. Expanding $\\vv x$ in the basis $\\vv v_1, \\ldots, \\vv v_n$, as $\\vv x = c_1\\vv v_1 + \\cdots + c_n\\vv v_n$ for some $c_1, \\ldots, c_n \\in \\mathbb{R}$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\vv y &= A\\vv x = A(c_1\\vv v_1 + \\cdots + c_n\\vv v_n) = c_1A\\vv v_1 + \\cdots + c_rA\\vv v_r + c_{r+1}A\\vv v_{r+1} + \\cdots + c_nA\\vv v_n \\\\\n",
    "&= c_1A\\vv v_1 + \\cdots + c_rA\\vv v_r.\n",
    "\\end{align*}\n",
    "\n",
    "We used that $\\|A\\vv v_i\\|^2 = \\lambda_i = 0$ for $i=r+1,\\ldots,n \\Leftrightarrow A\\vv v_i = 0$ for $i=r+1,\\ldots,n$ in the last equality.\n",
    "\n",
    "Therefore, we have that $\\vv y \\in \\text{span}\\{A\\vv v_1, \\ldots, A\\vv v_r\\}$. Thus $A\\vv v_1, \\ldots, A\\vv v_r$ is both linearly independent and a spanning set for Col$(A)$, meaning it is an orthogonal basis for Col$(A)$. Hence, by the Fundamental Theorem of Linear Algebra,\n",
    "\n",
    "$$\n",
    "\\text{rank}(A) = \\text{dim Col}(A) = r.\n",
    "$$\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    ":::{note}Numerical Note\n",
    "In certain cases, the rank of $A$ may be very sensitive to small changes in the entries of $A$. The obvious approach of counting the \\# of pivot columns in $A$ does not work well if $A$ is row reduced by a computer, as roundoff errors often create a row echelon form with full rank. In practice, the most reliable way of computing the rank of a large matrix $A$ is to count the number of singular values larger than a small threshold $\\epsilon$ (typically on the order of $10^{-12}$, but can vary depending on applications). In this case, singular values smaller than $\\epsilon$ are treated as zeros for all practical purposes, and the _effective rank of $A$_ is computed by counting the remaining nonzero singular values.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Singular Value Decomposition\n",
    "\n",
    "The decomposition of $A$ involves an $r \\times r$ diagonal matrix $\\Sigma$ of the form\n",
    "\n",
    "$$\n",
    "\\Sigma = \\text{diag}(\\sigma_1, \\ldots, \\sigma_r).\n",
    "$$\n",
    "\n",
    "We note that because $r = \\text{dim Col}(A) = \\text{dim Row}(A)$ by the Fundamental Theorem of Linear Algebra, we must have that $r \\leq \\min\\{m,n\\}$ if $A \\in \\mathbb{R}^{m \\times n}$.\n",
    "\n",
    ":::{prf:theorem}\n",
    ":label: svd_thm\n",
    "Let $A \\in \\mathbb{R}^{m \\times n}$ be an $m \\times n$ matrix of rank $r > 0$. Then $A$ can be factored as\n",
    "$$\n",
    "A = U \\Sigma V^T,\n",
    "$$\n",
    "where $U \\in \\mathbb{R}^{m \\times r}$ has orthonormal columns, so $U^TU = I_r$, $\\Sigma = \\text{diag}(\\sigma_1, \\ldots, \\sigma_r)$ is a diagonal matrix with the singular values of $A$ $\\sigma_i$ along the diagonal, and $V \\in \\mathbb{R}^{n \\times r}$ has orthonormal columns, so $V^TV = I_r$.\n",
    ":::\n",
    "\n",
    "Such a factorization of $A$ is called its _singular value decomposition_, and the columns of $U$ are called the _left singular vectors_ of $A$, while the columns of $V$ are called the _right singular vectors of $A$_.\n",
    "\n",
    "::::{prf:proof} Proof of [](#svd_thm)\n",
    ":label: proof-svd_thm\n",
    ":class: dropdown\n",
    "Let $\\lambda_i$ and $\\vv v_i$ be the eigenvalues/vectors of $A^TA$ as described previously, so that $A\\vv v_1, \\ldots, A\\vv v_r$ is an orthogonal basis for col$(A)$. Normalize each $A\\vv v_i$ to form an orthonormal basis for col$(A)$:\n",
    "\n",
    "$$\n",
    "\\vv u_i := \\frac{1}{\\|A\\vv v_i\\|} A\\vv v_i = \\frac{1}{\\sigma_i} A\\vv v_i\n",
    "$$\n",
    "\n",
    "and hence $A\\vv v_i = \\sigma_i \\vv u_i$ for $i=1,\\ldots,r$. Define the matrices\n",
    "\n",
    "$$\n",
    "U = \\bm \\vv u_1 & \\cdots  & \\vv u_r\\em  \\in \\mathbb{R}^{m \\times r} \\quad \\text{and} \\quad V = \\bm \\vv v_1 & \\cdots & \\vv v_r\\em \\in \\mathbb{R}^{n \\times r}\n",
    "$$\n",
    "\n",
    "By construction, the columns of $U$ are orthonormal: $U^TU = I_r$, and similarly for the columns of $V$: $V^TV = I_r$.\n",
    "\n",
    "Let's define the following \"full\" matrices:\n",
    "\n",
    "$$\n",
    "\\hat{U} = \\bm U &  U^\\perp \\em \\in \\mathbb{R}^{m \\times m} \\quad \\text{and} \\quad \\hat{V} = \\bm V & V^\\perp\\em \\in \\mathbb{R}^{n \\times n}\n",
    "$$\n",
    "\n",
    "Here, $V^\\perp = \\bm \\vv v_{r+1} & \\cdots & \\vv  v_n\\em$ has orthonormal columns spanning the orthogonal complement of span$\\{\\vv v_1, \\ldots, \\vv v_r\\}$, so that the columns of $\\hat{V}$ form an orthonormal basis of $\\mathbb{R}^n$.\n",
    "\n",
    "Similarly, let $U^\\perp$ have orthonormal columns spanning the orthogonal complement of span$\\{\\vv u_1, \\ldots, \\vv u_r\\}$, so the columns of $\\hat{U}$ form an orthonormal basis for $\\mathbb{R}^m$.\n",
    "\n",
    "Finally, define \n",
    "$$\n",
    "\\hat{\\Sigma} &= \\begin{bmatrix}  \\overbrace{\\Sigma}^d & \\quad \\overbrace{0}^{n-r}\\} r \\\\ 0 & \\quad \\quad \\quad 0 \\} m-r \\end{bmatrix}\n",
    "$$. \n",
    "We first show that\n",
    "\n",
    "$$\n",
    "A = \\hat{U} \\hat{\\Sigma} \\hat{V}^T, \\quad \\text{or equivalently (since $\\hat{V}$ is orthogonal),} \\quad A\\hat{V} = \\hat{U}\\hat{\\Sigma}.\n",
    "$$\n",
    "\n",
    "First,\n",
    "$$\n",
    "A\\hat{V} = \\bm A\\vv v_1 & \\cdots & A\\vv v_r & A\\vv v_{r+1} & \\cdots & A\\vv v_n\\em = \\bm \\sigma_1 \\vv u_1 & \\cdots & \\sigma_r\\vv u_r & \\vv 0 & \\cdots & \\vv 0\\em.\n",
    "$$\n",
    "\n",
    "Then, notice:\n",
    "\n",
    "$$\n",
    "\\hat{U} \\hat{\\Sigma} = \\bm \\vv u_1 \\cdots \\vv u_r & \\vv u_{r+1} & \\cdots & \\vv u_m\\em \n",
    "\\begin{bmatrix}\n",
    "\\sigma_1 & & 0 & 0 \\cdots 0 \\\\\n",
    "& \\ddots & & \\vdots \\\\\n",
    "0 & & \\sigma_r & 0 \\cdots 0 \\\\\n",
    "0 & \\cdots & 0 & 0 \\cdots 0 \\\\\n",
    "\\vdots & & \\vdots & \\vdots \\\\\n",
    "0 & \\cdots & 0 & 0 \\cdots 0\n",
    "\\end{bmatrix}\n",
    "= \\bm \\sigma_1 \\vv u_1 & \\cdots & \\sigma_r \\vv u_r & \\vv 0 \\cdots \\vv 0\\em.\n",
    "$$\n",
    "\n",
    "So that $A\\hat{V} = \\hat{U}\\hat{\\Sigma}$, or equivalently, $A = \\hat{U}\\hat{\\Sigma}\\hat{V}^T$. But, now, notice:\n",
    "\n",
    "$$\n",
    "A = \\hat{U} \\hat{\\Sigma} \\hat{V}^T = \\bm U & U^\\perp\\em  \\begin{bmatrix} \\Sigma & 0 \\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} V^T \\\\ (V^\\perp)^T \\end{bmatrix} = U \\Sigma V^T,\n",
    "$$\n",
    "proving our result.\n",
    "\n",
    ":::{note} \n",
    "Some textbooks define the singular value decomposition of $A$ as $A = \\hat{U} \\hat{\\Sigma} \\hat{V}^T$ --- this is necessary when allowing for singular values equal to zero. When only considering nonzero singular values, as we do, $A = U\\Sigma V^T$ is the appropriate definition. This is sometimes called the _compact SVD of $A$_, but we will just call it the SVD.\n",
    ":::\n",
    "\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{prf:example} \n",
    ":label: svd_thm_eg\n",
    "Let's use the results of the previous example to construct the SVD of\n",
    "$A = \\begin{bmatrix} 4 & 11 & 14 \\\\ 8 & 7 & -2 \\end{bmatrix}$.\n",
    "\n",
    "**Step 1: Find an orthogonal diagonalization of $A^TA$**. In general, for $A$ with many columns, this is done numerically, but we use the data from before:\n",
    "\n",
    "$$\n",
    "A^TA = \\hat{V} \\Lambda \\hat{V}^T = \\bm \\vv v_1 & \\vv v_2 & \\vv v_3 \\em \\begin{bmatrix}\n",
    "\\lambda_1 & & \\\\\n",
    "& \\lambda_2 & \\\\\n",
    "& & \\lambda_3\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "\\vv v_1^T \\\\\n",
    "\\vv v_2^T \\\\\n",
    "\\vv v_3^T\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "with $(\\lambda_i, \\vv v_i)$ as spectral above with $\\lambda_1 = 360$, $\\lambda_2 = 90$, $\\lambda_3 = 0$.\n",
    "\n",
    "**Step 2: Setup $V$ and $\\Sigma$**: Arrange the nonzero eigenvalues of $A^TA$ in decreasing order and compute the singular values. For this example:\n",
    "\n",
    "$$\n",
    "\\sigma_1 = 6\\sqrt{10} \\ \\text{and} \\ \\sigma_2 = 3\\sqrt{10}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\Sigma = \\text{diag}(\\sigma_1, \\sigma_2) = \\begin{bmatrix}\n",
    "6\\sqrt{10} & 0 \\\\\n",
    "0 & 3\\sqrt{10}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "Hence rank$ A = 2$, and $V \\in \\mathbb{R}^{3\\times2}$.\n",
    "\n",
    "The corresponding eigenvectors define the columns of $V$:\n",
    "\n",
    "$$\n",
    "V = \\bm \\vv v_1 & \\vv v_2\\em = \\begin{bmatrix}\n",
    "\\frac{1}{3} & -\\frac{2}{3} \\\\\n",
    "\\frac{2}{3} & -\\frac{1}{3} \\\\\n",
    "\\frac{2}{3} & \\frac{2}{3}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Step 3 Construct $U$**: Since rank$ A = 2$, $U \\in \\mathbb{R}^{2\\times2}$. The columns of $U$ are given by $A \\vv v_1$ and $A\\vv v_2$. Recall we showed above that $\\|A \\vv v_1\\| = \\sigma_1$ and $\\|A \\vv v_2\\| = \\sigma_2$. So $U = \\bm \\vv u_1 & \\vv u_2\\em$ with\n",
    "\n",
    "$$\n",
    "\\vv u_1 = \\frac{A\\vv v_1}{\\sigma_1} = \\frac{1}{6\\sqrt{10}} \\begin{bmatrix}\n",
    "18 \\\\\n",
    "6\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\frac{3}{\\sqrt{10}} \\\\\n",
    "\\frac{1}{\\sqrt{10}}\n",
    "\\end{bmatrix} \\ \\text{and} \\\\\n",
    "\\vv u_2 = \\frac{A\\vv v_2}{\\sigma_2} = \\frac{1}{3\\sqrt{10}} \\begin{bmatrix}\n",
    "3 \\\\\n",
    "-9\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\frac{1}{\\sqrt{10}} \\\\\n",
    "-\\frac{3}{\\sqrt{10}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Finally, the SVD of $A$ is:\n",
    "\n",
    "$$\n",
    "A = \\underbrace{\\begin{bmatrix}\n",
    "\\frac{3}{\\sqrt{10}} & \\frac{1}{\\sqrt{10}} \\\\\n",
    "\\frac{1}{\\sqrt{10}} & -\\frac{3}{\\sqrt{10}}\n",
    "\\end{bmatrix}}_U \\underbrace{\\begin{bmatrix}\n",
    "6\\sqrt{10} & 0 \\\\\n",
    "0 & 3\\sqrt{10}\n",
    "\\end{bmatrix}}_{\\Sigma} \\underbrace{\\begin{bmatrix}\n",
    "\\frac{1}{3} & \\frac{2}{3} & \\frac{2}{3} \\\\\n",
    "-\\frac{2}{3} & -\\frac{1}{3} & \\frac{2}{3}\n",
    "\\end{bmatrix}}_{V^T}\n",
    "$$\n",
    "\n",
    "You can check that indeed $A = U\\Sigma V^T$ here, and that $U^TU = V^TV = I_2$\n",
    ":::\n",
    "\n",
    "**TO DO**: Please add example 4 from LAA 7.1, subtly modified to use the compact SVD form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/09_Ch_10_SVD_Apps/111-SVD.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
