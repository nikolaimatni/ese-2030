{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 10.1 Applications\n",
    "subject:  SVD\n",
    "subtitle: \n",
    "short_title: 10.1 Applications\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: \n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/08_Ch_9_Symmetric_Matrices/102-Apps.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 17 - Introduction to Graph Theory and Consensus Protocols.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section*{Warmup}\n",
    "\n",
    "The diagonalization theorems we've seen for complete and symmetric matrices have played a role in many interesting applications. Unfortunately, not all matrices can be diagonalized. As we've seen before, if $A$ is not square, even a factorization makes no sense. If $A$ is not square, fortunately a factorization $A = P \\Delta Q^T$ is possible for any matrix $A$. A special factorization of this type, called the singular value decomposition, is one of the most useful and widely applicable matrix factorizations in linear algebra.\n",
    "\n",
    "The singular value decomposition is based on the following key points of matrix diagonalization which we'll show can be captured in general rectangular matrices:\n",
    "\n",
    "\\textcolor{magenta}{Key observation: The absolute values of the eigenvalues of a symmetric matrix $A$ measure the amounts that $A$ stretches or shrinks certain vectors (the eigenvectors). If $Ax = \\lambda x$ and $\\|x\\| = 1$, then}\n",
    "\n",
    "\\[\n",
    "\\|Ax\\| = \\lambda \\|x\\| = |\\lambda| \\|x\\| = |\\lambda|.\n",
    "\\]\n",
    "\n",
    "\\textcolor{magenta}{If $\\lambda_1$ is the eigenvalue with the greatest magnitude, i.e., if $|\\lambda_1| \\geq |\\lambda_i|$ for $i=1,\\ldots,n$, then a corresponding unit eigenvector $v_1$ identifies the direction in which stretching is greatest. That is, the length of $Ax$ is maximized when $x = v_1$, and $\\|Av_1\\| = |\\lambda_1|$.}\n",
    "\n",
    "This description is reminiscent of the optimization principle we saw for characterizing eigenvalues of symmetric matrices, albeit with a focus on maximizing length $\\|Ax\\|$ rather than the quadratic form $x^T A x$. What we'll see next is that this description of $v_1$ and $|\\lambda_1|$ has an analogue for rectangular matrices that will lead to the singular value decomposition.\n",
    "\n",
    "Example: The matrix $A = \\begin{bmatrix} 4 & 11 & 14 \\\\ 8 & 7 & -2 \\end{bmatrix}$ defines a linear map $x \\mapsto Ax$ from $\\mathbb{R}^3$ to $\\mathbb{R}^2$. If we consider the effects of this map on the unit sphere $\\{x \\in \\mathbb{R}^3 \\mid \\|x\\| = 1\\}$, we observe that multiplication by $A$ transforms this sphere in $\\mathbb{R}^3$ into an ellipse in $\\mathbb{R}^2$:\n",
    "\n",
    "\\begin{center}\n",
    "\\includegraphics[width=0.8\\textwidth]{figure1.png}\n",
    "\\captionof{figure}{A transformation from $\\mathbb{R}^3$ to $\\mathbb{R}^2$.}\n",
    "\\end{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to find a unit vector $x$ that maximizes the length $\\|Ax\\|$ is maximized, and compute this maximum length. That is, we want to solve the optimization problem:\n",
    "\n",
    "\\[\n",
    "\\max_{x \\in \\mathbb{R}^n} \\|Ax\\|\n",
    "\\]\n",
    "\n",
    "Our choice of $x$ satisfying $\\|x\\|=1$. Our first observation is that the quantity $\\|Ax\\|^2$ is easier to work with than $\\|Ax\\|$, but that $\\|Ax\\|^2$ is equal to $x^TA^TAx$. Specifically, note that\n",
    "\n",
    "\\[\n",
    "\\|Ax\\|^2 = \\langle Ax, Ax \\rangle = (Ax)^T(Ax) = x^TA^TAx = x^T(A^TA)x.\n",
    "\\]\n",
    "\n",
    "So our task is to now find a unit vector $\\|x\\|=1$ that maximizes the quadratic form $x^T(A^TA)x$. By the spectral (Courant-Fischer) theorem, we know how to do this. By our theorem concerning eigenpairs of symmetric matrices from an optimization perspective, we know the maximum value is the largest eigenvalue $\\lambda_1$ of the matrix $A^TA$, and is attained at the unit eigenvector $v_1$ of $A^TA$ corresponding to $\\lambda_1$.\n",
    "\n",
    "For the matrix in this example:\n",
    "\n",
    "\\[\n",
    "A^TA = \\begin{pmatrix}\n",
    "4 & 8 & 14 \\\\\n",
    "8 & 7 & -2 \\\\\n",
    "14 & -2 & 0\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "4 & 8 & 14 \\\\\n",
    "8 & 7 & -2 \\\\\n",
    "14 & -2 & 0\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "80 & 100 & 40 \\\\\n",
    "100 & 170 & 140 \\\\\n",
    "40 & 140 & 200\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "and the eigenvalue/vector pairs are:\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_1 = 360, \\quad v_1 &= \\begin{pmatrix} 1/3 \\\\ 2/3 \\\\ 2/3 \\end{pmatrix}, \\\\\n",
    "\\lambda_2 = 90, \\quad v_2 &= \\begin{pmatrix} -4/3 \\\\ -1/3 \\\\ 2/3 \\end{pmatrix}, \\\\\n",
    "\\lambda_3 = 0, \\quad v_3 &= \\begin{pmatrix} 2/3 \\\\ -2/3 \\\\ 1/3 \\end{pmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "The maximum value of $x^T(A^TA)x = \\|Ax\\|^2$ is thus $\\lambda_1 = 360$, and attained when $x = v_1$. The vector $Av_1$ is a point on the ellipse in Fig.1 above furthest from the origin, namely\n",
    "\n",
    "\\[\n",
    "Av_1 = \\begin{pmatrix}\n",
    "4 & 8 & 14 \\\\\n",
    "8 & 7 & -2 \\\\\n",
    "14 & -2 & 0\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "1/3 \\\\\n",
    "2/3 \\\\\n",
    "2/3\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "18 \\\\\n",
    "6 \\\\\n",
    "0\n",
    "\\end{pmatrix}.\n",
    "\\]\n",
    "\n",
    "For $\\|x\\| = 1$, the maximum value of $\\|Ax\\|$ is $\\|Av_1\\| = \\sqrt{360} = 6\\sqrt{10}$.\n",
    "\n",
    "This example suggests that the effect of a matrix $A$ on the unit sphere $\\|x\\| = 1$ is related to the eigenpairs of $A^TA$. In fact we'll see next that the entire geometric behavior of the map $x \\mapsto Ax$ is captured by this quadratic form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section*{The Singular Values of an m×n Matrix}\n",
    "\n",
    "Consider an m×n real matrix $A \\in \\mathbb{R}^{m \\times n}$. Then $A^TA$ is an n×n symmetric matrix, and can be orthogonally diagonalized. Let $V = [v_1 \\cdots v_n]$ be an orthogonal matrix composed of orthonormal eigenvectors of $A^TA$, and let $\\lambda_1, \\ldots, \\lambda_n$ be the associated eigenvalues of $A^TA$. Then for $i = 1, \\ldots, n$,\n",
    "\n",
    "\\begin{align*}\n",
    "\\|A v_i\\|^2 &= (Av_i)^T(Av_i) = v_i^T(A^TAv_i) \\\\\n",
    "&= v_i^T(\\lambda_i v_i) \\\\\n",
    "&= \\lambda_i v_i^Tv_i = \\lambda_i \\|v_i\\|^2 \\\\\n",
    "&= \\lambda_i.\n",
    "\\end{align*}\n",
    "\n",
    "This tells us that all of the eigenvalues $\\lambda_i = \\|Av_i\\|^2 \\geq 0$, i.e. $A^TA$ can only take on nonnegative values. $A^TA$ is a positive semidefinite matrix. Let's assume that we've ordered our eigenvalues in decreasing order:\n",
    "\n",
    "\\[\n",
    "\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_n \\geq 0.\n",
    "\\]\n",
    "\n",
    "The singular values of $A$ are the positive square roots of the nonzero eigenvalues $\\lambda_i > 0$ of $A^TA$ denoted $\\sigma_i$. That is, let $\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_r > 0$ and $\\lambda_{r+1} = \\cdots = \\lambda_n = 0$ be a partition of the eigenvalues such that $\\lambda_i > 0$ for $i = 1, \\ldots, r$, and $\\lambda_i = 0$ for $i = r+1, \\ldots, n$. Then $A$ has $r$ singular values, defined as\n",
    "\n",
    "\\[\n",
    "\\sigma_i = \\sqrt{\\lambda_i}, \\quad i = 1, \\ldots, r.\n",
    "\\]\n",
    "\n",
    "\\textbf{WARNING:} Some texts include the zero eigenvalues $\\lambda_{r+1}, \\ldots, \\lambda_n$ of $A^TA$ as singular values of $A$. This is simply a different convention and is mathematically equivalent. However, we find our definition to be more natural for our purposes.\n",
    "\n",
    "\\textbf{Example:} Using the same $A = \\begin{pmatrix} 4 & 8 & 14 \\\\ 8 & 7 & -2 \\\\ 14 & -2 & 0 \\end{pmatrix}$ as the previous example,\n",
    "\n",
    "we have $\\sigma_1 = \\sqrt{360} = 6\\sqrt{10}$, $\\sigma_2 = \\sqrt{90} = 3\\sqrt{10}$. In this case, $A$ only has two singular values as $\\lambda_3 = 0$. For this example, $r = 2$, and $\\lambda_1 = 360 > \\lambda_2 = 90 > \\lambda_3 = 0$.\n",
    "\n",
    "From the previous example, the first singular value of $A$ is the maximum of $\\|Ax\\|$ over all $\\|x\\| = 1$, attained at $v_1$. Our optimization-based characterization of eigenvectors of symmetric matrices tells us that the second singular value of $A$ is the maximum of $\\|Ax\\|$ over all unit vectors orthogonal to $v_1$; this is attained by $v_2$, the second eigenvector of $A^TA$. For $v_2$ from the previous example:\n",
    "\n",
    "\\[\n",
    "Av_2 = \\begin{pmatrix} 4 & 8 & 14 \\\\ 8 & 7 & -2 \\\\ 14 & -2 & 0 \\end{pmatrix} \\begin{pmatrix} -4/3 \\\\ -1/3 \\\\ 2/3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -7 \\\\ -7 \\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "This point is on the minor axis of the ellipse in Fig. 1 above, just as $Av_1$ is on the major axis (see Fig. 2 below). The two singular values of $A$ are the lengths of the semi-axes of this ellipse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "of the major and minor semiaxes of the ellipse\n",
    "\n",
    "\\begin{figure}[h]\n",
    "\\centering\n",
    "\\includegraphics[width=0.5\\textwidth]{figure2}\n",
    "\\caption{An ellipse showing $Av_1$ and $Av_2$ as orthogonal vectors along the major and minor axes.}\n",
    "\\end{figure}\n",
    "\n",
    "That $Av_1$ and $Av_2$ are orthogonal is no accident, as the next theorem shows:\n",
    "\n",
    "\\begin{theorem}\n",
    "Suppose that $v_1, \\ldots, v_n$ is an orthonormal basis for $\\mathbb{R}^n$ composed of the eigenvectors of $A^TA$, ordered so that the corresponding eigenvalues of $A^TA$ satisfy\n",
    "\\[\n",
    "\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_r > \\lambda_{r+1} = \\cdots = \\lambda_n = 0,\n",
    "\\]\n",
    "where $r$ denotes the number of nonzero eigenvalues of $A^TA$, i.e. the number of singular values $\\sigma_i = \\sqrt{\\lambda_i} > 0$, $i=1,\\ldots,r$ of $A$. Then, $Av_1, \\ldots, Av_r$ is an orthogonal basis for $Col(A)$, and $rank(A) = r$.\n",
    "\\end{theorem}\n",
    "\n",
    "\\begin{proof}\n",
    "Because $v_i$ and $v_j$ are orthogonal for $i \\neq j$,\n",
    "\\[\n",
    "(Av_i)^T(Av_j) = v_i^TA^TAv_j = v_i^T\\lambda_jv_j = 0.\n",
    "\\]\n",
    "\n",
    "Thus, $Av_1, \\ldots, Av_r$ are mutually orthogonal, and hence linearly independent. They are also clearly contained in $Col(A)$. Now, for any $y \\in Col(A)$, there must be an $x \\in \\mathbb{R}^n$ such that $y = Ax$. Expanding $x$ in the basis $v_1, \\ldots, v_n$, as $x = c_1v_1 + \\cdots + c_nv_n$ for some $c_1, \\ldots, c_n \\in \\mathbb{R}$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "y &= Ax = A(c_1v_1 + \\cdots + c_nv_n) = c_1Av_1 + \\cdots + c_rAv_r + c_{r+1}Av_{r+1} + \\cdots + c_nAv_n \\\\\n",
    "&= c_1Av_1 + \\cdots + c_rAv_r.\n",
    "\\end{align*}\n",
    "\n",
    "We used that $\\|Av_i\\|^2 = \\lambda_i = 0$ for $i=r+1,\\ldots,n \\Rightarrow Av_i = 0$ for $i=r+1,\\ldots,n$ in the last equality.\n",
    "\n",
    "Therefore, we have that $y \\in span\\{Av_1, \\ldots, Av_r\\}$. Thus $Av_1, \\ldots, Av_r$ is both linearly independent and a spanning set for $Col(A)$, meaning it is an orthogonal basis for $Col(A)$. Hence, by the Fundamental Theorem of Linear Algebra,\n",
    "\n",
    "\\[\n",
    "rank(A) = dim\\,Col(A) = r.\n",
    "\\]\n",
    "\\end{proof}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\textbf{Numerical Note:} In certain cases, the rank of $A$ may be very sensitive to small changes in the entries of $A$. The classic approach of counting the \\# of pivot columns in $A$ does not work well if $A$ is row reduced by a computer, as roundoff errors often create a row echelon form with full rank. In practice, the most reliable way of computing the rank of a large matrix $A$ is to count the number of singular values larger than a small threshold $\\epsilon$ (typically on the order of $10^{-12}$ for very accurate computations). In this case, singular values smaller than $\\epsilon$ are treated as zeros for all practical purposes, and the effective rank of $A$ is computed by counting the remaining nonzero singular values.\n",
    "\n",
    "\\section*{The Singular Value Decomposition}\n",
    "\n",
    "The decomposition of $A$ involves an $r \\times r$ diagonal matrix $\\Sigma$ of the form\n",
    "\n",
    "\\[\n",
    "\\Sigma = \\text{diag}(\\sigma_1, \\ldots, \\sigma_r).\n",
    "\\]\n",
    "\n",
    "We note that because $r = \\text{dim Col}(A) = \\text{dim Range}(A)$ by the FTLA, we must have that $r \\leq \\min\\{m,n\\}$ if $A \\in \\mathbb{R}^{m \\times n}$.\n",
    "\n",
    "\\begin{theorem}\n",
    "Let $A \\in \\mathbb{R}^{m \\times n}$ be an $m \\times n$ matrix of rank $r > 0$. Then $A$ can be factored as\n",
    "\\[\n",
    "A = U \\Sigma V^T,\n",
    "\\]\n",
    "where $U \\in \\mathbb{R}^{m \\times r}$ has orthonormal columns, so $U^TU = I_r$, $\\Sigma = \\text{diag}(\\sigma_1, \\ldots, \\sigma_r)$ is a diagonal matrix with the singular values of $A$ $\\sigma_i$ along the diagonal, and $V \\in \\mathbb{R}^{n \\times r}$ has orthonormal columns, so $V^TV = I_r$.\n",
    "\\end{theorem}\n",
    "\n",
    "Such a factorization of $A$ is called its singular value decomposition, and the columns of $U$ are called the left singular vectors of $A$, while the columns of $V$ are called the right singular vectors of $A$.\n",
    "\n",
    "\\begin{proof}\n",
    "Let $\\lambda_i$ and $v_i$ be the eigenvalues/vectors of $A^TA$ as described previously, so that $Av_1, \\ldots, Av_r$ is an orthogonal basis for col$(A)$. Normalize each $Av_i$ to form an orthonormal basis for col$(A)$:\n",
    "\n",
    "\\[\n",
    "u_i := \\frac{1}{\\|Av_i\\|} Av_i = \\frac{1}{\\sigma_i} Av_i\n",
    "\\]\n",
    "\n",
    "and hence $Av_i = \\sigma_i u_i$ for $i=1,\\ldots,r$. Define the matrices\n",
    "\n",
    "\\[\n",
    "U = [u_1 \\cdots u_r] \\in \\mathbb{R}^{m \\times r} \\quad \\text{and} \\quad V = [v_1 \\cdots v_r] \\in \\mathbb{R}^{n \\times r}\n",
    "\\]\n",
    "\n",
    "By construction, the columns of $U$ are orthonormal: $U^TU = I_r$, and similarly for the columns of $V$: $V^TV = I_r$.\n",
    "\n",
    "\\end{proof}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the following \"full\" matrices:\n",
    "\n",
    "\\[\n",
    "\\hat{U} = [U \\; U^\\perp] \\in \\mathbb{R}^{m \\times m} \\quad \\text{and} \\quad \\hat{V} = [V \\; V^\\perp] \\in \\mathbb{R}^{n \\times n}\n",
    "\\]\n",
    "\n",
    "Here, $V^\\perp = [v_{r+1} \\cdots v_n]$ has orthonormal columns spanning the orthogonal complement of span$\\{v_1, \\ldots, v_r\\}$, so that the columns of $\\hat{V}$ form an orthonormal basis of $\\mathbb{R}^n$.\n",
    "\n",
    "Similarly, let $U^\\perp$ have orthonormal columns spanning the orthogonal complement of span$\\{u_1, \\ldots, u_r\\}$, so the columns of $\\hat{U}$ form an orthonormal basis for $\\mathbb{R}^m$.\n",
    "\n",
    "Finally, define $\\hat{\\Sigma} = \\begin{bmatrix} \\Sigma & 0 \\\\ 0 & 0 \\end{bmatrix}_{m \\times n}$. We first show that\n",
    "\n",
    "\\[\n",
    "A = \\hat{U} \\hat{\\Sigma} \\hat{V}^T, \\quad \\text{or equivalently (since $\\hat{V}$ is orthogonal),} \\quad A\\hat{V} = \\hat{U}\\hat{\\Sigma}.\n",
    "\\]\n",
    "\n",
    "First,\n",
    "\\[\n",
    "A\\hat{V} = [Av_1 \\cdots Av_r \\; Av_{r+1} \\cdots Av_n] = [\\sigma_1u_1 \\cdots \\sigma_ru_r \\; 0 \\cdots 0].\n",
    "\\]\n",
    "\n",
    "Then, notice:\n",
    "\n",
    "\\[\n",
    "\\hat{U} \\hat{\\Sigma} = [u_1 \\cdots u_r \\; u_{r+1} \\cdots u_m] \n",
    "\\begin{bmatrix}\n",
    "\\sigma_1 & & 0 & 0 \\cdots 0 \\\\\n",
    "& \\ddots & & \\vdots \\\\\n",
    "0 & & \\sigma_r & 0 \\cdots 0 \\\\\n",
    "0 & \\cdots & 0 & 0 \\cdots 0 \\\\\n",
    "\\vdots & & \\vdots & \\vdots \\\\\n",
    "0 & \\cdots & 0 & 0 \\cdots 0\n",
    "\\end{bmatrix}\n",
    "= [\\sigma_1u_1 \\cdots \\sigma_ru_r \\; 0 \\cdots 0]\n",
    "\\]\n",
    "\n",
    "So that $A\\hat{V} = \\hat{U}\\hat{\\Sigma}$, or equivalently, $A = \\hat{U}\\hat{\\Sigma}\\hat{V}^T$. But, now, notice:\n",
    "\n",
    "\\[\n",
    "A = \\hat{U} \\hat{\\Sigma} \\hat{V}^T = [U \\; U^\\perp] \\begin{bmatrix} \\Sigma & 0 \\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} V^T \\\\ (V^\\perp)^T \\end{bmatrix} = U \\Sigma V^T, \\quad \\text{proving our result.}\n",
    "\\]\n",
    "\n",
    "\\textbf{NOTE:} Some textbooks define the singular value decomposition of $A$ as $A = \\hat{U} \\hat{\\Sigma} \\hat{V}^T$. This is necessary when allowing for singular values equal to zero. When only considering nonzero singular values, as we do, $A = U\\Sigma V^T$ is the appropriate definition. This is sometimes called the compact SVD of $A$, but we will just call it the SVD.\n",
    "\n",
    "\\textbf{Example:} Let's use the results of the previous example to construct the SVD of\n",
    "\\[\n",
    "A = \\begin{bmatrix} 4 & 8 & 14 \\\\ 8 & 7 & -2 \\end{bmatrix}.\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/08_Ch_9_Symmetric_Matrices/102-Apps.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
