{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 10.1 Applications\n",
    "subject:  SVD\n",
    "subtitle: \n",
    "short_title: 10.1 Applications\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: \n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/08_Ch_9_Symmetric_Matrices/102-Apps.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 17 - Introduction to Graph Theory and Consensus Protocols.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section*{Warmup}\n",
    "\n",
    "The diagonalization theorems we've seen for complete and symmetric matrices have played a role in many interesting applications. Unfortunately, not all matrices can be diagonalized. As we've seen before, if $A$ is not square, even a factorization makes no sense. If $A$ is not square, fortunately a factorization $A = P \\Delta Q^T$ is possible for any matrix $A$. A special factorization of this type, called the singular value decomposition, is one of the most useful and widely applicable matrix factorizations in linear algebra.\n",
    "\n",
    "The singular value decomposition is based on the following key points of matrix diagonalization which we'll show can be captured in general rectangular matrices:\n",
    "\n",
    "\\textcolor{magenta}{Key observation: The absolute values of the eigenvalues of a symmetric matrix $A$ measure the amounts that $A$ stretches or shrinks certain vectors (the eigenvectors). If $Ax = \\lambda x$ and $\\|x\\| = 1$, then}\n",
    "\n",
    "\\[\n",
    "\\|Ax\\| = \\lambda \\|x\\| = |\\lambda| \\|x\\| = |\\lambda|.\n",
    "\\]\n",
    "\n",
    "\\textcolor{magenta}{If $\\lambda_1$ is the eigenvalue with the greatest magnitude, i.e., if $|\\lambda_1| \\geq |\\lambda_i|$ for $i=1,\\ldots,n$, then a corresponding unit eigenvector $v_1$ identifies the direction in which stretching is greatest. That is, the length of $Ax$ is maximized when $x = v_1$, and $\\|Av_1\\| = |\\lambda_1|$.}\n",
    "\n",
    "This description is reminiscent of the optimization principle we saw for characterizing eigenvalues of symmetric matrices, albeit with a focus on maximizing length $\\|Ax\\|$ rather than the quadratic form $x^T A x$. What we'll see next is that this description of $v_1$ and $|\\lambda_1|$ has an analogue for rectangular matrices that will lead to the singular value decomposition.\n",
    "\n",
    "Example: The matrix $A = \\begin{bmatrix} 4 & 11 & 14 \\\\ 8 & 7 & -2 \\end{bmatrix}$ defines a linear map $x \\mapsto Ax$ from $\\mathbb{R}^3$ to $\\mathbb{R}^2$. If we consider the effects of this map on the unit sphere $\\{x \\in \\mathbb{R}^3 \\mid \\|x\\| = 1\\}$, we observe that multiplication by $A$ transforms this sphere in $\\mathbb{R}^3$ into an ellipse in $\\mathbb{R}^2$:\n",
    "\n",
    "\\begin{center}\n",
    "\\includegraphics[width=0.8\\textwidth]{figure1.png}\n",
    "\\captionof{figure}{A transformation from $\\mathbb{R}^3$ to $\\mathbb{R}^2$.}\n",
    "\\end{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to find a unit vector $x$ that maximizes the length $\\|Ax\\|$ is maximized, and compute this maximum length. That is, we want to solve the optimization problem:\n",
    "\n",
    "\\[\n",
    "\\max_{x \\in \\mathbb{R}^n} \\|Ax\\|\n",
    "\\]\n",
    "\n",
    "Our choice of $x$ satisfying $\\|x\\|=1$. Our first observation is that the quantity $\\|Ax\\|^2$ is easier to work with than $\\|Ax\\|$, but that $\\|Ax\\|^2$ is equal to $x^TA^TAx$. Specifically, note that\n",
    "\n",
    "\\[\n",
    "\\|Ax\\|^2 = \\langle Ax, Ax \\rangle = (Ax)^T(Ax) = x^TA^TAx = x^T(A^TA)x.\n",
    "\\]\n",
    "\n",
    "So our task is to now find a unit vector $\\|x\\|=1$ that maximizes the quadratic form $x^T(A^TA)x$. By the spectral (Courant-Fischer) theorem, we know how to do this. By our theorem concerning eigenpairs of symmetric matrices from an optimization perspective, we know the maximum value is the largest eigenvalue $\\lambda_1$ of the matrix $A^TA$, and is attained at the unit eigenvector $v_1$ of $A^TA$ corresponding to $\\lambda_1$.\n",
    "\n",
    "For the matrix in this example:\n",
    "\n",
    "\\[\n",
    "A^TA = \\begin{pmatrix}\n",
    "4 & 8 & 14 \\\\\n",
    "8 & 7 & -2 \\\\\n",
    "14 & -2 & 0\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "4 & 8 & 14 \\\\\n",
    "8 & 7 & -2 \\\\\n",
    "14 & -2 & 0\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "80 & 100 & 40 \\\\\n",
    "100 & 170 & 140 \\\\\n",
    "40 & 140 & 200\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "and the eigenvalue/vector pairs are:\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_1 = 360, \\quad v_1 &= \\begin{pmatrix} 1/3 \\\\ 2/3 \\\\ 2/3 \\end{pmatrix}, \\\\\n",
    "\\lambda_2 = 90, \\quad v_2 &= \\begin{pmatrix} -4/3 \\\\ -1/3 \\\\ 2/3 \\end{pmatrix}, \\\\\n",
    "\\lambda_3 = 0, \\quad v_3 &= \\begin{pmatrix} 2/3 \\\\ -2/3 \\\\ 1/3 \\end{pmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "The maximum value of $x^T(A^TA)x = \\|Ax\\|^2$ is thus $\\lambda_1 = 360$, and attained when $x = v_1$. The vector $Av_1$ is a point on the ellipse in Fig.1 above furthest from the origin, namely\n",
    "\n",
    "\\[\n",
    "Av_1 = \\begin{pmatrix}\n",
    "4 & 8 & 14 \\\\\n",
    "8 & 7 & -2 \\\\\n",
    "14 & -2 & 0\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "1/3 \\\\\n",
    "2/3 \\\\\n",
    "2/3\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "18 \\\\\n",
    "6 \\\\\n",
    "0\n",
    "\\end{pmatrix}.\n",
    "\\]\n",
    "\n",
    "For $\\|x\\| = 1$, the maximum value of $\\|Ax\\|$ is $\\|Av_1\\| = \\sqrt{360} = 6\\sqrt{10}$.\n",
    "\n",
    "This example suggests that the effect of a matrix $A$ on the unit sphere $\\|x\\| = 1$ is related to the eigenpairs of $A^TA$. In fact we'll see next that the entire geometric behavior of the map $x \\mapsto Ax$ is captured by this quadratic form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section*{The Singular Values of an m×n Matrix}\n",
    "\n",
    "Consider an m×n real matrix $A \\in \\mathbb{R}^{m \\times n}$. Then $A^TA$ is an n×n symmetric matrix, and can be orthogonally diagonalized. Let $V = [v_1 \\cdots v_n]$ be an orthogonal matrix composed of orthonormal eigenvectors of $A^TA$, and let $\\lambda_1, \\ldots, \\lambda_n$ be the associated eigenvalues of $A^TA$. Then for $i = 1, \\ldots, n$,\n",
    "\n",
    "\\begin{align*}\n",
    "\\|A v_i\\|^2 &= (Av_i)^T(Av_i) = v_i^T(A^TAv_i) \\\\\n",
    "&= v_i^T(\\lambda_i v_i) \\\\\n",
    "&= \\lambda_i v_i^Tv_i = \\lambda_i \\|v_i\\|^2 \\\\\n",
    "&= \\lambda_i.\n",
    "\\end{align*}\n",
    "\n",
    "This tells us that all of the eigenvalues $\\lambda_i = \\|Av_i\\|^2 \\geq 0$, i.e. $A^TA$ can only take on nonnegative values. $A^TA$ is a positive semidefinite matrix. Let's assume that we've ordered our eigenvalues in decreasing order:\n",
    "\n",
    "\\[\n",
    "\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_n \\geq 0.\n",
    "\\]\n",
    "\n",
    "The singular values of $A$ are the positive square roots of the nonzero eigenvalues $\\lambda_i > 0$ of $A^TA$ denoted $\\sigma_i$. That is, let $\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_r > 0$ and $\\lambda_{r+1} = \\cdots = \\lambda_n = 0$ be a partition of the eigenvalues such that $\\lambda_i > 0$ for $i = 1, \\ldots, r$, and $\\lambda_i = 0$ for $i = r+1, \\ldots, n$. Then $A$ has $r$ singular values, defined as\n",
    "\n",
    "\\[\n",
    "\\sigma_i = \\sqrt{\\lambda_i}, \\quad i = 1, \\ldots, r.\n",
    "\\]\n",
    "\n",
    "\\textbf{WARNING:} Some texts include the zero eigenvalues $\\lambda_{r+1}, \\ldots, \\lambda_n$ of $A^TA$ as singular values of $A$. This is simply a different convention and is mathematically equivalent. However, we find our definition to be more natural for our purposes.\n",
    "\n",
    "\\textbf{Example:} Using the same $A = \\begin{pmatrix} 4 & 8 & 14 \\\\ 8 & 7 & -2 \\\\ 14 & -2 & 0 \\end{pmatrix}$ as the previous example,\n",
    "\n",
    "we have $\\sigma_1 = \\sqrt{360} = 6\\sqrt{10}$, $\\sigma_2 = \\sqrt{90} = 3\\sqrt{10}$. In this case, $A$ only has two singular values as $\\lambda_3 = 0$. For this example, $r = 2$, and $\\lambda_1 = 360 > \\lambda_2 = 90 > \\lambda_3 = 0$.\n",
    "\n",
    "From the previous example, the first singular value of $A$ is the maximum of $\\|Ax\\|$ over all $\\|x\\| = 1$, attained at $v_1$. Our optimization-based characterization of eigenvectors of symmetric matrices tells us that the second singular value of $A$ is the maximum of $\\|Ax\\|$ over all unit vectors orthogonal to $v_1$; this is attained by $v_2$, the second eigenvector of $A^TA$. For $v_2$ from the previous example:\n",
    "\n",
    "\\[\n",
    "Av_2 = \\begin{pmatrix} 4 & 8 & 14 \\\\ 8 & 7 & -2 \\\\ 14 & -2 & 0 \\end{pmatrix} \\begin{pmatrix} -4/3 \\\\ -1/3 \\\\ 2/3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -7 \\\\ -7 \\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "This point is on the minor axis of the ellipse in Fig. 1 above, just as $Av_1$ is on the major axis (see Fig. 2 below). The two singular values of $A$ are the lengths of the semi-axes of this ellipse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "of the major and minor semiaxes of the ellipse\n",
    "\n",
    "\\begin{figure}[h]\n",
    "\\centering\n",
    "\\includegraphics[width=0.5\\textwidth]{figure2}\n",
    "\\caption{An ellipse showing $Av_1$ and $Av_2$ as orthogonal vectors along the major and minor axes.}\n",
    "\\end{figure}\n",
    "\n",
    "That $Av_1$ and $Av_2$ are orthogonal is no accident, as the next theorem shows:\n",
    "\n",
    "\\begin{theorem}\n",
    "Suppose that $v_1, \\ldots, v_n$ is an orthonormal basis for $\\mathbb{R}^n$ composed of the eigenvectors of $A^TA$, ordered so that the corresponding eigenvalues of $A^TA$ satisfy\n",
    "\\[\n",
    "\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_r > \\lambda_{r+1} = \\cdots = \\lambda_n = 0,\n",
    "\\]\n",
    "where $r$ denotes the number of nonzero eigenvalues of $A^TA$, i.e. the number of singular values $\\sigma_i = \\sqrt{\\lambda_i} > 0$, $i=1,\\ldots,r$ of $A$. Then, $Av_1, \\ldots, Av_r$ is an orthogonal basis for $Col(A)$, and $rank(A) = r$.\n",
    "\\end{theorem}\n",
    "\n",
    "\\begin{proof}\n",
    "Because $v_i$ and $v_j$ are orthogonal for $i \\neq j$,\n",
    "\\[\n",
    "(Av_i)^T(Av_j) = v_i^TA^TAv_j = v_i^T\\lambda_jv_j = 0.\n",
    "\\]\n",
    "\n",
    "Thus, $Av_1, \\ldots, Av_r$ are mutually orthogonal, and hence linearly independent. They are also clearly contained in $Col(A)$. Now, for any $y \\in Col(A)$, there must be an $x \\in \\mathbb{R}^n$ such that $y = Ax$. Expanding $x$ in the basis $v_1, \\ldots, v_n$, as $x = c_1v_1 + \\cdots + c_nv_n$ for some $c_1, \\ldots, c_n \\in \\mathbb{R}$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "y &= Ax = A(c_1v_1 + \\cdots + c_nv_n) = c_1Av_1 + \\cdots + c_rAv_r + c_{r+1}Av_{r+1} + \\cdots + c_nAv_n \\\\\n",
    "&= c_1Av_1 + \\cdots + c_rAv_r.\n",
    "\\end{align*}\n",
    "\n",
    "We used that $\\|Av_i\\|^2 = \\lambda_i = 0$ for $i=r+1,\\ldots,n \\Rightarrow Av_i = 0$ for $i=r+1,\\ldots,n$ in the last equality.\n",
    "\n",
    "Therefore, we have that $y \\in span\\{Av_1, \\ldots, Av_r\\}$. Thus $Av_1, \\ldots, Av_r$ is both linearly independent and a spanning set for $Col(A)$, meaning it is an orthogonal basis for $Col(A)$. Hence, by the Fundamental Theorem of Linear Algebra,\n",
    "\n",
    "\\[\n",
    "rank(A) = dim\\,Col(A) = r.\n",
    "\\]\n",
    "\\end{proof}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\textbf{Numerical Note:} In certain cases, the rank of $A$ may be very sensitive to small changes in the entries of $A$. The classic approach of counting the \\# of pivot columns in $A$ does not work well if $A$ is row reduced by a computer, as roundoff errors often create a row echelon form with full rank. In practice, the most reliable way of computing the rank of a large matrix $A$ is to count the number of singular values larger than a small threshold $\\epsilon$ (typically on the order of $10^{-12}$ for very accurate computations). In this case, singular values smaller than $\\epsilon$ are treated as zeros for all practical purposes, and the effective rank of $A$ is computed by counting the remaining nonzero singular values.\n",
    "\n",
    "\\section*{The Singular Value Decomposition}\n",
    "\n",
    "The decomposition of $A$ involves an $r \\times r$ diagonal matrix $\\Sigma$ of the form\n",
    "\n",
    "\\[\n",
    "\\Sigma = \\text{diag}(\\sigma_1, \\ldots, \\sigma_r).\n",
    "\\]\n",
    "\n",
    "We note that because $r = \\text{dim Col}(A) = \\text{dim Range}(A)$ by the FTLA, we must have that $r \\leq \\min\\{m,n\\}$ if $A \\in \\mathbb{R}^{m \\times n}$.\n",
    "\n",
    "\\begin{theorem}\n",
    "Let $A \\in \\mathbb{R}^{m \\times n}$ be an $m \\times n$ matrix of rank $r > 0$. Then $A$ can be factored as\n",
    "\\[\n",
    "A = U \\Sigma V^T,\n",
    "\\]\n",
    "where $U \\in \\mathbb{R}^{m \\times r}$ has orthonormal columns, so $U^TU = I_r$, $\\Sigma = \\text{diag}(\\sigma_1, \\ldots, \\sigma_r)$ is a diagonal matrix with the singular values of $A$ $\\sigma_i$ along the diagonal, and $V \\in \\mathbb{R}^{n \\times r}$ has orthonormal columns, so $V^TV = I_r$.\n",
    "\\end{theorem}\n",
    "\n",
    "Such a factorization of $A$ is called its singular value decomposition, and the columns of $U$ are called the left singular vectors of $A$, while the columns of $V$ are called the right singular vectors of $A$.\n",
    "\n",
    "\\begin{proof}\n",
    "Let $\\lambda_i$ and $v_i$ be the eigenvalues/vectors of $A^TA$ as described previously, so that $Av_1, \\ldots, Av_r$ is an orthogonal basis for col$(A)$. Normalize each $Av_i$ to form an orthonormal basis for col$(A)$:\n",
    "\n",
    "\\[\n",
    "u_i := \\frac{1}{\\|Av_i\\|} Av_i = \\frac{1}{\\sigma_i} Av_i\n",
    "\\]\n",
    "\n",
    "and hence $Av_i = \\sigma_i u_i$ for $i=1,\\ldots,r$. Define the matrices\n",
    "\n",
    "\\[\n",
    "U = [u_1 \\cdots u_r] \\in \\mathbb{R}^{m \\times r} \\quad \\text{and} \\quad V = [v_1 \\cdots v_r] \\in \\mathbb{R}^{n \\times r}\n",
    "\\]\n",
    "\n",
    "By construction, the columns of $U$ are orthonormal: $U^TU = I_r$, and similarly for the columns of $V$: $V^TV = I_r$.\n",
    "\n",
    "\\end{proof}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the following \"full\" matrices:\n",
    "\n",
    "\\[\n",
    "\\hat{U} = [U \\; U^\\perp] \\in \\mathbb{R}^{m \\times m} \\quad \\text{and} \\quad \\hat{V} = [V \\; V^\\perp] \\in \\mathbb{R}^{n \\times n}\n",
    "\\]\n",
    "\n",
    "Here, $V^\\perp = [v_{r+1} \\cdots v_n]$ has orthonormal columns spanning the orthogonal complement of span$\\{v_1, \\ldots, v_r\\}$, so that the columns of $\\hat{V}$ form an orthonormal basis of $\\mathbb{R}^n$.\n",
    "\n",
    "Similarly, let $U^\\perp$ have orthonormal columns spanning the orthogonal complement of span$\\{u_1, \\ldots, u_r\\}$, so the columns of $\\hat{U}$ form an orthonormal basis for $\\mathbb{R}^m$.\n",
    "\n",
    "Finally, define $\\hat{\\Sigma} = \\begin{bmatrix} \\Sigma & 0 \\\\ 0 & 0 \\end{bmatrix}_{m \\times n}$. We first show that\n",
    "\n",
    "\\[\n",
    "A = \\hat{U} \\hat{\\Sigma} \\hat{V}^T, \\quad \\text{or equivalently (since $\\hat{V}$ is orthogonal),} \\quad A\\hat{V} = \\hat{U}\\hat{\\Sigma}.\n",
    "\\]\n",
    "\n",
    "First,\n",
    "\\[\n",
    "A\\hat{V} = [Av_1 \\cdots Av_r \\; Av_{r+1} \\cdots Av_n] = [\\sigma_1u_1 \\cdots \\sigma_ru_r \\; 0 \\cdots 0].\n",
    "\\]\n",
    "\n",
    "Then, notice:\n",
    "\n",
    "\\[\n",
    "\\hat{U} \\hat{\\Sigma} = [u_1 \\cdots u_r \\; u_{r+1} \\cdots u_m] \n",
    "\\begin{bmatrix}\n",
    "\\sigma_1 & & 0 & 0 \\cdots 0 \\\\\n",
    "& \\ddots & & \\vdots \\\\\n",
    "0 & & \\sigma_r & 0 \\cdots 0 \\\\\n",
    "0 & \\cdots & 0 & 0 \\cdots 0 \\\\\n",
    "\\vdots & & \\vdots & \\vdots \\\\\n",
    "0 & \\cdots & 0 & 0 \\cdots 0\n",
    "\\end{bmatrix}\n",
    "= [\\sigma_1u_1 \\cdots \\sigma_ru_r \\; 0 \\cdots 0]\n",
    "\\]\n",
    "\n",
    "So that $A\\hat{V} = \\hat{U}\\hat{\\Sigma}$, or equivalently, $A = \\hat{U}\\hat{\\Sigma}\\hat{V}^T$. But, now, notice:\n",
    "\n",
    "\\[\n",
    "A = \\hat{U} \\hat{\\Sigma} \\hat{V}^T = [U \\; U^\\perp] \\begin{bmatrix} \\Sigma & 0 \\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} V^T \\\\ (V^\\perp)^T \\end{bmatrix} = U \\Sigma V^T, \\quad \\text{proving our result.}\n",
    "\\]\n",
    "\n",
    "\\textbf{NOTE:} Some textbooks define the singular value decomposition of $A$ as $A = \\hat{U} \\hat{\\Sigma} \\hat{V}^T$. This is necessary when allowing for singular values equal to zero. When only considering nonzero singular values, as we do, $A = U\\Sigma V^T$ is the appropriate definition. This is sometimes called the compact SVD of $A$, but we will just call it the SVD.\n",
    "\n",
    "\\textbf{Example:} Let's use the results of the previous example to construct the SVD of\n",
    "\\[\n",
    "A = \\begin{bmatrix} 4 & 8 & 14 \\\\ 8 & 7 & -2 \\end{bmatrix}.\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\textbf{Step 1:} Find an orthogonal diagonalization of $A^TA$. In general, for $A$ with many columns, this is done numerically, but we use the data from before:\n",
    "\n",
    "$A^TA = V \\Lambda V^T = [v_1\\ v_2\\ v_3] \\begin{bmatrix}\n",
    "\\lambda_1 & & \\\\\n",
    "& \\lambda_2 & \\\\\n",
    "& & \\lambda_3\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "v_1^T \\\\\n",
    "v_2^T \\\\\n",
    "v_3^T\n",
    "\\end{bmatrix}$\n",
    "\n",
    "with $(\\lambda_i, v_i)$ as spectral eigenpairs with $\\lambda_1 = 360$, $\\lambda_2 = 90$, $\\lambda_3 = 0$.\n",
    "\n",
    "\\textbf{Step 2:} Setup $V$ and $\\Sigma$. Arrange the nonzero eigenvalues of $A^TA$ in decreasing order and compute the singular values. For this example:\n",
    "\n",
    "$\\sigma_1 = \\sqrt{360}$ and $\\sigma_2 = 3\\sqrt{10}$,\n",
    "\n",
    "and\n",
    "\n",
    "$\\Sigma = diag(\\sigma_1, \\sigma_2) = \\begin{bmatrix}\n",
    "6\\sqrt{10} & 0 \\\\\n",
    "0 & 3\\sqrt{10}\n",
    "\\end{bmatrix}$. Hence $rank A = 2$, and $V \\in \\mathbb{R}^{3\\times2}$\n",
    "\n",
    "The corresponding eigenvectors define the columns of $V$:\n",
    "\n",
    "$V = [v_1\\ v_2] = \\begin{bmatrix}\n",
    "\\frac{\\sqrt{3}}{3} & -\\frac{1}{3} \\\\\n",
    "\\frac{2}{3} & -\\frac{1}{3} \\\\\n",
    "\\frac{2}{3} & \\frac{2}{3}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\\textbf{Step 3:} Construct $U$. Since $rank A = 2$, $U \\in \\mathbb{R}^{2\\times2}$. The columns of $U$ are given by $Av_1$ and $Av_2$. Recall that we should ensure that $\\|Av_1\\| = \\sigma_1$ and $\\|Av_2\\| = \\sigma_2$. So $U = [u_1\\ u_2]$ with\n",
    "\n",
    "$u_1 = \\frac{Av_1}{\\sigma_1} = \\frac{1}{6\\sqrt{10}} \\begin{bmatrix}\n",
    "18 \\\\\n",
    "6\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "3/\\sqrt{10} \\\\\n",
    "1/\\sqrt{10}\n",
    "\\end{bmatrix}$ and\n",
    "\n",
    "$u_2 = \\frac{Av_2}{\\sigma_2} = \\frac{1}{3\\sqrt{10}} \\begin{bmatrix}\n",
    "3 \\\\\n",
    "-9\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1/\\sqrt{10} \\\\\n",
    "-3/\\sqrt{10}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Finally, the SVD of $A$ is:\n",
    "\n",
    "$A = \\begin{bmatrix}\n",
    "3/\\sqrt{10} & 1/\\sqrt{10} \\\\\n",
    "1/\\sqrt{10} & -3/\\sqrt{10}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "6\\sqrt{10} & 0 \\\\\n",
    "0 & 3\\sqrt{10}\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "\\frac{\\sqrt{3}}{3} & \\frac{2}{3} & \\frac{2}{3} \\\\\n",
    "-\\frac{1}{3} & -\\frac{1}{3} & \\frac{2}{3}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\underbrace{}_U \\underbrace{}_\\Sigma \\underbrace{}_{V^T}$\n",
    "\n",
    "You can check that indeed $A = U\\Sigma V^T$ here, and that $U^TU = V^TV = I_2$\n",
    "\n",
    "\\textbf{ONLINE NOTES:} Please add example 4 from LAA 7.1, subtly modified to use the compact SVD form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section{Linear Algebra Applications of the SVD}\n",
    "\n",
    "The next few classes will focus on engineering and AI applications of the SVD. For now, we highlight some more technical linear algebra applications: these are all immensely important from a practical perspective and form subprotines for most real-world applications of linear algebra.\n",
    "\n",
    "\\subsection{The Condition Number}\n",
    "Most numerical calculations that require solving a linear equation $Ax=b$ are as reliable as possible when the SVD of $A$ is used. Since the matrices $U$ and $V$ have orthonormal columns, they do not affect lengths or angles between vectors. For example, for $U\\in\\mathbb{R}^{m\\times m}$, we have:\n",
    "\n",
    "\\[\\langle Ux, Uy \\rangle = x^T U^T U y = x^T y = \\langle x, y \\rangle\\]\n",
    "\n",
    "for any $x,y\\in\\mathbb{R}^m$. Therefore, any numerical issues that arise will be due to the diagonal entries of $\\Sigma$, i.e., due to the singular values of $A$.\n",
    "\n",
    "In particular, if some of singular values are much larger than others, this means certain directions are stretched out much more than others, which can lead to rounding errors. A common measure of such stretching is given by the singular values of $A$. If $A\\in\\mathbb{R}^{m\\times n}$ is an $m\\times n$ matrix so that $r=\\text{rank}(A)\\leq n$, we define the \\textbf{condition number of A} to be the ratio $\\kappa(A) = \\frac{\\sigma_1}{\\sigma_r}$ of the largest to smallest singular values of $A$. If $A$ is not invertible, it is conventional to set $\\kappa(A)=\\infty$, although the ratio $\\frac{\\sigma_1}{\\sigma_r}$ is a useful measure of the numerical stability of computing with a rectangular matrix $A\\in\\mathbb{R}^{m\\times n}$.\n",
    "\n",
    "\\textbf{ONLINE NOTES:} Using numpy, show that ill-conditioned $A$ can lead to bad solutions to $Ax=b$ even if $A$ is invertible. Case 1: assume we use $\\sigma_1 = b + n$ for $n$ some small measurement noise. Case 2: just make $A$ super ill-conditioned and show that $x=A^{-1}b$ computed using numpy doesn't actually satisfy $Ax=b$.\n",
    "\n",
    "\\subsection{Computing Bases of Fundamental Subspaces}\n",
    "\n",
    "Given an SVD for an $m\\times n$ matrix $A\\in\\mathbb{R}^{m\\times n}$ with $\\text{rank}(A)=r$, let $u_1,\\ldots,u_r$ be the left singular vectors, $v_1,\\ldots,v_r$ the right singular vectors, and $\\sigma_1,\\ldots,\\sigma_r$ the singular values.\n",
    "\n",
    "Recall that we showed that $u_1,\\ldots,u_r$ forms a basis for $\\text{Col}(A)$. Let $u_{r+1},\\ldots,u_m$ be an orthonormal basis for $\\text{Col}(A)^\\perp$ so that $u_1,\\ldots,u_m$ form a basis for $\\mathbb{R}^m$, computed for example using the Gram-Schmidt Process. Then, by the FTLA we have that $\\text{Col}(A)^\\perp = \\text{Null}(A^T) = \\text{span}\\{u_{r+1},\\ldots,u_m\\}$, i.e., these vectors form an orthonormal basis for $\\text{Null}(A^T)=\\text{Null}(A)^\\perp$.\n",
    "\n",
    "Next, recall that $v_1,\\ldots,v_r,v_{r+1},\\ldots,v_n$, the eigenvectors of $A^TA$, form an orthonormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basis of $\\mathbb{R}^n$. Since $Av_i = 0$ for $i > r$, we have that $v_{r+1},\\ldots,v_n$ span a subspace of $\\text{Null}(A)$ of dimension $n-r$. But, by the FTLA, $\\dim\\text{Null}(A) = n - \\text{rank}(A) = n-r$.\n",
    "\n",
    "Therefore, $v_{r+1},\\ldots,v_n$ are an orthonormal basis for $\\text{Null}(A)$.\n",
    "\n",
    "Finally, $\\text{Null}(A)^\\perp = \\text{Col}(A^T) = \\text{Row}(A)$. But $\\text{Null}(A)^\\perp = \\text{span}\\{v_1,\\ldots,v_r\\}$ since the $v_i$ are an orthonormal basis for $\\mathbb{R}^n$, and thus $v_1,\\ldots,v_r$ are an orthonormal basis for $\\text{Row}(A)$.\n",
    "\n",
    "Summarizing, we have:\n",
    "\n",
    "\\begin{itemize}\n",
    "\\item $\\text{Col}(A) = \\text{span}\\{u_1,\\ldots,u_r\\}$\n",
    "\\item $\\text{Col}(A)^\\perp = \\text{Null}(A^T) = \\text{span}\\{u_{r+1},\\ldots,u_m\\}$\n",
    "\\item $\\text{Col}(A^T) = \\text{Row}(A) = \\text{span}\\{v_1,\\ldots,v_r\\}$\n",
    "\\item $\\text{Col}(A^T)^\\perp = \\text{Null}(A) = \\text{span}\\{v_{r+1},\\ldots,v_n\\}$\n",
    "\\end{itemize}\n",
    "\n",
    "[Image of four fundamental subspaces and the action of A]\n",
    "\n",
    "Specializing these observations to square matrices, we have the following theorem summarizing invertible matrices:\n",
    "\n",
    "\\textbf{Theorem:} The following statements are equivalent for a square $n\\times n$ matrix $A\\in\\mathbb{R}^{n\\times n}$:\n",
    "\\begin{enumerate}\n",
    "\\item $\\text{Col}(A)^\\perp = \\text{Null}(A^T) = \\{0\\}$\n",
    "\\item $\\text{Null}(A)^\\perp = \\text{Col}(A^T) = \\text{Row}(A) = \\mathbb{R}^n$\n",
    "\\item $\\text{Col}(A^T)^\\perp = \\text{Null}(A) = \\{0\\}$\n",
    "\\item $\\text{Null}(A^T)^\\perp = \\text{Col}(A) = \\mathbb{R}^n$\n",
    "\\item $A$ has rank $=n$\n",
    "\\item $A$ has $n$ (nonzero) singular values\n",
    "\\end{enumerate}\n",
    "\n",
    "\\subsection{The Pseudoinverse of A}\n",
    "\n",
    "Recall the least squares problem of finding a vector $\\hat{x}$ that minimizes the objective $\\|Ax-b\\|^2$. We saw that the least squares solution is given by the solution to the normal equations:\n",
    "\n",
    "$A^TA\\hat{x} = A^Tb$. (NE)\n",
    "\n",
    "Let's rewrite (NE) using the SVD $A = U\\Sigma V^T$, $A^T = V\\Sigma^T U^T = V\\Sigma U^T$ ($\\Sigma = \\Sigma^T$)\n",
    "\n",
    "$A^TA\\hat{x} = V\\Sigma U^T U\\Sigma V^T\\hat{x} = V\\Sigma^2 V^T\\hat{x} = V\\Sigma U^Tb$\n",
    "$\\underbrace{}_{I}$ $\\underbrace{}_{=\\Sigma}$ $(a)$ $(b)$\n",
    "\n",
    "Let's start by left multiplying (a) and (b) by $V^T$ to take advantage of $V^TV = I$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V^T(V\\Sigma^2V^T\\hat{x}) = V^T(V\\Sigma U^Tb) \\Rightarrow \\Sigma^2V^T\\hat{x} = \\Sigma U^Tb$.\n",
    "\n",
    "Now, let's isolate $V^T\\hat{x}$ by multiplying both sides by $\\Sigma^{-2}$:\n",
    "\n",
    "$V^T\\hat{x} = \\Sigma^{-1}U^Tb$. $(*)$\n",
    "\n",
    "Finally, note that $\\hat{x}$ satisfies $(*)$ iff $\\hat{x} = V\\Sigma^{-1}U^Tb$ (again since $V^TV=I$)\n",
    "$+ n$\n",
    "for any $n \\in \\text{Null}(V^T) = \\text{Col}(V)^\\perp$. The special solution $\\hat{x}^* = V\\Sigma^{-1}U^Tb$\n",
    "can be shown to be the minimum norm least squares solution when several $\\hat{x}$\n",
    "exist such that $A\\hat{x} = b$. The matrix\n",
    "\n",
    "$A^+ = V\\Sigma^{-1}U^T$\n",
    "\n",
    "is called the \\textcolor{blue}{pseudoinverse of $A$}, and is also known as the \\textcolor{blue}{Moore-Penrose inverse of $A$}.\n",
    "\n",
    "If we look at $A\\hat{x}^* = AA^+b$, we observe that:\n",
    "\n",
    "$A\\hat{x}^* = U\\Sigma V^TV\\Sigma^{-1}U^Tb = UU^Tb$,\n",
    "$\\underbrace{}_{I}$\n",
    "\n",
    "i.e., $A\\hat{x}^*$ is the orthogonal projection $\\hat{b}$ of $b$ onto $\\text{Col}(A)$.\n",
    "\n",
    "\\textbf{ONLINE NOTES:} Please add the Practice Problems on p.471 of LAA at the end of Ch. 7.4 + solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section*{PRACTICE PROBLEM}\n",
    "\n",
    "Let $\\mathbf{p}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\end{bmatrix}$, \n",
    "$\\mathbf{p}_2 = \\begin{bmatrix} -1 \\\\ 2 \\\\ 1 \\end{bmatrix}$, \n",
    "$\\mathbf{n}_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ -2 \\end{bmatrix}$, and \n",
    "$\\mathbf{n}_2 = \\begin{bmatrix} -2 \\\\ 1 \\\\ 3 \\end{bmatrix}$; \n",
    "let $H_1$ be the hyperplane (plane) in $\\mathbb{R}^3$ passing through the point $\\mathbf{p}_1$ and having normal vector $\\mathbf{n}_1$; and let $H_2$ be the hyperplane passing through the point $\\mathbf{p}_2$ and having normal vector $\\mathbf{n}_2$. Give an explicit description of $H_1 \\cap H_2$ by a formula that shows how to generate all points.\n",
    "\n",
    "\\section*{SOLUTION TO PRACTICE PROBLEM}\n",
    "\n",
    "First, compute $\\mathbf{n}_1 \\cdot \\mathbf{p}_1 = -3$ and $\\mathbf{n}_2 \\cdot \\mathbf{p}_2 = 7$. The hyperplane $H_1$ is the solution set of the equation $x_1 + x_2 - 2x_3 = -3$, and $H_2$ is the solution set of the equation $-2x_1 + x_2 + 3x_3 = 7$. Then\n",
    "\n",
    "\\[H_1 \\cap H_2 = \\{\\mathbf{x} : x_1 + x_2 - 2x_3 = -3 \\text{ and } -2x_1 + x_2 + 3x_3 = 7\\}\\]\n",
    "\n",
    "This is an implicit description of $H_1 \\cap H_2$. To find an explicit description, solve the system of equations by row reduction:\n",
    "\n",
    "\\[\\begin{bmatrix}\n",
    "1 & 1 & -2 & -3 \\\\\n",
    "-2 & 1 & 3 & 7\n",
    "\\end{bmatrix} \\sim\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & -\\frac{5}{3} & -\\frac{10}{3} \\\\\n",
    "0 & 1 & -\\frac{1}{3} & \\frac{1}{3}\n",
    "\\end{bmatrix}\\]\n",
    "\n",
    "Thus $x_1 = -\\frac{10}{3} + \\frac{5}{3}x_3$, $x_2 = \\frac{1}{3} + \\frac{1}{3}x_3$, $x_3 = x_3$. Let \n",
    "$\\mathbf{p} = \\begin{bmatrix} -\\frac{10}{3} \\\\ \\frac{1}{3} \\\\ 0 \\end{bmatrix}$ and \n",
    "$\\mathbf{v} = \\begin{bmatrix} \\frac{5}{3} \\\\ \\frac{1}{3} \\\\ 1 \\end{bmatrix}$. The general solution can be written as $\\mathbf{x} = \\mathbf{p} + x_3\\mathbf{v}$. Thus $H_1 \\cap H_2$ is the line through $\\mathbf{p}$ in the direction of $\\mathbf{v}$. Note that $\\mathbf{v}$ is orthogonal to both $\\mathbf{n}_1$ and $\\mathbf{n}_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/08_Ch_9_Symmetric_Matrices/102-Apps.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
