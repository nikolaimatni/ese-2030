{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 5.1 Linear Functions\n",
    "subject: Linearity\n",
    "subtitle: a simple function class\n",
    "short_title: 5.1 Linear Functions\n",
    "authors:\n",
    "  - name: Nikolai Matni\n",
    "    affiliations:\n",
    "      - Dept. of Electrical and Systems Engineering\n",
    "      - University of Pennsylvania\n",
    "    email: nmatni@seas.upenn.edu\n",
    "license: CC-BY-4.0\n",
    "keywords: linear functions, matrix-vector multiplication, composition, inverse\n",
    "math:\n",
    "  '\\vv': '\\mathbf{#1}'\n",
    "  '\\bm': '\\begin{bmatrix}'\n",
    "  '\\em': '\\end{bmatrix}'\n",
    "  '\\R': '\\mathbb{R}'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/04_Ch_5_Linearity/061-Linear_functions.ipynb)\n",
    "\n",
    "{doc}`Lecture notes <../lecture_notes/Lecture 10 - Linearity, Linear Functions, Transformations, and Operators.pdf>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Material related to this page, as well as additional exercises, can be found in ALA 7.1.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this page, you should know:\n",
    "- definition of Linear functions and some examples\n",
    "- how to verify linearity of functions\n",
    "- how matrix-vector multiplication relates to linear functions\n",
    "- composition of linear functions\n",
    "- inverses of linear functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Linearity\n",
    "\n",
    "A strategy that we have embraced so far has been to turn algebraic questions into geometric ones. Our foundation for this strategy has been the vector space, which allows us to reason abut a wide range of objects (vectors, polynomials, word histograms, and functions) as “arrows” that we can add, stretch, flip and rotate. Our canonical approach to transforming one vector into another has been through matrix-vector multiplication: we start with a vector $\\vv x$ and create a new vector via the mapping $\\vv x \\mapsto A \\vv x$.\n",
    "\n",
    "Our goal in this lecture is to give you a brief introduction to the theory of _linear functions_, of which the function\n",
    "$f(\\vv x) =A \\vv x$, is a special case. Linear functions are also known as linear maps, or when applied to function spaces, linear operators. These functions lie at the heart of robotics, computer graphics, quantum mechanics, and dynamical systems. We will see that by introducing just a little bit more abstraction, we can reason about all of these different settings using the ere mathematical machinery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Functions\n",
    "\n",
    "We start with the basic definition of a _linear function_ which captures the fundamental idea of linearity: it does not matter of we sum two vectors and then transform them via a liner function, or apply the linear function to each vector individually, and then sum their transformations. A formal definition is given below.\n",
    ":::{prf:definition} Linear Function\n",
    ":label: linear_fn_defn\n",
    "Let $V$ and $W$ be a real vector space. A function $L:V \\to W$ mapping the _domain $V$_ to the _codomain $W$_ is called _linear_ if it obeys two basic rules:\n",
    "\\begin{equation}\n",
    "\\label{linear_fn_eqn}\n",
    "L(\\vv v+ \\vv w) &= L(\\vv v) + L(\\vv w) \\ &\\text{(L1)} \\\\\n",
    "L(c\\vv v) &= cL(\\vv v) \\ &\\text{(L2)}\n",
    "\\end{equation}\n",
    "for all $\\vv v,\\vv w \\in V$ and $c \\in \\mathbb{R}$.\n",
    ":::\n",
    "\n",
    "Before looking at some common examples, we make a few comments:\n",
    ":::{note} Comments\n",
    "1. Setting $c = 0$ in [rule (L2)](#linear_fn_eqn) tells us that a linear function always maps the zero element $\\vv 0 \\in V$ to the zero element $\\vv 0 \\in W$ (**note: these are different zero elements as they live in different vector spaces!**).\n",
    "2. A commonly used trick for verifying linearity is to combine [(L1)](#linear_fn_eqn) and [(L2)](#linear_fn_eqn) into the single rule:\n",
    "\\begin{equation}\n",
    "\\label{linear_fn_rule}\n",
    "L(c\\vv v + d\\vv w) = cL(\\vv v) + dL(\\vv w)\\quad \\text{for all} \\quad \\vv v, \\vv w \\in V, \\quad c,d \\in \\mathbb{R} \\quad \\text{(L)}\n",
    "\\end{equation}\n",
    "3. We can extend [rule (L)](#linear_fn_rule) to any finite linear combination:\n",
    "\\begin{equation}\n",
    "\\label{linear_comb}\n",
    "L(c_1\\vv v_1 + \\cdots + c_k\\vv v_k) = c_1L(\\vv v_1) + \\cdots + c_kL(\\vv v_k) \\quad \\text{(LL)}\n",
    "\\end{equation}\n",
    "for all $c_1,\\ldots,c_k \\in \\mathbb{R}$ and $\\vv v_1,\\ldots,\\vv v_k \\in V$.\n",
    "\n",
    ":::\n",
    "\n",
    ":::{note} Terminology\n",
    "Finally a quick note on terminology: we will use _linear function_ and _linear map_ interchangeably when $V$ and $W$ are both finite dimensional, _linear transformation_ when $V = W$, and _linear operator_ when $V$ and $W$ are function spaces.\n",
    ":::\n",
    "\n",
    ":::{prf:example}Zero, Identity, and Scalar Multiplication Functions\n",
    ":label: zero_id_scale_eg\n",
    "1. The zero function $O(\\vv v) = \\vv 0$ which maps any $\\vv v \\in V$ to $\\vv 0 \\in W$ is easily checked to satisfy [rule (L)](#linear_fn_rule) (both sides are zero!).\n",
    "2. The identity function $I(\\vv v) = \\vv v$, which leaves any vector $\\vv v \\in V$ unchanged satisfies [rule (L)](#linear_fn_rule) because both $I(c\\vv v + d\\vv w) = c\\vv v + d\\vv w$ and $cI(\\vv v) + dI(\\vv w) = c\\vv v + d\\vv w$.\n",
    "3. The scalar multiplication function $M_a(\\vv v) = a\\vv v$ which scales an element $\\vv v \\in V$ by the scalar $a \\in \\mathbb{R}$ defines a linear function from $V$ to itself, with $M_0(\\vv v) = O(\\vv v)$ and $M_1(\\vv v) = I(\\vv v)$ appearing as special cases.\n",
    ":::\n",
    "\n",
    ":::{note}\n",
    "We made no assumptions about $V$ and $W$ in [](#zero_id_scale_eg) beyond them being vector spaces.They could be Eucledian spaces, function spaces, or even matrix spaces, and our statements would be equally valid.\n",
    ":::\n",
    "\n",
    ":::{prf:example} Matrix Multiplication\n",
    "\n",
    "Let $V = \\mathbb{R}^n$ and $W = \\mathbb{R}^m$, and $A \\in \\mathbb{R}^{m \\times n}$. Then the function $L(\\vv v) = A\\vv v$ is a linear function since:\n",
    "$$\n",
    "A(c\\vv v + d\\vv w) = cA\\vv v + dA\\vv w \\ \\text{for all} \\ \\vv v,\\vv w \\in \\mathbb{R}^n \\ \\text{and} \\ c,d \\in \\mathbb{R}\n",
    "$$\n",
    "by the basic properties of matrix-vector multiplication.\n",
    "\n",
    "In fact, matrix-vector multiplications are not only a familiar example of linear maps between Eucledian space, they are the **only ones**! \n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::{prf:theorem}\n",
    ":label: linear_fn_thm\n",
    "Every linear function $L: \\mathbb{R}^n \\to \\mathbb{R}^m$ is given by matrix-vector multiplication, $L(\\vv v) = A\\vv v$, for some $A \\in \\mathbb{R}^{m \\times n}$.\n",
    "\n",
    ":::{prf:proof} Proof of [](#linear_fn_thm)\n",
    ":label: proof-linear_fn_thm\n",
    ":class: dropdown\n",
    "\n",
    "The key idea is to apply the linear combination property [(LL)](#linear_comb) to the expression $\\vv v = v_1 \\vv e_1 + \\cdots + v_n \\vv e_n$ of $\\vv v$ in the standard basis of $\\mathbb{R}^n$:\n",
    "\\begin{align*}\n",
    "L(\\vv v) &= L(v_1 \\vv e_1 + \\cdots + v_n \\vv e_n) \\\\\n",
    "&\\overset{(LL)}{=} v_1 L(\\vv e_1) + v_2 L(\\vv e_2) + \\cdots + v_n L(\\vv e_n) \\\\\n",
    "&= \\bm L(\\vv e_1) \\quad L(\\vv e_2) \\quad \\cdots \\quad L(\\vv e_n) \\em\n",
    "\\begin{bmatrix}\n",
    "v_1 \\\\\n",
    "v_2 \\\\\n",
    "\\vdots \\\\\n",
    "v_n\n",
    "\\end{bmatrix} \\\\\n",
    "&= A \\vv v, \\ \\text{where}, \\ A = \\bm L(\\vv e_1) \\quad L(\\vv e_2) \\quad \\cdots \\quad L(\\vv e_n) \\em, \\ \\vv v = \\begin{bmatrix}\n",
    "v_1 \\\\\n",
    "v_2 \\\\\n",
    "\\vdots \\\\\n",
    "v_n\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "Thus we have shown that the way to find the _matrix representation_ of a linear function is to evaluate it on the basis elements and then stack them into a matrix $A = \\bm L(\\vv e_1) &L(\\vv e_2) & \\cdots & L(\\vv e_n)\\em$.\n",
    ":::\n",
    "::::\n",
    "\n",
    ":::{warning}\n",
    "Pay attention to the order of $m$ and $n$: when $L: \\mathbb{R}^n \\to \\mathbb{R}^m$, from $\\mathbb{R}^n$ to $\\mathbb{R}^m$ , $A \\in \\mathbb{R}^{m \\times n}$, with $m$ rows and $n$ columns!\n",
    ":::\n",
    "\n",
    "::::{prf:example} 2D rotators\n",
    ":label: 2d_rot_eg\n",
    "Let's consider the function $R_{\\theta}: \\mathbb{R}^2 \\to \\mathbb{R}^2$ that rotates a vector ${\\vv v \\in \\mathbb{R}^2}$ counter-clockwise by $\\theta$ radians. To find its matrix representation, we look at the [figure](#2d_rotation) below and apply a little high school trigonometry (SOHCAHTOA anyone?).\n",
    ":::{figure}../figures/06-2d_rotation.jpg\n",
    ":label:2d_rotation\n",
    ":alt:2D Rotation\n",
    ":width: 400px\n",
    ":align: center\n",
    ":::\n",
    "Recalling that $\\|\\vv e_1\\| = \\|\\vv e_2\\| = 1$, and that rotating vectors preserve length, we have:\n",
    "$$\n",
    "R_{\\theta} (\\vv e_1) = \\begin{bmatrix} \\cos \\theta \\\\ \\sin \\theta \\end{bmatrix}, \\quad\n",
    "R_{\\theta} (\\vv e_2) = \\begin{bmatrix} -\\sin \\theta \\\\ \\cos \\theta \\end{bmatrix}\n",
    "$$\n",
    "which, when stacked together, give the matrix representation $R_{\\theta} (\\vv v) = A_{\\theta} \\vv v$ with\n",
    "\\begin{equation}\n",
    "\\label{A_rot}\n",
    "A_{\\theta} = \\begin{bmatrix} \\cos \\theta & -\\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{bmatrix}\n",
    "\\end{equation}\n",
    "This looks familiar! Indeed, [this](#A_rot) is the same expression we found when characterizing [orthogonal 2x2 matrices](../03_Ch_4_Orthogonality/053-orthogonal_matrices.ipynb#orthogonal-matrices-ex1). If we then apply $\\vv v \\mapsto A_{\\theta} \\vv v$ we obtain:\n",
    "$$\n",
    "\\hat{\\vv v} = R_{\\theta} (\\vv v) = A_{\\theta} \\vv v = \\begin{bmatrix} \\cos \\theta & -\\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} v_1 \\cos \\theta - v_2 \\sin \\theta \\\\ v_1 \\sin \\theta + v_2 \\cos \\theta \\end{bmatrix}\n",
    "$$\n",
    "which you can check are correct using trigonometry, but follow directly from the linearity of rotation.\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composition\n",
    "\n",
    "Applying one linear function after another is called _composition_: let $V, W, Z$ be vector spaces. If $L: V \\to W$ and $M: W \\to Z$ are linear functions, then the _composite function_ $M \\circ L: V \\to Z$, defined by $(M \\circ L)(\\vv v) = M(L(\\vv v))$, is also linear (easily checked to satisfy rule [(L)](#linear_fn_rule)).\n",
    "\n",
    "This gives us a \"dynamic\" interpretation of matrix-matrix multiplication. If $L(\\vv v) = A\\vv v$ maps $\\mathbb{R}^n$ to $\\mathbb{R}^m$ and $M(\\vv w) = B\\vv w$ maps $\\mathbb{R}^m$ to $\\mathbb{R}^l$, so that $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{l \\times m}$, then:\n",
    "$$\n",
    "(M \\circ L)(\\vv v) = M(L(\\vv v)) = B(A\\vv v) = (BA)\\vv v\n",
    "$$\n",
    "so that the matrix representation of $M \\circ L: \\mathbb{R}^n \\to \\mathbb{R}^l$ is the matrix product $BA \\in \\mathbb{R}^{l \\times n}$. And, like matrix multiplication, composition of linear functions is in general not commutative (order of applying the function matters!)\n",
    "\n",
    ":::{prf:example}Composing rotations\n",
    ":label: comp_rot_eg\n",
    "Composing two rotations results in another rotation: $R_{\\phi} \\circ R_{\\theta} = R_{\\phi + \\theta}$, i.e., if we first rotate by $\\theta$ and then by $\\phi$, it is the same as rotating by $\\theta + \\phi$. Using matrices:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \\cos \\phi & -\\sin \\phi \\\\ \\sin \\phi & \\cos \\phi \\end{bmatrix} \\begin{bmatrix} \\cos \\theta & -\\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{bmatrix} = A_{\\phi} A_{\\theta} = A_{\\phi+\\theta} = \\begin{bmatrix} \\cos( \\phi+\\theta) & -\\sin(\\phi+\\theta) \\\\ \\sin(\\phi+\\theta) & \\cos(\\phi+\\theta) \\end{bmatrix}\n",
    "$$\n",
    "Working out the LHS above, we can derive the well-known trigonometric addition formulas:\n",
    "\n",
    "$$\n",
    "cos(\\phi+\\theta) =  \\cos \\phi \\cos \\theta-  \\sin \\phi \\sin \\theta, \\quad \\sin(\\phi+\\theta) = \\cos \\phi \\sin \\theta + \\sin \\phi \\cos \\theta\n",
    "$$\n",
    "In fact, this counts as a proof!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverses\n",
    "\n",
    "Just as with square matrices, we can define the inverse of a linear function. Let $L: V \\to W$ be a linear function. If $M: W \\to V$ is a _linear function_ such that:\n",
    "\n",
    "$$\n",
    "L \\circ M = I_W \\quad \\text{and} \\quad M \\circ L = I_V\n",
    "$$\n",
    "\n",
    "where $I_W$ and $I_V$ are identity maps on $W$ and $V$ respectively, then $M$ is the _inverse of $L$_ and is denoted $M = L^{-1}$.\n",
    "\n",
    "::::{prf:example} Mapping polyamials $P^{(n)}$ to $\\mathbb{R}^n$ and back again\n",
    ":label: eg_poly_real_vec\n",
    "Let $V = P^{(n)}$ be the space of polynomials of degree $\\leq n$, and let $W = \\mathbb{R}^{n+1}$. Define the linear map $L : P^{(n)} → \\mathbb{R}^{n+1}$ as follows: for $p(x) = a_0 + a_1x + \\cdots + a_nx^n$,\n",
    "$$\n",
    "L(p) =\\bm\n",
    "a_0 \\\\ a_1 \\\\ \\vdots \\\\ a_n\n",
    "\\em,\n",
    "$$\n",
    "i.e, $L(p)$ stacks the coefficients of $p(x)$ into a vector $L(p) \\in \\mathbb{R}^{n+1}$.\n",
    "\n",
    "The inverse map $L^{-1}(\\vv a)$ is simply the mapping that takes a vector $\\vv a = \\bm a_0 ,a_1, \\ldots, a_n\\em^{\\top} \\in \\mathbb{R}^{n+1}$ and outputs the polynomial $L^{−1}(\\vv a)(x) = a_0 + a_1x + \\cdots + a_nx^n$. We check that it\n",
    "satisfies\n",
    "$$\n",
    "L \\circ L^{−1} = I_{\\mathbb{R}^{n+1}} \\ \\text{and} \\ L^{−1} \\circ L = I_{p^{(n)}}\n",
    "$$\n",
    "First,\n",
    "$$(L \\circ L^{−1})(\\vv a) = L (L^{−1}(\\vv a)) = L((a_0 + a_1x + \\cdots + a_nx^n) = \\bm\n",
    "a_0\\\\ a1\\\\ \\vdots \\\\ a_n \\em = \\vv a\n",
    "$$\n",
    "for any $\\vv a \\in \\mathbb{R}^{n+1}$, so that $L \\circ L^{−1} = I_{\\mathbb{R}^{n+1}}$. Next, we check, for any, $p(x) =\n",
    "a_0 + a_1x+ \\cdots + a_nx^n$ :\n",
    "$$\n",
    "(L^{−1} \\circ L) (p) = L^{−1}(L(p)) = L^{−1}\\left(\\bm\n",
    "a_0 \\\\ a_1 \\\\ \\vdots \\\\ \n",
    "a_n \\em \\right) = L^{−1}(\\vv a) = a_0+a_1×+ \\cdots +a_nx^n = p(x)\n",
    "$$\n",
    "So that $L^{−1}\\circ L = I_{p(n)}$.\n",
    "\n",
    ":::{note} Isomoprhic\n",
    ":label: iso_state\n",
    "Because there exists an invertible linear map between $\\mathbb{R}^{n+1}$  and $P^{(n)}$, they are\n",
    "said to be _isomorphic_. As we saw earlier in the semester, this means that \"they behave the same\" and we can do vector space operations in either $\\mathbb{R}^{n+1}$ or $P^{(n)}$, whichever is convenient to us.\n",
    ":::\n",
    "::::\n",
    "\n",
    ":::{note}\n",
    "A more general statement can be made than the previous one: **any** vector space of dimension $n$ is isomorphic to $\\mathbb{R}^n$, and so by studying Eucledian space, we in fact are gaining an\n",
    "understanding of _all finite dimensional vector spaces_.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nikolaimatni/ese-2030/HEAD?labpath=/04_Ch_5_Linearity/061-Linear_functions.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
